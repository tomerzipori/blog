[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Tomer's stats blog",
    "section": "",
    "text": "US Presidential Elections - A Bayesian Perspective\n\n\n\n\n\n\ncode\n\n\nBayes\n\n\nvisualization\n\n\nggplot2\n\n\npolls\n\n\n\nI want to be a pollster too!\n\n\n\n\n\nTomer Zipori\n\n\n\n\n\n\n\n\n\n\n\n\nWhat’s Artificial Intelligence all about?\n\n\n\n\n\n\ncode\n\n\nMachine Learning\n\n\n\nOr, why do we care about square, and not absolute, errors?\n\n\n\n\n\nTomer Zipori\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Modeling for Psychologists, Part 2\n\n\n\n\n\n\ncode\n\n\ntutorial\n\n\nBayes\n\n\n\nMixed effects linear models, generalized linear models\n\n\n\n\n\nJan 13, 2025\n\n\nTomer Zipori\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Modeling for Psychologists, Part 1\n\n\n\n\n\n\ncode\n\n\ntutorial\n\n\nBayes\n\n\n\nUpdating our statistical skills\n\n\n\n\n\nFeb 11, 2024\n\n\nTomer Zipori\n\n\n\n\n\n\n\n\n\n\n\n\nRejecting the null hypothesis or not?\n\n\n\n\n\n\ncode\n\n\ntheory\n\n\n\nWhen different interpretations of probability disagree on the meaning of p-values\n\n\n\n\n\nSep 4, 2023\n\n\nTomer Zipori\n\n\n\n\n\n\n\n\n\n\n\n\nNetflix trends\n\n\n\n\n\n\ncode\n\n\ntext analysis\n\n\nvisualization\n\n\nNLP\n\n\n\nText data visualization and analysis\n\n\n\n\n\nJun 13, 2023\n\n\nTomer Zipori\n\n\n\n\n\n\n\n\n\n\n\n\nReal Vs. Fake news in football\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nMachine Learning\n\n\nNLP\n\n\n\nExperiment in NLP and some text classification\n\n\n\n\n\nMay 12, 2023\n\n\nTomer Zipori\n\n\n\n\n\n\n\n\n\n\n\n\nWhat makes FIFA 23 players good?\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nMachine Learning\n\n\n\nExperimentation with Elastic-net regression and decision tree boosting.\n\n\n\n\n\nMay 6, 2023\n\n\nTomer Zipori\n\n\n\n\n\n\n\n\n\n\n\n\nWho is the leading US state in UFO reports? probably not what you thought\n\n\n\n\n\n\ncode\n\n\nvisualization\n\n\nggplot2\n\n\ntidytuesday\n\n\n\nVisualizing UFO reports per state capita\n\n\n\n\n\nApr 16, 2023\n\n\nTomer Zipori\n\n\n\n\n\n\n\n\n\n\n\n\nCage-free vs. caged hens in the US\n\n\n\n\n\n\ncode\n\n\nvisualization\n\n\nggplot2\n\n\ntidytuesday\n\n\n\n\n\n\n\n\n\nApr 12, 2023\n\n\nTomer Zipori\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tomer Zipori",
    "section": "",
    "text": "Data science & data visualization nerd. I’m studying for my MA in Cognitive Psychology at Ben-Gurion University of the Negev. I hope to record and share my journey in studying and applying cool data-science stuff!"
  },
  {
    "objectID": "posts/eggs/index.html",
    "href": "posts/eggs/index.html",
    "title": "Cage-free vs. caged hens in the US",
    "section": "",
    "text": "Some time ago, when the fall semester began I’ve enrolled on a course called Data science lab as part of the ‘Data Science for the Social Sciences’ program. One of the assignments was to make some TidyTuesday contribution and present it in class. So this is my submission and what I consider as my first respectable attempts at visualizing data."
  },
  {
    "objectID": "posts/eggs/index.html#libraries",
    "href": "posts/eggs/index.html#libraries",
    "title": "Cage-free vs. caged hens in the US",
    "section": "Libraries",
    "text": "Libraries\n\nlibrary(tidytuesdayR) # for easy data loading\nlibrary(tidyverse)    # for data pre-processing and wrangling\nlibrary(lubridate)    # makes dealing with date format much easier\nlibrary(showtext)     # fonts"
  },
  {
    "objectID": "posts/eggs/index.html#loading-fonts",
    "href": "posts/eggs/index.html#loading-fonts",
    "title": "Cage-free vs. caged hens in the US",
    "section": "Loading fonts",
    "text": "Loading fonts\nshowtext is an awesome package that allows to load installed fonts into R and use it in ggplot2 plots (for example). For a really helpful video that I used see this video from the Riffomonas Project.\n\nfont_add(family = \"Stencil\", regular = \"STENCIL.TTF\")\nshowtext_auto()"
  },
  {
    "objectID": "posts/eggs/index.html#taking-a-peak",
    "href": "posts/eggs/index.html#taking-a-peak",
    "title": "Cage-free vs. caged hens in the US",
    "section": "Taking a peak",
    "text": "Taking a peak\n\nhead(eggproduction)\n\n# A tibble: 6 × 6\n  observed_month prod_type     prod_process   n_hens     n_eggs source          \n  &lt;date&gt;         &lt;chr&gt;         &lt;chr&gt;           &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;           \n1 2016-07-31     hatching eggs all          57975000 1147000000 ChicEggs-09-23-…\n2 2016-08-31     hatching eggs all          57595000 1142700000 ChicEggs-10-21-…\n3 2016-09-30     hatching eggs all          57161000 1093300000 ChicEggs-11-22-…\n4 2016-10-31     hatching eggs all          56857000 1126700000 ChicEggs-12-23-…\n5 2016-11-30     hatching eggs all          57116000 1096600000 ChicEggs-01-24-…\n6 2016-12-31     hatching eggs all          57750000 1132900000 ChicEggs-02-28-…\n\n\neggproduction holds monthly data about number of hens and produced eggs in the US. prod_type specifies the type of egg produced, it has 2 levels:\n\nunique(eggproduction$prod_type)\n\n[1] \"hatching eggs\" \"table eggs\"   \n\n\nFor the current mini-project, I’ll stay with “table eggs” only.\nThe variable prod_process specifies the type of housing of the egg-producing hens. It has 3 levels:\n\nunique(eggproduction$prod_process)\n\n[1] \"all\"                     \"cage-free (non-organic)\"\n[3] \"cage-free (organic)\"    \n\n\nI’ll leave data of all hens for now.\n\nPre-processing 1\nFiltering out irrelevant data and renaming some variables.\n\negg_clean &lt;- eggproduction %&gt;%\n  filter(prod_type != \"hatching eggs\" & prod_process == \"all\") %&gt;% # Leave only eggs meant for eating and general data\n  select(-source, -prod_type, -prod_process, n_hens_all = n_hens, n_eggs_all = n_eggs) # Irrelevant columns\n\n\n\nTaking a peak 2\n\nhead(cagefreepercentages)\n\n# A tibble: 6 × 4\n  observed_month percent_hens percent_eggs source                             \n  &lt;date&gt;                &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;                              \n1 2007-12-31              3.2           NA Egg-Markets-Overview-2019-10-19.pdf\n2 2008-12-31              3.5           NA Egg-Markets-Overview-2019-10-19.pdf\n3 2009-12-31              3.6           NA Egg-Markets-Overview-2019-10-19.pdf\n4 2010-12-31              4.4           NA Egg-Markets-Overview-2019-10-19.pdf\n5 2011-12-31              5.4           NA Egg-Markets-Overview-2019-10-19.pdf\n6 2012-12-31              6             NA Egg-Markets-Overview-2019-10-19.pdf\n\n\nThis data-frame also holds monthly data. The variable percent_hens specifies observed or computed percentage of cage-free hens relative to all table-egg-laying hens (from the Github repo). We’ll select these variables and use them to merge with the first data-frame.\n\negg_clean2 &lt;- cagefreepercentages %&gt;%\n  drop_na(percent_eggs) %&gt;%                                                # droping rows with missing percent_eggs data\n  select(-source, -percent_eggs, cagefree_percent_hens = percent_hens) %&gt;% # Irrelevant\\to many NA's columns + renaming\n  inner_join(egg_clean, by = \"observed_month\", multiple = \"all\") %&gt;%       # joining with the first data-frame\n  mutate(Cagefree = (cagefree_percent_hens * n_hens_all) / 100) %&gt;%        # calculating number of cage-free hens\n  mutate(Traditional = n_hens_all - Cagefree) %&gt;%                          # calculating number of traditional housing hens\n  select(observed_month, Cagefree, Traditional) %&gt;%\n  pivot_longer(cols = c(\"Cagefree\", \"Traditional\"), names_to = \"housing\", values_to = \"n_hens\")"
  },
  {
    "objectID": "posts/eggs/index.html#gameplan",
    "href": "posts/eggs/index.html#gameplan",
    "title": "Cage-free vs. caged hens in the US",
    "section": "Gameplan",
    "text": "Gameplan\nFew hours deep, I’ve decided it would be interesting to see the change in number of cage-free hens compared to caged hens during the time period we have data about. Because I wanted to plot only certain points of data along the time axis, I needed to create a subset of the big data-frame that holds the data for the points I wanted to plot.\n\nIdentitfying the dates of interest\nFirst thing, I found 6 dates that are equally spaced between the start and end points. The repetitive code below is quite ugly, and lubridate probably has a nice and elegant solution, I didn’t want to spend to much time on it.\n\ndates_for_plot &lt;- seq.Date(egg_clean2$observed_month[1], egg_clean2$observed_month[nrow(egg_clean2)], length.out = 6)\ndates_for_plot[2] &lt;- as.Date(\"2017-07-31\")\ndates_for_plot[3] &lt;- as.Date(\"2018-05-31\")\ndates_for_plot[4] &lt;- as.Date(\"2019-04-30\")\ndates_for_plot[5] &lt;- as.Date(\"2020-03-31\")\n\n\n\nCreating the subset\nI will use this subset in order to plot the 6 points on the general data. I will only need to specify data = subset_for_points in the relevant geom object.\n\nsubset_for_points &lt;- egg_clean2 %&gt;%\n  filter((housing == \"Cagefree\" & observed_month %in% dates_for_plot) |\n           (housing == \"Traditional\" & (observed_month == dates_for_plot[1] | observed_month == dates_for_plot[6]))) %&gt;%\n  inner_join(select(egg_clean, observed_month, n_hens = n_hens_all), by = \"observed_month\") %&gt;%\n  mutate(n_hens = case_when(housing == \"Cagefree\" ~ n_hens.x,\n                            housing == \"Traditional\" ~ n_hens.y)) %&gt;%\n  select(-n_hens.x, -n_hens.y)"
  },
  {
    "objectID": "posts/eggs/index.html#actually-plotting",
    "href": "posts/eggs/index.html#actually-plotting",
    "title": "Cage-free vs. caged hens in the US",
    "section": "Actually plotting",
    "text": "Actually plotting\nI went with a simple stacked density plot. Watch how the number of cage-free goes from\n\n\nCode\nplot &lt;- egg_clean2 %&gt;%\n  ggplot(aes(x = observed_month, y = n_hens/1000000, fill = factor(housing, levels = c(\"Traditional\", \"Cagefree\")),\n             color = factor(housing, levels = c(\"Traditional\", \"Cagefree\")),\n             label = n_hens/1000000)) +\n  geom_density(position = 'stack', stat = 'identity') +\n  geom_point(data = subset_for_points) +\n  geom_text(data = subset_for_points, aes(label = round(n_hens/1000000)), hjust = 0.5, vjust = -1, size = 6, family = \"serif\") +\n  scale_fill_manual(values = c(\"#ffefd5\", \"#e1bf92\")) +\n  scale_color_manual(values = c(\"#e1bf92\", \"#83502e\")) +\n  annotate(geom = \"text\", x = as_date(\"2019/1/1\"), y = 30, label = \"Cage-Free\", color = \"#808080\",\n           family = \"Stencil\", angle = 2.5, size = 15, alpha = 0.5) +\n  annotate(geom = \"text\", x = as_date(\"2019/1/1\"), y = 190, label = \"Caged\", color = \"#808080\",\n           family = \"Stencil\", angle = 2.5, size = 15, alpha = 0.5) +\n  scale_x_date(breaks = dates_for_plot) +\n  xlab(\"\") +\n  ylab(\"Number of egg-producing hens (millions)\") +\n  labs(fill = \"Housing\", title = \"Number of Cage-free hens in the US is constatly rising\",\n       subtitle = \"Relative number of Cage-free hens in the US in the years 2016-2021\",\n       caption = \"Tomer Zipori | #TidyTuesday | Source: The Humane League's US Egg Production dataset\") +\n  guides(color = \"none\", fill = \"none\") +\n  theme_classic() +\n  theme(axis.title = element_text(size = 16, color = \"#83502e\"),\n        axis.text.x = element_text(size = 13, color = \"#83502e\"),\n        axis.text.y = element_text(size = 13, color = \"#83502e\"),\n        plot.title = element_text(hjust = 0.5, size = 18, color = \"#83502e\"),\n        plot.subtitle = element_text(hjust = 0.5, size = 13, family = \"serif\", color = \"#83502e\"),\n        plot.caption = element_text(family = \"serif\", color = \"#83502e\"),\n        plot.margin = margin(0.5,0.5,0.5,0.7, \"cm\"),\n        plot.background = element_rect(fill = \"#F9EBDE\"),\n        panel.background = element_rect(fill = \"#F9EBDE\"))\nplot"
  },
  {
    "objectID": "posts/fifa23/index.html",
    "href": "posts/fifa23/index.html",
    "title": "What makes FIFA 23 players good?",
    "section": "",
    "text": "The current Data is an upload to Kaggle by Babatunde Zenith, and it includes information about players in the popular FIFA 23 video game. Information includes: name, age, nationality, position, various football ratings and contract deals.\nThe current notebook is an attempt at:\n      1. Accurately and efficiently predicting player’s overall rating.\n      2. Identifying important variables (features) for this prediction.\nBoth goals will be achieved using two methods: Elastic-net regression and Decision tree Boosting. Data pre-processing will be done with tidyverse, Model fitting and evaluation will be done with the caret and gbm packages."
  },
  {
    "objectID": "posts/fifa23/index.html#re-naming-columns",
    "href": "posts/fifa23/index.html#re-naming-columns",
    "title": "What makes FIFA 23 players good?",
    "section": "Re-naming columns",
    "text": "Re-naming columns\nReplacing spaces with underscores for ease.\n\nnames(players) &lt;- str_replace_all(names(players), pattern = \" \", replacement = \"_\")"
  },
  {
    "objectID": "posts/fifa23/index.html#non-numeric-variables",
    "href": "posts/fifa23/index.html#non-numeric-variables",
    "title": "What makes FIFA 23 players good?",
    "section": "non-numeric variables",
    "text": "non-numeric variables\nFirst we’ll look at potential garbage variables.\n\nnames(select(players, where(is.character)))\n\n [1] \"Known_As\"                    \"Full_Name\"                  \n [3] \"Positions_Played\"            \"Best_Position\"              \n [5] \"Nationality\"                 \"Image_Link\"                 \n [7] \"Club_Name\"                   \"Club_Position\"              \n [9] \"Contract_Until\"              \"Club_Jersey_Number\"         \n[11] \"On_Loan\"                     \"Preferred_Foot\"             \n[13] \"National_Team_Name\"          \"National_Team_Image_Link\"   \n[15] \"National_Team_Position\"      \"National_Team_Jersey_Number\"\n[17] \"Attacking_Work_Rate\"         \"Defensive_Work_Rate\"        \n\n\nAlmost all garbage data. Since I’ve noted that Work Rate variables are ordered (low-medium-high) We’ll re-code them:\n\nplayers &lt;- players %&gt;%\n  mutate(Attacking_Work_Rate = case_when(Attacking_Work_Rate == \"Low\" ~ 1,\n                                         Attacking_Work_Rate == \"Medium\" ~ 2,\n                                         Attacking_Work_Rate == \"High\" ~ 3),\n         Defensive_Work_Rate = case_when(Defensive_Work_Rate == \"Low\" ~ 1,\n                                         Defensive_Work_Rate == \"Medium\" ~ 2,\n                                         Defensive_Work_Rate == \"High\" ~ 3)) %&gt;%\n  select(-Known_As, -Full_Name, -Positions_Played, -Nationality, -Image_Link, -Club_Name, -Contract_Until, -Club_Jersey_Number, -National_Team_Name, -National_Team_Image_Link, -National_Team_Jersey_Number, -On_Loan) %&gt;% # getting rid of garbage variables\n  mutate(across(where(is.character), ~na_if(., \"-\"))) # replacing all \"-\" with NA"
  },
  {
    "objectID": "posts/fifa23/index.html#searching-for-variables-with-large-number-of-nas",
    "href": "posts/fifa23/index.html#searching-for-variables-with-large-number-of-nas",
    "title": "What makes FIFA 23 players good?",
    "section": "Searching for variables with large number of NA’s",
    "text": "Searching for variables with large number of NA’s\n\ncolSums(is.na(players))\n\n                 Overall                Potential           Value(in_Euro) \n                       0                        0                        0 \n           Best_Position                      Age            Height(in_cm) \n                       0                        0                        0 \n           Weight(in_kg)               TotalStats                BaseStats \n                       0                        0                        0 \n           Wage(in_Euro)           Release_Clause            Club_Position \n                       0                        0                       86 \n               Joined_On           Preferred_Foot         Weak_Foot_Rating \n                       0                        0                        0 \n             Skill_Moves International_Reputation   National_Team_Position \n                       0                        0                    16746 \n     Attacking_Work_Rate      Defensive_Work_Rate               Pace_Total \n                       0                        0                        0 \n          Shooting_Total            Passing_Total          Dribbling_Total \n                       0                        0                        0 \n         Defending_Total        Physicality_Total                 Crossing \n                       0                        0                        0 \n               Finishing         Heading_Accuracy            Short_Passing \n                       0                        0                        0 \n                 Volleys                Dribbling                    Curve \n                       0                        0                        0 \n       Freekick_Accuracy              LongPassing              BallControl \n                       0                        0                        0 \n            Acceleration             Sprint_Speed                  Agility \n                       0                        0                        0 \n               Reactions                  Balance               Shot_Power \n                       0                        0                        0 \n                 Jumping                  Stamina                 Strength \n                       0                        0                        0 \n              Long_Shots               Aggression            Interceptions \n                       0                        0                        0 \n             Positioning                   Vision                Penalties \n                       0                        0                        0 \n               Composure                  Marking          Standing_Tackle \n                       0                        0                        0 \n          Sliding_Tackle        Goalkeeper_Diving      Goalkeeper_Handling \n                       0                        0                        0 \n       GoalkeeperKicking   Goalkeeper_Positioning      Goalkeeper_Reflexes \n                       0                        0                        0 \n               ST_Rating                LW_Rating                LF_Rating \n                       0                        0                        0 \n               CF_Rating                RF_Rating                RW_Rating \n                       0                        0                        0 \n              CAM_Rating                LM_Rating                CM_Rating \n                       0                        0                        0 \n               RM_Rating               LWB_Rating               CDM_Rating \n                       0                        0                        0 \n              RWB_Rating                LB_Rating                CB_Rating \n                       0                        0                        0 \n               RB_Rating                GK_Rating \n                       0                        0 \n\n\nNational team position seems sparse, we’ll have to get rid of club_position as well for the model fitting. We’ll also get rid of best_position because it creates so much dummy vars. I’ll analyzed it in another day…\n\nplayers &lt;- select(players, -National_Team_Position, -Club_Position, -Best_Position)"
  },
  {
    "objectID": "posts/fifa23/index.html#data-splitting",
    "href": "posts/fifa23/index.html#data-splitting",
    "title": "What makes FIFA 23 players good?",
    "section": "Data splitting",
    "text": "Data splitting\n\nset.seed(14)\ntrain_id &lt;- createDataPartition(y = players$Overall, p = 0.7, list = F)\n\nplayers_train &lt;- players[train_id,]\nplayers_test &lt;- players[-train_id,]"
  },
  {
    "objectID": "posts/fifa23/index.html#tuning-grid-for-hyper-parameters",
    "href": "posts/fifa23/index.html#tuning-grid-for-hyper-parameters",
    "title": "What makes FIFA 23 players good?",
    "section": "Tuning grid for hyper-parameters",
    "text": "Tuning grid for hyper-parameters\n\ntg &lt;- expand.grid(alpha = c(seq(0, 1, length.out = 25)),\n                  lambda = c(2 ^ seq(10, -10, length = 100)))\n\nSetting a relatively large range of hyper-parameters because elastic-net regression is not super expansive computationally."
  },
  {
    "objectID": "posts/fifa23/index.html#training",
    "href": "posts/fifa23/index.html#training",
    "title": "What makes FIFA 23 players good?",
    "section": "Training",
    "text": "Training\n\nelastic_reg &lt;- train(Overall ~ ., \n                    data = players_train,\n                    method = \"glmnet\",\n                    preProcess = c(\"center\", \"scale\"), # for better interpatation of coefficients\n                    tuneGrid = tg,\n                    trControl =  trainControl(method = \"cv\", number = 10)) # 10-fold Cross-Validation"
  },
  {
    "objectID": "posts/fifa23/index.html#best-hyper-parameters",
    "href": "posts/fifa23/index.html#best-hyper-parameters",
    "title": "What makes FIFA 23 players good?",
    "section": "Best hyper-parameters",
    "text": "Best hyper-parameters\n\nelastic_reg$bestTune\n\n     alpha       lambda\n1501 0.625 0.0009765625"
  },
  {
    "objectID": "posts/fifa23/index.html#traincv-error",
    "href": "posts/fifa23/index.html#traincv-error",
    "title": "What makes FIFA 23 players good?",
    "section": "Train/CV error",
    "text": "Train/CV error\n\npar(bg = '#F9EBDE')\nplot(elastic_reg, xTrans = log, digits = 3)\n\n\n\n\n\n\n\nelastic_reg$results[elastic_reg$results$RMSE == min(elastic_reg$results$RMSE, na.rm = T),]\n\n     alpha       lambda     RMSE  Rsquared     MAE     RMSESD  RsquaredSD\n1501 0.625 0.0009765625 1.601089 0.9440492 1.24766 0.04469899 0.003446449\n          MAESD\n1501 0.02690937\n\n\nAll mixes of \\(\\alpha\\) and \\(\\lambda\\) hyper-parameters converge in the end."
  },
  {
    "objectID": "posts/fifa23/index.html#model-coefficients",
    "href": "posts/fifa23/index.html#model-coefficients",
    "title": "What makes FIFA 23 players good?",
    "section": "Model coefficients",
    "text": "Model coefficients\n\npar(bg = '#F9EBDE')\n\nelasnet_coeffs &lt;- coef(elastic_reg$finalModel, s = elastic_reg$bestTune$lambda)\nplot(elasnet_coeffs, ylab = \"Coefficient\")\n\n\n\n\n\n\n\nround(elasnet_coeffs, 4)\n\n73 x 1 sparse Matrix of class \"dgCMatrix\"\n                              s1\n(Intercept)              65.9420\nPotential                 2.3510\n`Value(in_Euro)`          0.6037\nAge                       2.0174\n`Height(in_cm)`          -0.1111\n`Weight(in_kg)`           0.0979\nTotalStats               -2.7794\nBaseStats                 0.0004\n`Wage(in_Euro)`           0.2468\nRelease_Clause           -0.2689\nJoined_On                 0.0710\nPreferred_FootRight      -0.0694\nWeak_Foot_Rating         -0.0397\nSkill_Moves               0.3644\nInternational_Reputation -0.2143\nAttacking_Work_Rate      -0.0729\nDefensive_Work_Rate      -0.1001\nPace_Total                0.6887\nShooting_Total            0.4371\nPassing_Total             0.6242\nDribbling_Total           1.4907\nDefending_Total          -0.0937\nPhysicality_Total         0.9278\nCrossing                  0.3602\nFinishing                -0.4256\nHeading_Accuracy          0.8768\nShort_Passing             0.2001\nVolleys                   0.0255\nDribbling                -1.3137\nCurve                     0.0000\nFreekick_Accuracy         0.1731\nLongPassing              -0.6509\nBallControl               0.1516\nAcceleration              0.0289\nSprint_Speed             -0.1612\nAgility                  -0.1264\nReactions                 1.1084\nBalance                  -0.0032\nShot_Power               -0.0565\nJumping                   0.0664\nStamina                   0.0582\nStrength                 -0.1455\nLong_Shots               -0.3293\nAggression               -0.1703\nPositioning              -1.1545\nVision                   -0.7001\nPenalties                 0.1108\nComposure                 0.4431\nMarking                   0.6215\nStanding_Tackle           0.2082\nSliding_Tackle            0.2331\nGoalkeeper_Diving         0.1745\nGoalkeeper_Handling      -0.0091\nGoalkeeperKicking         0.0767\nGoalkeeper_Positioning   -0.0696\nGoalkeeper_Reflexes      -0.0828\nST_Rating                 2.5495\nLW_Rating                -0.0648\nLF_Rating                 0.0000\nCF_Rating                 0.0000\nRF_Rating                 0.0000\nRW_Rating                 .     \nCAM_Rating               -0.1751\nLM_Rating                 0.5492\nCM_Rating                 1.9575\nRM_Rating                 0.0129\nLWB_Rating                .     \nCDM_Rating                1.3227\nRWB_Rating                .     \nLB_Rating                -0.0021\nCB_Rating                -0.9501\nRB_Rating                 .     \nGK_Rating                 0.6368\n\n\nThe intercept is quite large. Let’s look at the variables in a more informative scale.\n\npar(bg = '#F9EBDE')\n\nplot(elasnet_coeffs[-1,], ylab = \"Coefficient\")"
  },
  {
    "objectID": "posts/fifa23/index.html#test-error",
    "href": "posts/fifa23/index.html#test-error",
    "title": "What makes FIFA 23 players good?",
    "section": "Test error",
    "text": "Test error\n\nelasticreg_pred &lt;- predict(elastic_reg, newdata = players_test) # calculating model's prediction for test set\n\nTest error and effect size\n\\(RMSE=1.60955711701293\\)\n\\(R^2=0.944839845538989\\)\nVery nice!"
  },
  {
    "objectID": "posts/fifa23/index.html#training-control",
    "href": "posts/fifa23/index.html#training-control",
    "title": "What makes FIFA 23 players good?",
    "section": "Training control",
    "text": "Training control\nWe’ll use adaptive cross-validation in order to make the hyper-parameter search more efficient.\nFor further explanation on implementation in R see. For further reading on theory see.\n\ntr &lt;- trainControl(method = \"adaptive_cv\",\n                   number = 10, repeats = 10,\n                   adaptive = list(min = 5, alpha = 0.05, \n                                   method = \"BT\", complete = TRUE),\n                   search = \"random\")"
  },
  {
    "objectID": "posts/fifa23/index.html#training-1",
    "href": "posts/fifa23/index.html#training-1",
    "title": "What makes FIFA 23 players good?",
    "section": "Training",
    "text": "Training\n\nset.seed(14)\nboost_model &lt;- train(Overall ~ ., \n                   data = players_train,\n                   method = \"gbm\",\n                   trControl = tr, # No explicit tuning grid is needed\n                   verbose = T)"
  },
  {
    "objectID": "posts/fifa23/index.html#traincv-error-1",
    "href": "posts/fifa23/index.html#traincv-error-1",
    "title": "What makes FIFA 23 players good?",
    "section": "Train/CV error",
    "text": "Train/CV error\nGetting the results of the best tuning parameters found.\n\nboost_model$results[boost_model$results$RMSE == min(boost_model$results$RMSE, na.rm = T),5:10]\n\n       RMSE  Rsquared       MAE      RMSESD   RsquaredSD       MAESD\n2 0.7146858 0.9893686 0.5457707 0.005980442 0.0002966158 0.002603466\n\n\nSeems quite optimized, but is it overfitted?"
  },
  {
    "objectID": "posts/fifa23/index.html#test-error-1",
    "href": "posts/fifa23/index.html#test-error-1",
    "title": "What makes FIFA 23 players good?",
    "section": "Test error",
    "text": "Test error\n\nboost_pred &lt;- predict(boost_model, players_test)\n\nTest error and effect size\n\\(RMSE=0.700272523649518\\)\n\\(R^2=0.989645490170188\\)\nVery Very nice!"
  },
  {
    "objectID": "posts/fifa23/index.html#variable-importance",
    "href": "posts/fifa23/index.html#variable-importance",
    "title": "What makes FIFA 23 players good?",
    "section": "Variable importance",
    "text": "Variable importance\n\nvarimp &lt;- caret::varImp(boost_model, scale = T)\n\nvarimp\n\ngbm variable importance\n\n  only 20 most important variables shown (out of 72)\n\n                        Overall\n`Value(in_Euro)`       100.0000\nReactions               50.2939\nBaseStats               16.4047\nAge                      7.5742\n`Wage(in_Euro)`          3.8700\nPotential                3.6002\nCB_Rating                2.3274\nDefending_Total          1.0533\nGoalkeeper_Positioning   0.6619\nCrossing                 0.3162\nTotalStats               0.2916\nShooting_Total           0.2559\nStrength                 0.2359\nPositioning              0.2113\nStanding_Tackle          0.1991\nLF_Rating                0.1927\nRelease_Clause           0.1814\nDribbling_Total          0.1650\nLB_Rating                0.1578\nHeading_Accuracy         0.1497\n\n\n\nPlotting variable importance\n\n\nShow the plot’s code\n# data preparation\nvarimp$importance %&gt;%\n  rownames_to_column(var = \"Feature\") %&gt;%\n  dplyr::rename(Importance = Overall) %&gt;%\n  filter(Importance != 0) %&gt;% # Only features that have an above 0 importance\n  \n  # Plotting\n  ggplot(aes(x = reorder(Feature, -Importance), y = Importance)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip(ylim = c(0, 100)) +\n  scale_y_continuous(limits = c(0,100), expand = c(0, 0)) +\n  labs(x = \"Feature\", y = \"Importance\", title = \"Variable importance in boosted model\", caption = \"Tomer Zipori | FIFA 23 Player Research by Babatunde Zenith | Kaggle\") +\n  theme_classic() +\n  theme(axis.text.y = element_text(size = 7),\n        plot.title = element_text(size = 16, hjust = 0.5),\n        plot.margin = unit(c(1,1,1,1), \"cm\"),\n        plot.caption = element_text(size = 6, hjust = 0.5, vjust = -5),\n        plot.background = element_rect(fill = \"#F9EBDE\"),\n        panel.background = element_rect(fill = \"#F9EBDE\"))\n\n\n\n\n\n\n\n\n\nPlayer value is the strongest predictor by far, with a few interesting ones right behind it (CB_rating?)."
  },
  {
    "objectID": "posts/football_fake_news/index.html",
    "href": "posts/football_fake_news/index.html",
    "title": "Real Vs. Fake news in football",
    "section": "",
    "text": "As part of my data-science courses this fall semester, I have started also to get into Natural Language Processing (NLP, but of the good kind). This is the first time I have ever tried it, so let’s see how it went.\nBrief overview of the post:\n      1. Basic preprocessing and creation of Data Feature Matrix (DFM), using packages stringr and quanteda.\n      2. Classification with Naive-Bayes method.\n      3. Classification with Logistic regression, and feature importance plot."
  },
  {
    "objectID": "posts/football_fake_news/index.html#adding-labels",
    "href": "posts/football_fake_news/index.html#adding-labels",
    "title": "Real Vs. Fake news in football",
    "section": "Adding labels",
    "text": "Adding labels\n\nreal$label &lt;- \"real\"\nfake$label &lt;- \"fake\""
  },
  {
    "objectID": "posts/football_fake_news/index.html#combining-data-frames",
    "href": "posts/football_fake_news/index.html#combining-data-frames",
    "title": "Real Vs. Fake news in football",
    "section": "Combining data frames",
    "text": "Combining data frames\n\ntweets &lt;- rbind(real, fake) |&gt;\n  drop_na() |&gt; # dropping NA rows\n  mutate(tweet = str_trim(tweet) |&gt; str_squish()) # removing white-spaces in the start and beginning of tweet, and between words"
  },
  {
    "objectID": "posts/football_fake_news/index.html#creating-dfms-cleaning-text",
    "href": "posts/football_fake_news/index.html#creating-dfms-cleaning-text",
    "title": "Real Vs. Fake news in football",
    "section": "Creating DFMs & cleaning text",
    "text": "Creating DFMs & cleaning text\nCreating a count-based DFM (frequency of each word in each document). Before creating the DFM I employ 3 steps of normalization:\n      1. Removing special characters and expressions (punctuation, urls…).\n      2. Converting words to their stem-form (the word example converts to exampl). This step should decrease       the number of unique tokens, so that our DFM would be less sparse.\n      3. Converting every character to lower-case letters.\n\ndata_feature_mat &lt;- corp |&gt;\n  tokens(remove_punct = T, remove_numbers = T, remove_url = T, remove_separators = T, remove_symbols = T) |&gt;\n  tokens_wordstem() |&gt;\n  dfm(tolower = T)\n\nhead(data_feature_mat)\n\nDocument-feature matrix of: 6 documents, 18,869 features (99.89% sparse) and 1 docvar.\n       features\ndocs    sun down technic director al-ah respect us and play to\n  text1   1    1       1        1     1       1  1   1    1  1\n  text2   0    0       0        0     0       0  0   0    0  2\n  text3   0    0       1        1     0       0  0   0    0  0\n  text4   0    0       0        0     0       0  0   1    0  0\n  text5   0    0       0        0     0       0  0   0    0  1\n  text6   0    0       0        0     0       0  0   0    0  0\n[ reached max_nfeat ... 18,859 more features ]"
  },
  {
    "objectID": "posts/football_fake_news/index.html#naive-bayes-classifier",
    "href": "posts/football_fake_news/index.html#naive-bayes-classifier",
    "title": "Real Vs. Fake news in football",
    "section": "Naive-Bayes classifier",
    "text": "Naive-Bayes classifier\nA Naive-Bayes classifier predicts the class (in this case of a document) in the following way:\n\n\nThe likelihood of each token to appear in documents of each class is calculated from the training data. For example, if the word manager appeared in \\(11\\)% of the documents in the first class, and in \\(2\\)% of the documents in the second class, the likelihood of it in each class is:\n\\[\n\\displaylines{p(manager\\ |\\ class\\ 1)=0.11\\\\p(manager\\ |\\ class\\ 2)=0.02}\n\\]\nThe prior probability of each document to be classified to each class is also learned from the training data, and it is the base-rate frequencies of the two classes.\n\nFor each document in the test set, the likelihood of it given it is from each of the classes is calculated by multiplying the likelihoods of the tokens appearing in it. So if a document is for example the sentence I like turtles, then the likelihood of it to belong class 1 is:\n\\[\n\\displaylines{p(I\\ |\\ class\\ 1)\\cdotp(like\\ |\\ class\\ 1)\\cdotp(turtles\\ |\\ class\\ 1)}\n\\]\nMore formally, if a document belong to a certain class \\(k\\), then it’s likelihood of being comprised of a set of tokens \\(t\\) is:\n\\[\n\\prod_{i=1}^{n}p(t_i\\ |\\ class\\ k)\n\\]\nAccording to Bayes theorem, the probability of the document to belong to class \\(k\\) - the posterior probability - is proportional to the product of the likelihood of it’s tokens given this class and the prior probability of any document to belong to this class:\n\\[\np(class\\ k\\ |\\ t) \\propto p(t\\ |\\ class\\ k)\\cdotp(class\\ k)\n\\]\nBecause the Naive-Bayes classifier is comparing between classes, the standardizing term is not needed. The class that has the largest product of prior and likelihood is the class the document will be classified to."
  },
  {
    "objectID": "posts/football_fake_news/index.html#fitting-the-model",
    "href": "posts/football_fake_news/index.html#fitting-the-model",
    "title": "Real Vs. Fake news in football",
    "section": "Fitting the model",
    "text": "Fitting the model\n\nnb_model &lt;- textmodel_nb(train_dfm, y = docvars(train_dfm, \"label\"))"
  },
  {
    "objectID": "posts/football_fake_news/index.html#test-performance",
    "href": "posts/football_fake_news/index.html#test-performance",
    "title": "Real Vs. Fake news in football",
    "section": "Test performance",
    "text": "Test performance\n\npred_nb &lt;- predict(nb_model, newdata = test_dfm)\n\n(conmat_nb &lt;- table(pred_nb, docvars(test_dfm, \"label\")))\n\n       \npred_nb fake real\n   fake 3893  380\n   real  143 3956\n\n\nSeems nice, let’s look at some metrics.\nConfusion matrix\n\ncaret::confusionMatrix(conmat_nb, mode = \"everything\", positive = \"real\")\n\nConfusion Matrix and Statistics\n\n       \npred_nb fake real\n   fake 3893  380\n   real  143 3956\n                                          \n               Accuracy : 0.9375          \n                 95% CI : (0.9321, 0.9426)\n    No Information Rate : 0.5179          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.8752          \n                                          \n Mcnemar's Test P-Value : &lt; 2.2e-16       \n                                          \n            Sensitivity : 0.9124          \n            Specificity : 0.9646          \n         Pos Pred Value : 0.9651          \n         Neg Pred Value : 0.9111          \n              Precision : 0.9651          \n                 Recall : 0.9124          \n                     F1 : 0.9380          \n             Prevalence : 0.5179          \n         Detection Rate : 0.4725          \n   Detection Prevalence : 0.4896          \n      Balanced Accuracy : 0.9385          \n                                          \n       'Positive' Class : real            \n                                          \n\n\nNice results!"
  },
  {
    "objectID": "posts/football_fake_news/index.html#logistic-regression",
    "href": "posts/football_fake_news/index.html#logistic-regression",
    "title": "Real Vs. Fake news in football",
    "section": "Logistic regression",
    "text": "Logistic regression\nThe nice thing about the textmodel_lr method from the quanteda.textmodels package is that it does the Cross-Validation for us!\n\nlr_model &lt;- textmodel_lr(x = train_dfm, y = docvars(train_dfm, \"label\"))"
  },
  {
    "objectID": "posts/football_fake_news/index.html#test-performance-1",
    "href": "posts/football_fake_news/index.html#test-performance-1",
    "title": "Real Vs. Fake news in football",
    "section": "Test performance",
    "text": "Test performance\n\npred_lr &lt;- predict(lr_model, newdata = test_dfm)\n\n(conmat_lr &lt;- table(pred_lr, docvars(test_dfm, \"label\")))\n\n       \npred_lr fake real\n   fake 3795  188\n   real  241 4148\n\n\nAlso seems nice.\nConfusion matrix\n\ncaret::confusionMatrix(conmat_lr, mode = \"everything\", positive = \"real\")\n\nConfusion Matrix and Statistics\n\n       \npred_lr fake real\n   fake 3795  188\n   real  241 4148\n                                          \n               Accuracy : 0.9488          \n                 95% CI : (0.9438, 0.9534)\n    No Information Rate : 0.5179          \n    P-Value [Acc &gt; NIR] : &lt; 2e-16         \n                                          \n                  Kappa : 0.8973          \n                                          \n Mcnemar's Test P-Value : 0.01205         \n                                          \n            Sensitivity : 0.9566          \n            Specificity : 0.9403          \n         Pos Pred Value : 0.9451          \n         Neg Pred Value : 0.9528          \n              Precision : 0.9451          \n                 Recall : 0.9566          \n                     F1 : 0.9508          \n             Prevalence : 0.5179          \n         Detection Rate : 0.4955          \n   Detection Prevalence : 0.5242          \n      Balanced Accuracy : 0.9485          \n                                          \n       'Positive' Class : real            \n                                          \n\n\nSlight improvement…"
  },
  {
    "objectID": "posts/football_fake_news/index.html#plot-important-words-for-classification",
    "href": "posts/football_fake_news/index.html#plot-important-words-for-classification",
    "title": "Real Vs. Fake news in football",
    "section": "Plot important words for classification",
    "text": "Plot important words for classification\n\n\nCode\nlr_summary &lt;- summary(lr_model) # summarizing the model\n\ncoefs &lt;- data.frame(lr_summary$estimated.feature.scores) # extracting coefficients\n\n\ncol_vec &lt;- c(\"#fc7753\", \"#e3e3e3\", \"#66d7d1\", \"black\")\n\ncoefs |&gt;\n  \n  # preparing df for plot\n  rownames_to_column(var = \"Token\") |&gt;\n  rename(Coefficient = real) |&gt;\n  filter(Coefficient != 0 & Token != \"(Intercept)\") |&gt;\n  mutate(bigger_then_0 = Coefficient &gt; 0) |&gt;\n  \n  # ggplotting\n  ggplot(aes(x = Token, y = Coefficient, color = bigger_then_0)) +\n  geom_point() +\n  scale_color_manual(values = c(col_vec[1], col_vec[3])) +\n  scale_y_continuous(n.breaks = 10) +\n  labs(title = \"Most important words for classifying if a tweet is fake news\",\n       x = \"\") +\n  theme_classic() +\n  geom_hline(yintercept = 0, color = \"black\", linetype = 2, linewidth = 1, show.legend = F) +\n  theme(axis.line = element_line(color = col_vec[4]),\n        axis.title = element_text(color = col_vec[4]),\n        axis.text = element_text(color = col_vec[4]),\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1, size = 12, face = \"bold\"),\n        legend.position = \"none\",\n        plot.title = element_text(size = 16, color = col_vec[4], hjust = .5, family = \"serif\", face = \"bold\"),\n        plot.background = element_rect(fill = \"#F9EBDE\"),\n        panel.background = element_rect(fill = \"#F9EBDE\"))"
  },
  {
    "objectID": "posts/netflix/index.html",
    "href": "posts/netflix/index.html",
    "title": "Netflix trends",
    "section": "",
    "text": "This post will be dedicated yet again to some experimentation with text analysis. I will be using mostly basic and classic NLP tools like word counting and DFM’s (Document-Feature matrix). I hope that in the next NLP post I will be using some more advanced methods like transformers (go BERT!).\nI will be using the popular Netflix dataset from Kaggle. It contains data of movies and TV shows added to Netflix’s streaming service since 2008.\nIt’s going to be a relatively short post, the first part will include some nice visualization (word clouds), and in the second part I will try and build a mini show-recommendation function."
  },
  {
    "objectID": "posts/netflix/index.html#text-analysis-of-show-description",
    "href": "posts/netflix/index.html#text-analysis-of-show-description",
    "title": "Netflix trends",
    "section": "Text analysis of show description",
    "text": "Text analysis of show description\nHere I’m creating the DFM. It is basically a huge matrix with each document (show description) being a row, and each unique word being a column. Values represent the frequency of each word in each document.\n\ndata_clean_dfm &lt;- data_clean |&gt;\n  mutate(description = str_remove_all(description, pattern = \"[[:punct:]]\")) |&gt;\n  corpus(text_field = \"description\") |&gt;\n  tokens(remove_separators = T) |&gt;\n  tokens_remove(stopwords()) |&gt;\n  dfm()\n\ndata_clean_dfm\n\nDocument-feature matrix of: 8,807 documents, 20,824 features (99.93% sparse) and 13 docvars.\n       features\ndocs    father nears end life filmmaker kirsten johnson stages death inventive\n  text1      1     1   1    1         1       1       1      1     1         1\n  text2      0     0   0    0         0       0       0      0     0         0\n  text3      0     0   0    0         0       0       0      0     0         0\n  text4      0     0   0    0         0       0       0      0     0         0\n  text5      0     0   0    1         0       0       0      0     0         0\n  text6      0     0   0    0         0       0       0      0     0         0\n[ reached max_ndoc ... 8,801 more documents, reached max_nfeat ... 20,814 more features ]\n\n\nFor example: The word ‘father’ appear once in the first document, and doesn’t appear in documents 2 to 6."
  },
  {
    "objectID": "posts/netflix/index.html#tf-idf-term-frequency---inverse-document-frequency",
    "href": "posts/netflix/index.html#tf-idf-term-frequency---inverse-document-frequency",
    "title": "Netflix trends",
    "section": "TF-IDF (Term Frequency - Inverse Document Frequency)",
    "text": "TF-IDF (Term Frequency - Inverse Document Frequency)\n\nTF - Term Frequency\nUp until now I have used the most basic DFM - the count DFM. It basically counts the number of times each word appear in the document, and is sometimes also called ‘Bag of Words’.\nOne problem that arises in our inner statistician is that longer documents contain more words! therefore, different term frequencies can be hard to compare. The simple solution is to calculate Proportional frequency in the following way:\n\\[\nterm \\ frequency= \\frac{N \\ times \\ a \\ word \\ appear \\ in \\ the \\ document}{Total \\ number \\ of \\ words \\ in \\ the \\ document}\n\\]\nThis gives us the Term frequency part of the tf-idf method.\n\n\nInverse Document Frequency\nSo we have the term frequency of every token (word). We can start and make show recommendations, right? Another problem that still exist is that some words could be prevalent in every document. For example, if the word ‘Life’ appears in a large number of descriptions, it is not so useful in identifying someones taste in TV shows and movies. If, on the other hand, the word ‘Father’ appear only in a small number of descriptions, it will be a lot more helpful to us. Therefore, we want to represent term Uniqueness. How can we do that?\nFor each token (word) the Inverse-Document-Frequency is calculated:\n\\[\nidf=log( \\frac{Total \\ number \\ of \\ documents}{N \\ documents \\ containing \\ the \\ term})\n\\]\nThis number is getting close to zero the more documents containing the word, and is getting larger the less documents contain it (and the more documents we have).\nTo get the final tf-idf value for every term, we simply multiply the Term-Frequency with the Inverse-Document-Frequency:\n\\[\ntfidf(word)=tf(word) \\cdot idf(word)\n\\]\nIn creating the tf-idf DFM, I have also converted every word to lower case and to it’s stem.\n\nnetflix_tf_idf &lt;- data_clean_dfm |&gt;\n  dfm_tolower() |&gt;\n  dfm_wordstem() |&gt;\n  dfm_tfidf()"
  },
  {
    "objectID": "posts/netflix/index.html#cosine-similarity",
    "href": "posts/netflix/index.html#cosine-similarity",
    "title": "Netflix trends",
    "section": "Cosine Similarity",
    "text": "Cosine Similarity\nNow that we have the DFM, we have a vector representation of every description of every show. Every word in our dataset is a dimension and it’s tf-idf in each document is the vector component. For example, the vector representation of the description of “Attack on Titan” is:\n\ndfm_subset(netflix_tf_idf, subset = title == \"Attack on Titan\")\n\nDocument-feature matrix of: 1 document, 15,097 features (99.90% sparse) and 13 docvars.\n         features\ndocs      father near end life filmmak kirsten johnson stage death invent\n  text779      0    0   0    0       0       0       0     0     0      0\n[ reached max_nfeat ... 15,087 more features ]\n\n\nNaturally these vector are mostly zeros…\nBut now that we have vectors we can do all the cool things that we can do with vectors! For example, we can recommend TV shows and movies based on how their vector representations are similar to a previously watched show. But how to calculate the similarity of two vectors?\nIf we return to Physics class, we can think of vectors as arrows with length and direction.\n\n\n\nTwo vectors\n\n\nIt is natural to think of the similarity between two vectors as the degree to which they point in the same direction. In other words, the angle between two vectors. It is possible to calculate the cosine between two vectors the following way:\n\\[\ncos \\theta= \\frac{ \\vec{a} \\cdot \\vec{b}}{|\\vec{a}| \\cdot |\\vec{b}|}\n\\]\nWithout turning this into a real Physics class, in the numerator is the dot-product of the vectors, and in the denominator is the product of lengths of the vectors. For example, the cosine between the vectors \\(\\vec{a}=(1,1)\\) and \\(\\vec{b}=(-4,3)\\) is:\n\\[\ncos \\theta= \\frac{1 \\cdot (-4)+1 \\cdot (3)}{ \\sqrt{1^2+1^2} \\cdot \\sqrt{(-4)^2+3^2}}=\\frac{-1}{5 \\sqrt{2}}\n\\]\nBut what is the angle between ~15,000 dimensions vectors?! fortunately, this calculation remains the same in any dimension.\nWhat is the relation between the angle and it’s cosine? without digging too deep, the larger the angle, the smaller the cosine. So the larger the cosine, the greater the similarity.\n\n\n\n\n\n\nNote\n\n\n\nNice insight: the cosine similarity is nothing but the un-standardized pearson correlation coefficient! Let \\(x\\) and \\(y\\) be vectors of Z scores. The correlation coefficient is:\n\\[\nr_{xy}= \\frac{\\sum_{i=1}^{N}{x_i \\cdot y_i}}{N}\n\\] While the cosine of the angle between them is:\n\\[\ncos \\theta= \\frac{\\vec{x} \\cdot \\vec{y}}{|\\vec{x}| \\cdot |\\vec{y}|}\n\\] The length of a vector of Z scores is \\(\\sqrt{N}\\), therefore the denominator is always: \\(\\sqrt{N} \\cdot \\sqrt{N}=N\\).\nThe nominator is the dot product of the vectors, which is exactly the sum of the by-component products. Finally we get:\n\\[\nr_{xy}=cos \\ \\theta_{xy}\n\\]\n\n\n\nFunctions\nThe main idea of the recommendation generator is simple. If you liked a TV show or a movie, you will probably like shows with similar description. Not any similarity, cosine similarity!\n\nget_recommendation &lt;- function(show, liked = T) {\n  library(dplyr)\n  library(quanteda)\n  \n  features &lt;- netflix_tf_idf@docvars\n  show_id &lt;- features$docname_[tolower(features$title) %in% tolower(show)]\n  show_id &lt;- as.integer(str_remove_all(show_id, pattern = \"text\"))\n  \n  simil_mat &lt;- textstat_simil(netflix_tf_idf[show_id,], netflix_tf_idf, method = \"cosine\")\n  \n  if (liked) {\n  simil_df &lt;- data.frame(shows = simil_mat@Dimnames[[2]],\n                      simil = simil_mat@x) |&gt;\n  arrange(-simil) |&gt;\n  inner_join(select(netflix_tf_idf@docvars, docname_, title),\n             by = join_by(shows == docname_)) |&gt;\n    select(-shows, match = simil) |&gt;\n    mutate(match = (match-min(match))/(max(match)-min(match))) |&gt;\n    head(11)\n  }\n  if (!liked){\n    simil_df &lt;- data.frame(shows = simil_mat@Dimnames[[2]],\n                      simil = simil_mat@x) |&gt;\n  arrange(simil) |&gt;\n  inner_join(select(netflix_tf_idf@docvars, docname_, title),\n             by = join_by(shows == docname_)) |&gt;\n    select(-shows, match = simil) |&gt;\n    mutate(match = 1-(match-min(match))/(max(match)-min(match))) |&gt;\n    head(11)\n  }\n  \n  return(simil_df[-1,])\n}\n\nget_recommendation_plot &lt;- function(show) {\n  library(ggplot2)\n  \n  plot_df &lt;- get_recommendation(show)\n  \n  ggplot(plot_df, aes(reorder(title, -match, identity), match, fill = match)) +\n  geom_col() +\n  labs(x = \"Show\", y = \"Match\") +\n  scale_fill_gradient(low = \"#000000\", high = \"#990011FF\") +\n  theme_classic() +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),\n        legend.position = \"none\",\n        plot.background = element_rect(fill = \"#F9EBDE\"),\n        panel.background = element_rect(fill = \"#F9EBDE\"))\n}\n\n\n\nGetting show recommendations\nLet’s use an example. Maybe I watched “Attack on Titan”, what are the top 10 recommended shows for me?\n\nwatched &lt;- \"Attack on Titan\"\n\nget_recommendation(watched)\n\n       match                               title\n2  0.1570728 Little Singham aur Kaal ka Mahajaal\n3  0.1444591                             The BFG\n4  0.1367644                    Sym-Bionic Titan\n5  0.1339566                           The Giant\n6  0.1335654                               Agent\n7  0.1227649                Pup Star: World Tour\n8  0.1207346              Power Battle Watch Car\n9  0.1159523                         Rising High\n10 0.1149643           League of Legends Origins\n11 0.1136349                A Sort of Homecoming\n\n\nWhat if I did not like a show? no problem, you will be recommended shows and movies that are not similar.\n\nget_recommendation(\"I Am Sam\", liked = FALSE)\n\n   match                               title\n2      1                           Ganglands\n3      1               Jailbirds New Orleans\n4      1                        Kota Factory\n5      1    My Little Pony: A New Generation\n6      1                             Sankofa\n7      1       The Great British Baking Show\n8      1                        The Starling\n9      1 Vendetta: Truth, Lies and The Mafia\n10     1                    Bangkok Breaking\n11     1    Confessions of an Invisible Girl\n\n\n\nIn a plot\n\nget_recommendation_plot(watched)\n\n\n\n\n\n\n\n\n\n\n\nInteractive app\nPlay around with show recommendations. and see other visualizations of the current data in this shiny app.\n\nShiny-Live\nUpdate from 11/3/24 - Apparently there is a new way of embedding shiny apps into quarto documents. The current app might not load for you as shinylive still struggles with heavy calculations…\n#| standalone: true\n#| code-fold: true\n#| cache: false\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"Test app\"),\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(inputId = \"watched_recommendation\",\n                  label = \"I watched...\",\n                  choices = data_clean$title,\n                  selected = data_clean$title[14])\n    ),\n    mainPanel(\n      plotOutput('plot1', height = \"350px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  show_watched &lt;- reactive({input$watched_recommendation})\n  \n  output$plot1 &lt;- renderPlot({\n  get_recommendation_plot(show_watched())\n  })\n}\n\nshinyApp(ui, server)\nIf my app doesn’t load for you, comfort yourself with this demo app from Posit:\n#| standalone: true\n#| viewerHeight: 650\n#| cache: false\nlibrary(shiny)\n\nvars &lt;- setdiff(names(iris), \"Species\")\n\nui &lt;- pageWithSidebar(\n  headerPanel('Iris k-means clustering'),\n  sidebarPanel(\n    selectInput('xcol', 'X Variable', vars),\n    selectInput('ycol', 'Y Variable', vars, selected = vars[[2]]),\n    numericInput('clusters', 'Cluster count', 3, min = 1, max = 9)\n  ),\n  mainPanel(\n    plotOutput('plot1')\n  )\n)\n\nserver &lt;- function(input, output, session) {\n\n  # Combine the selected variables into a new data frame\n  selectedData &lt;- reactive({\n    iris[, c(input$xcol, input$ycol)]\n  })\n\n  clusters &lt;- reactive({\n    kmeans(selectedData(), input$clusters)\n  })\n\n  output$plot1 &lt;- renderPlot({\n    palette(c(\"#E41A1C\", \"#377EB8\", \"#4DAF4A\", \"#984EA3\",\n      \"#FF7F00\", \"#FFFF33\", \"#A65628\", \"#F781BF\", \"#999999\"))\n\n    par(mar = c(5.1, 4.1, 0, 1))\n    plot(selectedData(),\n         col = clusters()$cluster,\n         pch = 20, cex = 3)\n    points(clusters()$centers, pch = 4, cex = 4, lwd = 4)\n  })\n\n}\n\nshinyApp(ui, server)"
  },
  {
    "objectID": "posts/ufo/index.html",
    "href": "posts/ufo/index.html",
    "title": "Who is the leading US state in UFO reports? probably not what you thought",
    "section": "",
    "text": "As I told in another post, I’ve recently enrolled to a course about Data-Viz (and maybe some NLP later-on, stay tuned). One of the assignments in the course was to make some TidyTuesday contribution and present it in class. Although this is not what I’ve presented in class, this was made shortly after and I think it came out quite nice :)"
  },
  {
    "objectID": "posts/ufo/index.html#initial-pre-processing",
    "href": "posts/ufo/index.html#initial-pre-processing",
    "title": "Who is the leading US state in UFO reports? probably not what you thought",
    "section": "Initial pre-processing",
    "text": "Initial pre-processing\n\nufo_sightings &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-06-25/ufo_sightings.csv\")\n\nusa &lt;- map_data(\"state\") # US map\n\nstate_codes &lt;- read_csv(\"state_code.csv\") %&gt;% # converting from state name to 2-letter code and back\n  select(state, code) %&gt;%\n  mutate(state = tolower(state), code = tolower(code))\n\nuspop &lt;- read_excel(\"uspop.xlsx\", col_names = c(\"region\", \"pop_2010\", \"pop_2011\", \"pop_2012\", \"pop_2013\", \"pop_2014\")) %&gt;% # info about state population\n  mutate(region = tolower(str_remove(region, \".\"))) %&gt;%\n  rowwise() %&gt;%\n  mutate(mean_pop = mean(c(pop_2010, pop_2011, pop_2012, pop_2013, pop_2014))) %&gt;%\n  ungroup() %&gt;%\n  select(region, mean_pop)\n\nUS population info taken from the United States Census Bureau."
  },
  {
    "objectID": "posts/ufo/index.html#glimpsing-at-the-data",
    "href": "posts/ufo/index.html#glimpsing-at-the-data",
    "title": "Who is the leading US state in UFO reports? probably not what you thought",
    "section": "Glimpsing at the data",
    "text": "Glimpsing at the data\n\nglimpse(ufo_sightings)\n\nRows: 80,332\nColumns: 11\n$ date_time                  &lt;chr&gt; \"10/10/1949 20:30\", \"10/10/1949 21:00\", \"10…\n$ city_area                  &lt;chr&gt; \"san marcos\", \"lackland afb\", \"chester (uk/…\n$ state                      &lt;chr&gt; \"tx\", \"tx\", NA, \"tx\", \"hi\", \"tn\", NA, \"ct\",…\n$ country                    &lt;chr&gt; \"us\", NA, \"gb\", \"us\", \"us\", \"us\", \"gb\", \"us…\n$ ufo_shape                  &lt;chr&gt; \"cylinder\", \"light\", \"circle\", \"circle\", \"l…\n$ encounter_length           &lt;dbl&gt; 2700, 7200, 20, 20, 900, 300, 180, 1200, 18…\n$ described_encounter_length &lt;chr&gt; \"45 minutes\", \"1-2 hrs\", \"20 seconds\", \"1/2…\n$ description                &lt;chr&gt; \"This event took place in early fall around…\n$ date_documented            &lt;chr&gt; \"4/27/2004\", \"12/16/2005\", \"1/21/2008\", \"1/…\n$ latitude                   &lt;dbl&gt; 29.88306, 29.38421, 53.20000, 28.97833, 21.…\n$ longitude                  &lt;dbl&gt; -97.941111, -98.581082, -2.916667, -96.6458…"
  },
  {
    "objectID": "posts/ufo/index.html#defining-functions",
    "href": "posts/ufo/index.html#defining-functions",
    "title": "Who is the leading US state in UFO reports? probably not what you thought",
    "section": "Defining functions",
    "text": "Defining functions\nSome helper function will help us later, mainly to work with dates.\nconvert_to_date takes a vector of character formatted dates and converts it to lubridate’s date format. floor_decade takes a vector of dates and converts it to a vector of decades.\n\nconvert_to_date &lt;- function(x) { \n  sub_string &lt;- str_sub(x, 1, 10)\n  d &lt;- mdy(sub_string)\n  return(as.numeric(d))\n}\nfloor_decade &lt;- function(x){\n  return(lubridate::year(x) - lubridate::year(x) %% 10)\n  }\n\n\nConverting the dates\n\nufo_sightings &lt;- ufo_sightings %&gt;%\n  mutate(date = as_date(purrr::map_dbl(date_time, ~convert_to_date(.)))) # Convert to 'Date' format. Run only once, its slow af"
  },
  {
    "objectID": "posts/ufo/index.html#globals",
    "href": "posts/ufo/index.html#globals",
    "title": "Who is the leading US state in UFO reports? probably not what you thought",
    "section": "Globals",
    "text": "Globals\nHere I’m loading some images and fonts that will be of use later to beautify the plot.\n\nnightsky_img &lt;- \"nightsky2.jpg\"\n\n#font_files() %&gt;% tibble() %&gt;% filter(str_detect(family, \"Showcard Gothic\"))\nfont_add(family = \"Showcard Gothic\", regular = \"SHOWG.TTF\")\nshowtext_auto()"
  },
  {
    "objectID": "posts/ufo/index.html#data-pre-processing",
    "href": "posts/ufo/index.html#data-pre-processing",
    "title": "Who is the leading US state in UFO reports? probably not what you thought",
    "section": "Data pre-processing",
    "text": "Data pre-processing\nPreparing the data for plotting:       1. Leaving only reports from the US.\n      2. Leaving only reports for continental US.\n      3. Selecting the relevant variables.\n      4. Calculating decades.\n\nufo &lt;- ufo_sightings %&gt;%\n  filter(country == \"us\") %&gt;% # Leaving only sightings in US\n  filter(!(state %in% c(\"ak\", \"pr\", \"hi\"))) %&gt;% # Only mainland US\n  select(date, code = state, description, encounter_length, latitude, longitude) %&gt;%\n  left_join(state_codes, by = \"code\") %&gt;%\n  mutate(decade = as.factor(purrr::map_dbl(date, ~floor_decade(.)))) %&gt;% # Create decade variable\n  drop_na(decade)\n\nAfter some experimenting I’ve decided to make a heat map to visualize the number of UFO reports per state. But first, some more data processing. I first counted the number of cases per state (by_state), and then combined it with the USA map data frame (by_state2).\n\nby_state &lt;- ufo %&gt;%\n    group_by(state, decade, .drop = F) %&gt;%\n    summarise(cases = n(),\n              .groups = \"drop\")\n  \nby_state2 &lt;- left_join(usa, by_state, by = c(\"region\" = \"state\"), multiple = \"all\") %&gt;%\n  filter(decade %in% c(2000, 2010)) %&gt;%\n    left_join(uspop, by = \"region\")\n\nNow I need to summarize the number of cases per state.\n\ncases_per_state &lt;- by_state2 %&gt;%\n  group_by(region) %&gt;%\n  summarise(cases = sum(cases), .groups = \"drop\")\n\nAnd finally merge it back together with the geographic data.\n\nby_state2 &lt;- left_join(by_state2, select(cases_per_state, region, cases_total = cases), by = \"region\")"
  },
  {
    "objectID": "posts/ufo/index.html#heatmap-1",
    "href": "posts/ufo/index.html#heatmap-1",
    "title": "Who is the leading US state in UFO reports? probably not what you thought",
    "section": "Heatmap 1",
    "text": "Heatmap 1\nAnd now for the heat map… drum roll\n\nheatmap &lt;- ggplot(by_state2, aes(x = long, y = lat, fill = cases_total, group = group)) +\n  geom_polygon(color = \"black\", show.legend = T) +\n  scale_fill_gradient(low = \"#ffae00\", high = \"#d90000\", limits = c(0, 2864832), breaks = c(0, 2850000)) +\n  coord_fixed(1.3, clip = \"off\") +\n  theme_classic() +\n  theme(axis.title = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.line = element_blank(),\n        plot.background = element_rect(fill = \"#F9EBDE\"),\n        panel.background = element_rect(fill = \"#F9EBDE\"),\n        legend.background = element_rect(fill = \"#F9EBDE\"),\n        plot.margin = unit(c(-11,-0.5,-11,-0.5), units = \"cm\"),\n        legend.margin = margin(0, 20, 0, 0))\n\nheatmap\n\n\n\n\n\n\n\n\nLooking at these results, I thought that of course California, Texas and Florida have the most reports, they also have the most people!\nA per capita measure will probably be more informative.\nCalculating reports per capita.\n\ncases_per_capita &lt;- by_state2 %&gt;%\n  group_by(region) %&gt;%\n  summarise(cases = sum(cases), .groups = \"drop\") %&gt;%\n  left_join(uspop, by = \"region\") %&gt;%\n  mutate(cases_per_capita = cases/mean_pop)\n\nMerging again with the geographical data.\n\nby_state2 &lt;- left_join(by_state2, select(cases_per_capita, region, cases_per_capita, cases_total = cases), by = \"region\")"
  },
  {
    "objectID": "posts/ufo/index.html#heatmap-2",
    "href": "posts/ufo/index.html#heatmap-2",
    "title": "Who is the leading US state in UFO reports? probably not what you thought",
    "section": "Heatmap 2",
    "text": "Heatmap 2\n\nheatmap2 &lt;- ggplot(by_state2, aes(x = long, y = lat, fill = cases_per_capita, group = group)) +\n  geom_polygon(color = \"black\", show.legend = T) +\n  scale_fill_gradient(low = \"#ffae00\", high = \"#d90000\", limits = c(0, 0.2), breaks = seq(0, 0.2, length.out = 6)) +\n  coord_fixed(1.3, clip = \"off\") +\n  theme_classic() +\n  theme(axis.title = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.line = element_blank(),\n        plot.background = element_rect(fill = \"#F9EBDE\"),\n        panel.background = element_rect(fill = \"#F9EBDE\"),\n        legend.background = element_rect(fill = \"#F9EBDE\"),\n        plot.margin = unit(c(-11,-0.5,-11,-0.5), units = \"cm\"),\n        legend.margin = margin(0, 20, 0, 0))\n\nheatmap2\n\n\n\n\n\n\n\n\nNice!"
  },
  {
    "objectID": "posts/ufo/index.html#heatmap3",
    "href": "posts/ufo/index.html#heatmap3",
    "title": "Who is the leading US state in UFO reports? probably not what you thought",
    "section": "Heatmap3",
    "text": "Heatmap3\nLet’s add some aesthetics because why not (I’ve only spent 6 hours on Google researching color theory and ggplot2’s internal logic). Unfold the code chunk if you are interested in seeing the monstrosity.\n\n\nCode\nheatmap3 &lt;- ggplot(by_state2, aes(x = long, y = lat, fill = cases_per_capita, group = group)) +\n  geom_polygon(color = \"#00670c\", show.legend = T) +\n  scale_fill_gradient(low = \"black\", high = \"#5dff00\", limits = c(0, 0.2), breaks = seq(0, 0.2, length.out = 6), guide = guide_colorbar(\"Number of reported cases per capita\", \n                                                                               title.position = \"top\",\n                                                                               title.theme = element_text(color = \"#5dff00\", family = \"serif\"),\n                                                                               title.hjust = 0.5,\n                                                                               barwidth = 30,\n                                                                               ticks.colour = NA)) +\n  labs(title = \"15 years of UFO sightings in the US between 2000 and 2014\",\n       caption = \"Tomer Zipori | #TidyTuesday | Source: National UFO Reporting Center\") +\n  coord_fixed(1.3, clip = \"off\") +\n  theme_minimal() +\n  annotate(\"label\", x = -130, y = 45.4, label = \"Washington is spooky!\\n # of cases: 1,228,975\\n Cases per capita: 0.18\",\n           color = \"#5dff00\", fill = \"black\", family = \"serif\", fontface = \"bold\") +\n  geom_curve(aes(x = -127.5, y = 46.4, xend = -124.48, yend = 47.4), color = \"#5dff00\", linewidth = 1, curvature = -0.35,\n             arrow = arrow(type = \"closed\", length = unit(0.02, \"npc\"))) +\n  annotate(\"label\", x = -124, y = 30.4, label = \"Utah has the lowest rate in the US\\n # of cases: 22,715\\n Cases per capita: 0.0079\",\n           color = \"#5dff00\", fill = \"black\", family = \"serif\", fontface = \"bold\") +\n  geom_curve(aes(x = -119.4, y = 31.4, xend = -111.5, yend = 39), color = \"#5dff00\", linewidth = 1, curvature = 0.3,\n             arrow = arrow(type = \"closed\", length = unit(0.02, \"npc\"))) +\n  theme(plot.title = element_text(size = 24, vjust = -4, hjust = 0.5, color = \"#5dff00\", family = \"Showcard Gothic\"),\n        plot.caption = element_text(color = \"#5dff00\", hjust = 1.05, family = \"serif\", size = 9),\n        axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        panel.grid = element_blank(),\n        legend.position = \"bottom\", legend.box = \"horizontal\", legend.text = element_text(color = \"#5dff00\", family = \"mono\", size = 14))\n\nheatmap3 &lt;- ggbackground(heatmap3, nightsky_img)"
  },
  {
    "objectID": "posts/p_values_paradox/p_values_paradox.html",
    "href": "posts/p_values_paradox/p_values_paradox.html",
    "title": "Rejecting the null hypothesis or not?",
    "section": "",
    "text": "In the current post I will try and explain what appears to be a paradox: given the identical data, different schools of statistics disagree about the appropriate conclusion.\nTraditionally, there are two ways to interpret Probability:\n1. Probability is the relative frequency of events at infinity - the Frequentist school.\n2. Probability represents the degree of beliefs - The Bayesian school.\nClassic hypothesis testing in the social sciences usually follows the Frequentist school, the probability of errors is calculated as the relative frequency of extreme data under the relevant hypotheses."
  },
  {
    "objectID": "posts/p_values_paradox/p_values_paradox.html#theoretical-distributions",
    "href": "posts/p_values_paradox/p_values_paradox.html#theoretical-distributions",
    "title": "Rejecting the null hypothesis or not?",
    "section": "Theoretical distributions",
    "text": "Theoretical distributions\n\ndata.frame(x = c(-1, 1.8)) |&gt;\n  ggplot(aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = 0, sd = 1/sqrt(n)), geom = \"area\", alpha = 0.76) +\n  stat_function(fun = dnorm, args = list(mean = 1.0376218, sd = 1/sqrt(n)), geom = \"area\", fill = \"gray\", alpha = 0.76) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"#FF6666\") +\n  geom_vline(xintercept = 1.0376218, linetype = \"dashed\", color = \"#FF6666\") +\n  scale_x_continuous(breaks = seq(-1, 2, 0.2)) +\n  theme_classic() +\n  labs(x = \"\", y = \"\") +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())\n\n\n\n\n\nn &lt;- 30\nmu0 &lt;- 0\nsd0 &lt;- 1\nmu1 &lt;- 1.0376218\n\np_values_99 &lt;- rep(NA, 10000)\n\nfor (i in c(1:10000)) {\n  group1 &lt;- rnorm(n, mu0, sd0)\n  group2 &lt;- rnorm(n, mu1, sd0)\n  \n  test &lt;- t.test(group1, group2, alternative = \"less\")\n  \n  p_values_99[i] &lt;- test$p.value\n}\n\nlength(p_values_99[p_values_99 &lt; .05])/length(p_values_99)\n\n[1] 0.9894\n\n\n\ndata.frame(\"p\" = p_values_99) |&gt;\nggplot(aes(x = p)) +\n  geom_density(alpha = .6, fill = \"#FF6666\") +\n  geom_area(aes(x = stage(p, after_stat = oob_censor(x, c(0.03, 0.04)))), stat = \"density\", fill = \"gray\") +\n  geom_vline(xintercept = 0.03, color = \"black\", linewidth = 0.7) +\n  geom_vline(xintercept = 0.04, color = \"black\", linewidth = 0.7) +\n  scale_x_continuous(breaks = seq(0, 1, 0.05), limits = c(0, 1)) +\n  theme_classic() +\n  labs(y = \"\", x = \"P-Value\", title = \"Density distribution of p-values when the effect size is 1.03\",\n       subtitle = \"Gray area represents the probability of p between 0.03 and 0.04\") +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        plot.title = element_text(family = \"serif\", size = 16, hjust = 0.5),\n        plot.subtitle = element_text(family = \"serif\", size = 13, hjust = 0.5))"
  },
  {
    "objectID": "posts/p_values_paradox/p_values_paradox.html#distribution-of-p-values-with-80-power",
    "href": "posts/p_values_paradox/p_values_paradox.html#distribution-of-p-values-with-80-power",
    "title": "Rejecting the null hypothesis or not?",
    "section": "Distribution of p-values with 80% power",
    "text": "Distribution of p-values with 80% power\n\nn &lt;- 30\nmu0 &lt;- 0\nsd0 &lt;- 1\nmu1 &lt;- 0.65\n\np_values_80 &lt;- rep(NULL, 140000)\n\nfor (i in c(1:140000)) {\n  group1 &lt;- rnorm(n, mu0, sd0)\n  group2 &lt;- rnorm(n, mu1, sd0)\n  \n  test &lt;- t.test(group1, group2, alternative = \"less\")\n  \n  p_values_80[i] &lt;- test$p.value\n}\n\nWhat is the probability of getting a significant result?\n\nlength(p_values_80[p_values_80 &lt; .05])/length(p_values_80)\n\n[1] 0.80015\n\n\nHow does the distribution of p-values look when there is 80% Power (probability of 0.8 of getting a significant result)?\n\nhist(p_values_80, breaks = 50, xlab = NULL, freq = F)\n\n\n\n\nSmaller p-values are more probable, and the uniform distribution from before “gets skewed”."
  },
  {
    "objectID": "posts/p_values_paradox/p_values_paradox.html#likelihood",
    "href": "posts/p_values_paradox/p_values_paradox.html#likelihood",
    "title": "Rejecting the null hypothesis or not?",
    "section": "Likelihood",
    "text": "Likelihood\nWhat is the probability of p-values given each hypothesis? we actually answered this question earlier when we visualized the density distributions of p-values.\nFor example, the probability of observing a p-value between 0.03 and 0.04 given the null hypothesis is:\n\ndata.frame(\"p\" = p_values_null) |&gt;\nggplot(aes(x = p)) +\n  geom_density(alpha = .6, fill = \"#FF6666\") +\n  geom_area(aes(x = stage(p, after_stat = oob_censor(x, c(0.03, 0.04)))), stat = \"density\", fill = \"gray\") +\n  annotate(\"text\", x = 0.14, y = 0.5, label = \"italic(p) == 0.0099\",\n  parse = TRUE) +\n  geom_vline(xintercept = 0.03, color = \"black\", linewidth = 0.7) +\n  geom_vline(xintercept = 0.04, color = \"black\", linewidth = 0.7) +\n  scale_x_continuous(breaks = seq(0, 1, 0.05)) +\n  theme_classic() +\n  labs(y = \"\", x = \"P-Value\", title = \"Density distribution of p-values under the null hypothesis\",\n       subtitle = \"Gray area represents the probability of p between 0.03 and 0.04\") +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        plot.title = element_text(family = \"serif\", size = 16, hjust = 0.5),\n        plot.subtitle = element_text(family = \"serif\", size = 13, hjust = 0.5))\n\n\n\n\nThe probability of observing the same range of p-values when the effect size is \\(0.65\\) is:\n\ndata.frame(\"p\" = p_values_80) |&gt;\nggplot(aes(x = p)) +\n  geom_density(alpha = .6, fill = \"#FF6666\") +\n  geom_area(aes(x = stage(p, after_stat = oob_censor(x, c(0.03, 0.04)))), stat = \"density\", fill = \"gray\") +\n  annotate(\"text\", x = 0.14, y = 8, label = \"italic(p) == 0.0437\",\n  parse = TRUE) +\n  geom_vline(xintercept = 0.03, color = \"black\", linewidth = 0.7) +\n  geom_vline(xintercept = 0.04, color = \"black\", linewidth = 0.7) +\n  scale_x_continuous(breaks = seq(0, 1, 0.05)) +\n  theme_classic() +\n  labs(y = \"\", x = \"P-Value\", title = \"Density distribution of p-values when the effect size is 0.65\",\n       subtitle = \"Gray area represents the probability of p between 0.03 and 0.04\") +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        plot.title = element_text(family = \"serif\", size = 16, hjust = 0.5),\n        plot.subtitle = element_text(family = \"serif\", size = 13, hjust = 0.5))\n\n\n\n\nAnd for effect size of \\(1.03\\)?\n\ndata.frame(\"p\" = p_values_99) |&gt;\nggplot(aes(x = p)) +\n  geom_density(alpha = .6, fill = \"#FF6666\") +\n  geom_area(aes(x = stage(p, after_stat = oob_censor(x, c(0.03, 0.04)))), stat = \"density\", fill = \"gray\") +\n  annotate(\"text\", x = 0.14, y = 376, label = \"italic(p) == 0.0056\",\n  parse = TRUE) +\n  geom_vline(xintercept = 0.03, color = \"black\", linewidth = 0.7) +\n  geom_vline(xintercept = 0.04, color = \"black\", linewidth = 0.7) +\n  scale_x_continuous(breaks = seq(0, 1, 0.05), limits = c(0, 1)) +\n  theme_classic() +\n  labs(y = \"\", x = \"P-Value\", title = \"Density distribution of p-values when the effect size is 1.03\",\n       subtitle = \"Gray area represents the probability of p between 0.03 and 0.04\") +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        plot.title = element_text(family = \"serif\", size = 16, hjust = 0.5),\n        plot.subtitle = element_text(family = \"serif\", size = 13, hjust = 0.5))\n\n\n\n\nThe probability of getting a p-value in the range of \\([0.03,0.04]\\) is lower for greater effect sizes."
  },
  {
    "objectID": "posts/p_values_paradox/index.html",
    "href": "posts/p_values_paradox/index.html",
    "title": "Rejecting the null hypothesis or not?",
    "section": "",
    "text": "In the current post I try to explain what appears to be a paradox: given identical data, different schools of statistics disagree about the appropriate conclusion.\nTraditionally, there are two ways to interpret Probability:\n1. Probability is the relative frequency of events at infinity - the Frequentist school.\n2. Probability represents the degree of beliefs - The Bayesian school.\nClassic hypothesis testing in the social sciences usually follows the Frequentist school, the probability of errors is calculated as the relative frequency of extreme data under the relevant hypotheses."
  },
  {
    "objectID": "posts/p_values_paradox/index.html#distribution-of-p-values-with-80-power",
    "href": "posts/p_values_paradox/index.html#distribution-of-p-values-with-80-power",
    "title": "Rejecting the null hypothesis or not?",
    "section": "Distribution of p-values with 80% power",
    "text": "Distribution of p-values with 80% power\n\nn &lt;- 30\nmu0 &lt;- 0\nsd0 &lt;- 1\nmu1 &lt;- 0.65\n\np_values_80 &lt;- rep(NULL, 140000)\n\nfor (i in c(1:140000)) {\n  group1 &lt;- rnorm(n, mu0, sd0)\n  group2 &lt;- rnorm(n, mu1, sd0)\n  \n  test &lt;- t.test(group1, group2, alternative = \"less\")\n  \n  p_values_80[i] &lt;- test$p.value\n}\n\nWhat is the probability of getting a significant result?\n\nlength(p_values_80[p_values_80 &lt; .05])/length(p_values_80)\n\n[1] 0.80015\n\n\nHow does the distribution of p-values look when there is 80% Power (probability of 0.8 of getting a significant result)?\n\npar(bg = '#F9EBDE')\n\nhist(p_values_80, breaks = 50, xlab = NULL, freq = F)\n\n\n\n\n\n\n\n\nSmaller p-values are more probable, and the uniform distribution from before “gets skewed”."
  },
  {
    "objectID": "posts/p_values_paradox/index.html#likelihood",
    "href": "posts/p_values_paradox/index.html#likelihood",
    "title": "Rejecting the null hypothesis or not?",
    "section": "Likelihood",
    "text": "Likelihood\nWhat is the probability of p-values given each hypothesis? we actually answered this question earlier when we visualized the density distributions of p-values.\nFor example, the probability of observing a p-value between 0.03 and 0.04 given the null hypothesis is:\n\ndata.frame(\"p\" = p_values_null) |&gt;\nggplot(aes(x = p)) +\n  geom_density(alpha = .6, fill = \"#815854\") +\n  geom_area(aes(x = stage(p, after_stat = oob_censor(x, c(0.03, 0.04)))), stat = \"density\", fill = \"gray\") +\n  annotate(\"text\", x = 0.14, y = 0.5, label = \"italic(p) == 0.0099\",\n  parse = TRUE) +\n  geom_vline(xintercept = 0.03, color = \"black\", linewidth = 0.7) +\n  geom_vline(xintercept = 0.04, color = \"black\", linewidth = 0.7) +\n  scale_x_continuous(breaks = seq(0, 1, 0.05)) +\n  theme_classic() +\n  labs(y = \"\", x = \"P-Value\", title = \"Density distribution of p-values under the null hypothesis\",\n       subtitle = \"Gray area represents the probability of p between 0.03 and 0.04\") +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        plot.title = element_text(family = \"serif\", size = 16, hjust = 0.5),\n        plot.subtitle = element_text(family = \"serif\", size = 13, hjust = 0.5),\n        plot.background = element_rect(fill = \"#F9EBDE\"),\n        panel.background = element_rect(fill = \"#F9EBDE\"))\n\n\n\n\n\n\n\n\nThe probability of observing the same range of p-values when the effect size is \\(0.65\\) is:\n\ndata.frame(\"p\" = p_values_80) |&gt;\nggplot(aes(x = p)) +\n  geom_density(alpha = .6, fill = \"#815854\") +\n  geom_area(aes(x = stage(p, after_stat = oob_censor(x, c(0.03, 0.04)))), stat = \"density\", fill = \"gray\") +\n  annotate(\"text\", x = 0.14, y = 8, label = \"italic(p) == 0.0437\",\n  parse = TRUE) +\n  geom_vline(xintercept = 0.03, color = \"black\", linewidth = 0.7) +\n  geom_vline(xintercept = 0.04, color = \"black\", linewidth = 0.7) +\n  scale_x_continuous(breaks = seq(0, 1, 0.05)) +\n  theme_classic() +\n  labs(y = \"\", x = \"P-Value\", title = \"Density distribution of p-values when the effect size is 0.65\",\n       subtitle = \"Gray area represents the probability of p between 0.03 and 0.04\") +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        plot.title = element_text(family = \"serif\", size = 16, hjust = 0.5),\n        plot.subtitle = element_text(family = \"serif\", size = 13, hjust = 0.5),\n        plot.background = element_rect(fill = \"#F9EBDE\"),\n        panel.background = element_rect(fill = \"#F9EBDE\"))\n\n\n\n\n\n\n\n\nAnd for effect size of \\(1.03\\)?\n\ndata.frame(\"p\" = p_values_99) |&gt;\nggplot(aes(x = p)) +\n  geom_density(alpha = .6, fill = \"#815854\") +\n  geom_area(aes(x = stage(p, after_stat = oob_censor(x, c(0.03, 0.04)))), stat = \"density\", fill = \"gray\") +\n  annotate(\"text\", x = 0.14, y = 376, label = \"italic(p) == 0.0056\",\n  parse = TRUE) +\n  geom_vline(xintercept = 0.03, color = \"black\", linewidth = 0.7) +\n  geom_vline(xintercept = 0.04, color = \"black\", linewidth = 0.7) +\n  scale_x_continuous(breaks = seq(0, 1, 0.05), limits = c(0, 1)) +\n  theme_classic() +\n  labs(y = \"\", x = \"P-Value\", title = \"Density distribution of p-values when the effect size is 1.03\",\n       subtitle = \"Gray area represents the probability of p between 0.03 and 0.04\") +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        plot.title = element_text(family = \"serif\", size = 16, hjust = 0.5),\n        plot.subtitle = element_text(family = \"serif\", size = 13, hjust = 0.5),\n        plot.background = element_rect(fill = \"#F9EBDE\"),\n        panel.background = element_rect(fill = \"#F9EBDE\"))\n\n\n\n\n\n\n\n\nThe probability of getting a p-value in the range of \\([0.03,0.04]\\) is lower for greater effect sizes."
  },
  {
    "objectID": "posts/bayes101/index.html",
    "href": "posts/bayes101/index.html",
    "title": "Bayesian Modeling for Psychologists, Part 1",
    "section": "",
    "text": "Loading some packages that will be used for demonstrations and analysis:\n\nlibrary(tidyverse)      # Data wrangling and plotting\nlibrary(ggdist)         # Easy and aesthetic plotting of distributions\nlibrary(ggExtra)        # Adding marginal distributions for 2d plots\nlibrary(tidybayes)      # Tidy processing of Bayesian models and extraction of MCMC chains\nlibrary(brms)           # Bayesian modeling and posterior estimation with MCMC using Stan\nlibrary(distributional) # Plotting of theoretical distributions (for likelihood functions plots)"
  },
  {
    "objectID": "posts/bayes101/index.html#bayes-theorem",
    "href": "posts/bayes101/index.html#bayes-theorem",
    "title": "Bayesian Modeling for Psychologists, Part 1",
    "section": "Bayes theorem",
    "text": "Bayes theorem\n\nDiscrete events\nSo what is Bayesian about Bayesian statistics? going back to Intro to statistics in your BA, you learned to derive the Bayes theorem in probability: \\[\nP(A|B)=\\frac{P(B|A) \\cdot P(A)}{P(B)}\n\\]\nIn plain words: the probability of \\(A\\) conditional on \\(B\\) is equal to the product of the probability of \\(B\\) conditional on \\(A\\) and the prior probability of \\(A\\), divided by the probability of \\(B\\). The Bayes theorem gives a rigorous and mathematically defined way of updating beliefs in face of observed data. Sounds familiar? it should! this is exactly what you are doing (or at least want to do) in your thesis/research! you observe some Data and try to learn from it about the World. In your case, World = some statistical model = some set of parameters.1\n\n\nProbability Distributions\nSo what is the probability of your hypothesized set of parameters given the data you observed? Replacing \\(A\\) with your set of parameters, and \\(B\\) with the observed data we get:\n\\[\nP(Parameters|Data)=\\frac{P(Data|Parameters) \\cdot P(Parameters)}{P(Data)}\n\\]\nOne thing that we need to remember is almost all2 of the terms above are no longer single probability values, but continuous probability distributions. For example, the term \\(P(Parameters)\\) is a probability density distribution in which each possible combination of values of your parameters is assigned a probability.\nEach of the terms above has an important role in the world of Bayesian modeling.\n\n\nLikelihood\nThe term \\(P(Data|Parameters)\\) describes the likelihood of the observed data under different sets of parameter values. In order to keep things simple I will not delve deeper into the theoretical aspects of likelihood (see Etz (2018), Pawitan (2001)). The main things that are important to know as researchers about the likelihood function is that you are responsible to choosing it! For example, if I suspect that my dependent variable comes from a normal distribution (where you otherwise would fit some sort of linear model), I will use the normal likelihood function3:\n\n\nggplot code\nggplot(data = data.frame(x = c(-4, 4)), aes(x)) +\n  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1), linewidth = 1, color = \"#fca903\") +\n  ylab(expression(L(theta,x))) +\n  xlab(expression(theta)) +\n  scale_y_continuous(breaks = NULL) +\n  scale_x_continuous(breaks = seq(-4, 4, 1), labels = NULL) +\n  theme_classic() +\n  theme(axis.title = element_text(size = 13),\n        plot.background = element_rect(fill = \"#E3E2DF\"),\n        panel.background = element_rect(fill = \"#E3E2DF\"))\n\n\n\n\n\n\n\n\n\nIf, on the other hand, I suspect that my dependent variable comes from a Poisson distribution (e.g. number of times a participant scratched their nose), I will use the Poisson likelihood function:\n\n\nggplot code\nggplot(data = data.frame(x = seq(0, 16, 1)), aes(x)) +\n  geom_line(aes(y = dpois(x, 6)), linewidth = 1, color = \"#fca903\") +\n  ylab(expression(L(theta,x))) +\n  xlab(expression(theta)) +\n  scale_y_continuous(breaks = NULL) +\n  scale_x_continuous(breaks = seq(0, 16, 1), labels = NULL) +\n  theme_classic() +\n  theme(axis.title = element_text(size = 13),\n        plot.background = element_rect(fill = \"#E3E2DF\"),\n        panel.background = element_rect(fill = \"#E3E2DF\"))\n\n\n\n\n\n\n\n\n\nIn general, you need to define the Data Generating Process (DGP) - How were your observations generated?\nThis choice is analogous to the choice of statistical model/test in the frequentist paradigm. A linear model (t-test, ANOVA, linear regression) assumes a normal DGP, a Generalized linear model assumes some other DGP like a Binomial, Poisson, Inverse Gaussian etc.\n\n\nPrior\nThe next term - \\(P(Parameters)\\) is called the Prior probability distribution. This term represents the prior knowledge about the parameters in your model. Choosing a prior, a process that is sometime called prior elicitation, is difficult. How can one take into account all prior knowledge about something and neatly represent it with one probability distribution?? For this reason I will refer to 2 relatively-easy-to-define aspects of prior elicitation:\n\n\nRange of possible/probable parameter values - Some parameters can only take certain values, creating a-priori hard limits for their value. For example, a normal distribution’s variance \\(\\sigma^2\\) can only take positive values. Softer limits can also exist, for example: in a new/old recognition task, it is reasonable to predict that new items will be identified as new, more than old items - the regression coefficient should be positive. In the first case, a prior with hard boundary will be appropriate:\n\n\n\nggplot code\nggplot(data = data.frame(x = c(0, 13)), aes(x)) +\n  stat_function(fun = dexp, n = 101, args = list(rate = 1), linewidth = 1, fill = \"#a4c8f5\", color = \"#0373fc\", geom = \"area\", outline.type = \"upper\") +\n  ylab(expression(density(theta))) +\n  xlab(expression(theta)) +\n  scale_y_continuous(breaks = NULL) +\n  scale_x_continuous(breaks = seq(0, 13, 1)) +\n  labs(title = expression(Exp(1)), subtitle = \"Values below 0 are assigned a probability of 0\") +\n  theme_classic() +\n  theme(plot.title = element_text(size = 17, family = \"serif\", hjust = 0.5),\n        plot.subtitle = element_text(size = 13, family = \"serif\", hjust = 0.5),\n        axis.title = element_text(size = 13),\n        plot.background = element_rect(fill = \"#E3E2DF\"),\n        panel.background = element_rect(fill = \"#E3E2DF\"))\n\n\n\n\n\n\n\n\n\nWhile in the second case, a prior like this would be better:\n\n\nggplot code\nggplot(data = data.frame(x = c(-5, 9)), aes(x)) +\n  stat_function(fun = dnorm, n = 101, args = list(mean = 2, sd = 1.5), linewidth = 1, fill = \"#a4c8f5\", color = \"#0373fc\", geom = \"area\", outline.type = \"upper\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\", linewidth = 0.5) +\n  ylab(expression(density(theta))) +\n  xlab(expression(theta)) +\n  scale_y_continuous(breaks = NULL) +\n  scale_x_continuous(breaks = seq(-5, 9, 1)) +\n  labs(title = expression(N(2, 1.5^2)), subtitle = \"Negative values are possible, yet less probable than positive values\") +\n  theme_classic() +\n  theme(plot.title = element_text(size = 17, family = \"serif\", hjust = 0.5),\n        plot.subtitle = element_text(size = 13, family = \"serif\", hjust = 0.5),\n        axis.title = element_text(size = 13),\n        plot.background = element_rect(fill = \"#E3E2DF\"),\n        panel.background = element_rect(fill = \"#E3E2DF\"))\n\n\n\n\n\n\n\n\n\n\nDegree of informativeness - Priors also differ from each other in their impact on the model. A prior can be weak or strong in this aspect.\n\nWeak Priors - These priors are typically wide and cover a large range of possible values.\nStrong Priors - These priors are narrower and leaves few parameter values probable.\n\n\n\nggplot code\nggplot(data = data.frame(x = c(-28, 28)), aes(x)) +\n  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 8), linewidth = 1, fill = \"#a4c8f5\", color = \"#0373fc\", geom = \"area\", outline.type = \"upper\") +\n  ylab(expression(density(theta))) +\n  xlab(expression(theta)) +\n  scale_y_continuous(breaks = NULL) +\n  scale_x_continuous(breaks = seq(-28, 28, 2)) +\n  labs(title = expression(N(0, 8^2))) +\n  theme_classic() +\n  theme(plot.title = element_text(size = 17, family = \"serif\", hjust = 0.5),\n        axis.title = element_text(size = 13),\n        plot.background = element_rect(fill = \"#E3E2DF\"),\n        panel.background = element_rect(fill = \"#E3E2DF\"))\nggplot(data = data.frame(x = c(-28, 28)), aes(x)) +\n  stat_function(fun = dt, args = list(df = 10), linewidth = 1, fill = \"#a4c8f5\", color = \"#0373fc\", geom = \"area\", outline.type = \"upper\") +\n  ylab(expression(density(theta))) +\n  xlab(expression(theta)) +\n  scale_y_continuous(breaks = NULL) +\n  scale_x_continuous(breaks = seq(-28, 28, 2)) +\n  labs(title = expression(students_t(df = 10))) +\n  theme_classic() +\n  theme(plot.title = element_text(size = 17, family = \"serif\", hjust = 0.5),\n        axis.title = element_text(size = 13),\n        plot.background = element_rect(fill = \"#E3E2DF\"),\n        panel.background = element_rect(fill = \"#E3E2DF\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvidence\nThe denominator in Bayes’ theorem - \\(P(Data)\\) is called the evidence. It’s important role is to act as the standardizing term to the numerator (\\(Likelihood \\cdot Prior\\)). Recall that the likelihood is not a probability distribution, so in order for it’s product with the Prior - a probability distribution - to be equal to the Posterior - another probability distribution - a standardizing term is needed. This term is often impossible to calculate analytically, and therefore fancy numeric estimations are used.4\n\n\nPosterior\nThe posterior probability of your model - \\(P(Parameters|Data)\\) - is the ultimate goal of your analysis. This probability distribution tells you what values of your parameters are probable and what values not so much. This is a shift of perspective from the usual frequentist logic - instead of resulting in one number as estimate for your parameter, you end up with a whole distribution!\n\n\n\nggplot code\nset.seed(14)\n\nmeans &lt;- rnorm(10000, -2, 1.5)\nfreq_estimate &lt;- mean(means)\n\nggplot(data.frame(y = means), aes(x = y)) +\n  geom_density(fill = \"#2ea64e\", color = \"#145726\", linewidth = 1) +\n  geom_point(data = data.frame(x = freq_estimate, y = 0), aes(x = x, y = y), size = 3) +\n  scale_x_continuous(limits = c(-7, 3), breaks = seq(-7, 3, 1), labels = seq(-7, 3, 1)) +\n  theme_classic() +\n  labs(title = \"Posterior probability distribution\", subtitle = \"The black dot represent the frequentist point estimate of the parameter\", x = expression(theta)) +\n  theme(axis.title.y = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        plot.title = element_text(size = 17, family = \"serif\", hjust = .5),\n        plot.subtitle = element_text(size = 13, family = \"serif\", hjust = .5),\n        axis.title.x = element_text(size = 13),\n        plot.background = element_rect(fill = \"#E3E2DF\"),\n        panel.background = element_rect(fill = \"#E3E2DF\"))\n\n\n\n\n\n\n\n\n\nI think that Getting a posterior from the Likelihood and the prior makes more sense visually (make sure to also play with Kristoffer Magnusson’s wonderful interactive visualization):\n\n\nsimulated parameters\nset.seed(14)\n\nactual_lambda &lt;- 44\nn &lt;- 24\ndata &lt;- rpois(n, actual_lambda)\nprior_alpha &lt;- 122.5\nprior_beta &lt;- 3.5\n\nposterior_alpha &lt;- prior_alpha + sum(data)\nposterior_beta &lt;- prior_beta + n\n\n\n\n\nggplot code\npois_likelihood &lt;- function(theta, data, scale = 1) {\n  sapply(theta, function(.theta) {\n    scale * exp(sum(dpois(data, lambda = .theta, log = T)))\n  })\n}\n\ndists &lt;- data.frame(dist = c(\"Prior\", \"Posterior\"),\n                    Alpha = c(prior_alpha, posterior_alpha),\n                    Beta = c(prior_beta, posterior_beta)) |&gt;\n  mutate(xdist = dist_gamma(shape = Alpha, rate = Beta))\n\nggplot(dists) +\n  geom_rug(aes(x = data, color = \"Likelihood\", fill = \"Likelihood\"), data = data.frame(), linewidth = 2) +\n  stat_slab(aes(xdist = xdist, fill = dist), normalize = \"none\", alpha = 0.6) +\n  stat_function(aes(color = \"Likelihood\"), fun = pois_likelihood,\n                args = list(data = data, scale = 7e33),\n                geom = \"line\", linewidth = 1) + \n  scale_fill_manual(breaks = c(\"Prior\", \"Likelihood\", \"Posterior\"), \n                    values = c(\"#a4c8f5\", \"#fca903\", \"#2ea64e\")) +\n  scale_color_manual(breaks = c(\"Prior\", \"Likelihood\", \"Posterior\"), \n                    values = c(\"black\", \"#fca903\", \"black\"),\n                    aesthetics = \"color\") +\n  coord_cartesian(ylim = c(0, 0.45), xlim = c(20, 70)) +\n  guides(colour = \"none\", fill = guide_legend(title = \"\")) +\n  theme_classic() +\n  labs(title = \"Posterior as a compromise between likelihood and prior\",\n       subtitle = \"Orange dots represent observed data\") +\n  theme(axis.title = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks.y = element_blank(),\n        plot.title = element_text(size = 17, family = \"serif\", hjust = .5),\n        plot.subtitle = element_text(size = 13, family = \"serif\", hjust = .5),\n        plot.background = element_rect(fill = \"#E3E2DF\"),\n        panel.background = element_rect(fill = \"#E3E2DF\"),\n        legend.background = element_rect(fill = \"#E3E2DF\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA note on higher dimesions\n\n\n\nAll of the examples above refer to estimation of one parameter. Often, your models will include several parameters (100’s if you include random effects). So how does it work?\nwithout getting into detail, know that the simple one-parameter case scales quite nicely into n-parameter models. For example, instead of the \\(P(\\theta)\\) prior from before, you can have \\(P(\\theta_0, \\theta_1,...,\\theta_n)\\) prior. A prior of an intercept and a slope for a simple regression model could look like this:\n\n\n2d ggplot code\nset.seed(14)\n\nmu0 &lt;- -2\nmu1 &lt;- 2.5\nsd0 &lt;- 1.5\nsd1 &lt;- 2\nr &lt;- -0.54\n\ndata &lt;- MASS::mvrnorm(n = 10000, mu = c(mu0, mu1), Sigma = matrix(c(sd0^2, sd0*sd1*r, sd0*sd1*r, sd1^2), nrow = 2)) |&gt;\n  data.frame() |&gt;\n  rename(b0 = X1, b1 = X2) |&gt;\n  mutate(b0 = b0 + runif(1, -4, 4)^2,\n         b1 = b1 + runif(1, -1, 1)^2)\n\np &lt;- data |&gt;\n  ggplot(aes(x = b0, y = b1)) +\n    geom_point() +\n    stat_density_2d(aes(fill = after_stat(density)), geom = \"raster\", contour = F) +\n    theme_classic() +\n    labs(x = expression(beta[0]), y = expression(beta[1])) +\n    scale_x_continuous(expand = c(0, 0), limits = c(-6, 12), breaks = seq(-6, 12, 2)) +\n    scale_y_continuous(expand = c(0, 0), limits = c(-6, 12), breaks = seq(-6, 12, 2)) +\n    scale_fill_fermenter(palette = \"Blues\", direction = 1) +\n    theme(legend.position = \"none\",\n          axis.title = element_text(size = 13))\n\nggMarginal(p, type = \"density\",\n           xparams = list(fill = \"#a4c8f5\", colour = \"black\"),\n           yparams = list(fill = \"#a4c8f5\", colour = \"black\"))\n\n\n\n\n\n\n\n\n\nIn general, we will denote \\(\\theta\\) to be the set of m parameters \\([\\theta_0, \\theta_1,...,\\theta_m]\\), and \\(X\\) as the set of n data points \\([x_1, x_2,...,x_n]\\)."
  },
  {
    "objectID": "posts/bayes101/index.html#brms-setup",
    "href": "posts/bayes101/index.html#brms-setup",
    "title": "Bayesian Modeling for Psychologists, Part 1",
    "section": "brms setup",
    "text": "brms setup\n\n# install.packages(\"brms\")\nlibrary(brms)\n\n\n\n\n\n\n\nImportant\n\n\n\nbrms is actually translating your R code into Stan (Stan Development Team (2018)), a programming language that is specialized in estimating posterior distributions. Therefore, you will also need to install some sort of backend. Using cmdstanr is probably optimal, but because it’s installation can be tricky, Rstan is another option."
  },
  {
    "objectID": "posts/bayes101/index.html#simple-linear-regression",
    "href": "posts/bayes101/index.html#simple-linear-regression",
    "title": "Bayesian Modeling for Psychologists, Part 1",
    "section": "Simple linear regression",
    "text": "Simple linear regression\nLet’s start with the most simple example. Consider the sex differences in cats’ body weight:\n\ndata &lt;- MASS::cats\n\nhead(data)\n\n  Sex Bwt Hwt\n1   F 2.0 7.0\n2   F 2.0 7.4\n3   F 2.0 9.5\n4   F 2.1 7.2\n5   F 2.1 7.3\n6   F 2.1 7.6\n\n\nFitting a linear regression model:\n\nlm_cats &lt;- lm(Bwt ~ Sex,\n              data = data)\n\nparameters::model_parameters(lm_cats) |&gt; insight::print_html()\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(142)\np\n\n\n\n\n(Intercept)\n2.36\n0.06\n(2.24, 2.48)\n39.00\n&lt; .001\n\n\nSex (M)\n0.54\n0.07\n(0.39, 0.69)\n7.33\n&lt; .001\n\n\n\n\n\n\n\n\n\n\n\n\nMale cats weigh \\(0.54\\) Kg more than female cats, and this difference is statistically significant."
  },
  {
    "objectID": "posts/bayes101/index.html#prior-elicitation",
    "href": "posts/bayes101/index.html#prior-elicitation",
    "title": "Bayesian Modeling for Psychologists, Part 1",
    "section": "Prior elicitation",
    "text": "Prior elicitation\nHow many parameters are there?\n1. Intercept - Representing the value of Bwt when the IV is \\(0\\). Meaning it is the estimated mean weight of female cats.\n2. Slope of \\(SexMale\\) - Estimated difference in body weight between male and female cats.\n3. Sigma - The variance of the residuals (variance of the conditional-mean normal distribution of Bwt).\n\nWhat prior knowledge can we incorporate into each parameter?\n1. average weight of female cats should probably be ~2Kg-3Kg.\n2. Male cats should probably weight more than female cats.\n3. The sigma is not so interesting, so I will just define a wide prior.\n\n\nprior &lt;- set_prior(\"normal(2.5, 2)\", class = \"Intercept\") +\n  set_prior(\"normal(1, 2)\", coef = \"SexM\") +\n  set_prior(\"exponential(0.01)\", class = \"sigma\")"
  },
  {
    "objectID": "posts/bayes101/index.html#tip",
    "href": "posts/bayes101/index.html#tip",
    "title": "Bayesian Modeling for Psychologists, Part 1",
    "section": "Tip",
    "text": "Tip\nIf you are not sure what are the parameters of your model, use the brms::get_prior() function to get your model’s parameters and their default brms priors."
  },
  {
    "objectID": "posts/bayes101/index.html#footnotes",
    "href": "posts/bayes101/index.html#footnotes",
    "title": "Bayesian Modeling for Psychologists, Part 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor example, consider the hypothesis that people in condition A are faster in a certain task than people in condition B. Your formal hypothesis look something like this: Reaction time follows a normal distribution with \\(\\mu=\\beta_0+\\beta_1condition\\) and \\(\\sigma^2\\). This model has 3 total parameters (\\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\)).↩︎\nThe term \\(P(Data|Parameters)\\) is not a probability distribution, but a special function called a Likelihood function and should be denoted by \\(L(Parameters|Data)\\).↩︎\nWhich is identical to the normal density function.↩︎\nFor the math nerds, the evidence is defined as this integral: \\(\\int_{\\theta}P(X|\\theta)P(\\theta)d\\theta\\).↩︎"
  },
  {
    "objectID": "posts/bayes102/index.html",
    "href": "posts/bayes102/index.html",
    "title": "Bayesian Modeling for Psychologists, Part 2",
    "section": "",
    "text": "Loading some packages for demonstrations and analysis:\n\nlibrary(tidyverse)      # Data wrangling and plotting\nlibrary(ggdist)         # Easy and aesthetic plotting of distributions\nlibrary(ggExtra)        # Adding marginal distributions for 2d plots\nlibrary(tidybayes)      # Tidy processing of Bayesian models and extraction of MCMC chains\nlibrary(bayestestR)     # Nice plots for brms models\nlibrary(brms)           # Bayesian modeling and posterior estimation with MCMC using Stan\nlibrary(lme4)           # fitting frequentist hierarchical linear models\nlibrary(lmerTest)       # fitting frequentist hierarchical linear models\nlibrary(parameters)     # clean extraction of model parameters\nlibrary(distributional) # Plotting of theoretical distributions (for likelihood functions plots)"
  },
  {
    "objectID": "posts/bayes102/index.html#ols-linear-model",
    "href": "posts/bayes102/index.html#ols-linear-model",
    "title": "Bayesian Modeling for Psychologists, Part 2",
    "section": "OLS Linear model",
    "text": "OLS Linear model\nIn the first part we looked at a simple linear regression model with a categorical predictor (a t-test). Let’s look at linear regression with one continuous predictor:\n\niris &lt;- iris\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nProbably some relationship between Petal width and length:\n\nols_model &lt;- lm(Petal.Length ~ Petal.Width, data = iris)\n\n\nmodel_parameters(ols_model) |&gt; insight::print_html()\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(148)\np\n\n\n\n\n(Intercept)\n1.08\n0.07\n(0.94, 1.23)\n14.85\n&lt; .001\n\n\nPetal Width\n2.23\n0.05\n(2.13, 2.33)\n43.39\n&lt; .001\n\n\n\n\n\n\n\n\n\n\n\n\nMaybe some interaction with species?\n\nols_model_int &lt;- lm(Petal.Length ~ Petal.Width * Species, data = iris)\n\n\nmodel_parameters(ols_model_int) |&gt; insight::print_html()\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(144)\np\n\n\n\n\n(Intercept)\n1.33\n0.13\n(1.07, 1.59)\n10.14\n&lt; .001\n\n\nPetal Width\n0.55\n0.49\n(-0.42, 1.52)\n1.12\n0.267\n\n\nSpecies (versicolor)\n0.45\n0.37\n(-0.28, 1.19)\n1.21\n0.227\n\n\nSpecies (virginica)\n2.91\n0.41\n(2.11, 3.72)\n7.17\n&lt; .001\n\n\nPetal Width × Species (versicolor)\n1.32\n0.56\n(0.23, 2.42)\n2.38\n0.019\n\n\nPetal Width × Species (virginica)\n0.10\n0.52\n(-0.94, 1.14)\n0.19\n0.848\n\n\n\n\n\n\n\n\n\n\n\n\nWe get the following model:\n\\[\nPetal.Length=1.33+0.55 \\cdot Petal.Width+0.45 \\cdot SpeciesVersicolor+2.91 \\cdot SpeciesVirginica+1.32 \\cdot Petal.Width \\cdot SpeciesVersicolor+0.1 \\cdot Petal.Width \\cdot SpeciesVirginica\n\\]\nIn order for the intercept to have practical meaning we need to center the predictor Petal.Width:\n\niris$Petal.Width_c &lt;- scale(iris$Petal.Width, scale = FALSE)[,1]\n\nFitting the model again:\n\nols_model_centered &lt;- lm(Petal.Length ~ Petal.Width_c * Species, data = iris)\n\nNow the intercept will be the predicted Petal.Length for Setosa flowers with mean Petal.Width:\n\nmodel_parameters(ols_model_centered) |&gt; insight::print_html()\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(144)\np\n\n\n\n\n(Intercept)\n1.98\n0.47\n(1.05, 2.91)\n4.22\n&lt; .001\n\n\nPetal Width c\n0.55\n0.49\n(-0.42, 1.52)\n1.12\n0.267\n\n\nSpecies (versicolor)\n2.04\n0.47\n(1.10, 2.98)\n4.31\n&lt; .001\n\n\nSpecies (virginica)\n3.03\n0.50\n(2.05, 4.02)\n6.10\n&lt; .001\n\n\nPetal Width c × Species (versicolor)\n1.32\n0.56\n(0.23, 2.42)\n2.38\n0.019\n\n\nPetal Width c × Species (virginica)\n0.10\n0.52\n(-0.94, 1.14)\n0.19\n0.848\n\n\n\n\n\n\n\n\n\n\n\n\nThe model is:\n\\[\nPetal.Length=1.98+0.55 \\cdot Petal.Width+2.04 \\cdot SpeciesVersicolor+3.03 \\cdot SpeciesVirginica+1.32 \\cdot Petal.Width \\cdot SpeciesVersicolor+0.1 \\cdot Petal.Width \\cdot SpeciesVirginica\n\\]\n\nVisualization\n\nf_plot &lt;- iris |&gt;\n  ggplot(aes(x = Petal.Width, y = Petal.Length, color = Species, fill = Species)) +\n  geom_point(show.legend = F) +\n  geom_smooth(method = \"lm\") +\n  scale_fill_manual(values = c(\"#edae49\", \"#d1495b\", \"#00798c\")) +\n  scale_color_manual(values = c(\"#edae49\", \"#d1495b\", \"#00798c\")) +\n  blog_theme\n\nf_plot"
  },
  {
    "objectID": "posts/bayes102/index.html#bayesian-linear-model",
    "href": "posts/bayes102/index.html#bayesian-linear-model",
    "title": "Bayesian Modeling for Psychologists, Part 2",
    "section": "Bayesian linear model",
    "text": "Bayesian linear model\nIn order to fit the same model in a Bayesian way, we will first need to define the prior distribution. In this case there are a total of 7 parameters.\nLet’s verify this with the get_prior() function:\n\nget_prior(formula = Petal.Length ~ Petal.Width_c * Species,\n          data = iris,\n          family = gaussian())\n\n                  prior     class                            coef group resp\n                 (flat)         b                                           \n                 (flat)         b                   Petal.Width_c           \n                 (flat)         b Petal.Width_c:Speciesversicolor           \n                 (flat)         b  Petal.Width_c:Speciesvirginica           \n                 (flat)         b               Speciesversicolor           \n                 (flat)         b                Speciesvirginica           \n student_t(3, 4.3, 2.5) Intercept                                           \n   student_t(3, 0, 2.5)     sigma                                           \n dpar nlpar lb ub       source\n                       default\n                  (vectorized)\n                  (vectorized)\n                  (vectorized)\n                  (vectorized)\n                  (vectorized)\n                       default\n             0         default\n\n\nI will model some positive relationship between length and width, a relatively weakly-informed prior on the intercept, and a very wide prior on the not-interesting sigma parameter. On the other parameters I will have an “agnostic” zero-centered Normal distribution:\n\n\nstudent-t prior\nThe student-t distribution is symmetrical like the Gaussian distribution, but it has thicker “tails”. This allows the model to explore wider area of the possible parameter value space.\n\n\n\n\n\n\n\n\n\n\nprior_iris &lt;- set_prior(\"normal(1, 3)\", coef = \"Petal.Width_c\") +\n  set_prior(\"normal(0, 3)\", coef = \"Speciesversicolor\") +\n  set_prior(\"normal(0, 3)\", coef = \"Speciesvirginica\") +\n  set_prior(\"normal(0, 3)\", coef = \"Petal.Width_c:Speciesversicolor\") +\n  set_prior(\"normal(0, 3)\", coef = \"Petal.Width_c:Speciesvirginica\") +\n  set_prior(\"normal(0, 3)\", class = \"Intercept\") +\n  set_prior(\"exponential(0.001)\", class = \"sigma\")\n\n\n\nExponential prior\nBecause variances are strictly positive, the exponential distribution is a common choice for it (and other variance parameters):\n\n\n\n\n\n\n\n\n\nFitting a model:\n\nbayes_model_iris &lt;- brm(Petal.Length ~ Petal.Width_c * Species,\n                        data = iris,\n                        family = gaussian(),\n                        prior = prior_iris,\n                        cores = 4,\n                        chains = 4,\n                        iter = 4000,\n                        backend = \"cmdstanr\")\n\n\nmodel_parameters(bayes_model_iris, centrality = \"all\") |&gt; insight::print_html()\n\n\n\n\n\n\n\nModel Summary\n\n\nParameter\nMedian\nMean\nMAP\n95% CI\npd\nRhat\nESS\n\n\n\n\nFixed Effects\n\n\n(Intercept)\n2.14\n2.14\n2.15\n(1.24, 3.04)\n100%\n1.002\n1473.00\n\n\nPetal.Width_c\n0.71\n0.71\n0.71\n(-0.22, 1.65)\n93.39%\n1.002\n1466.00\n\n\nSpeciesversicolor\n1.89\n1.88\n1.90\n(0.98, 2.79)\n100%\n1.002\n1507.00\n\n\nSpeciesvirginica\n2.87\n2.87\n2.85\n(1.92, 3.81)\n100%\n1.002\n1676.00\n\n\nPetal.Width_c:Speciesversicolor\n1.16\n1.16\n1.16\n(0.11, 2.22)\n98.38%\n1.002\n1712.00\n\n\nPetal.Width_c:Speciesvirginica\n-0.05\n-0.05\n-0.04\n(-1.07, 0.97)\n54.23%\n1.002\n1618.00\n\n\nSigma\n\n\nsigma\n0.36\n0.36\n0.36\n(0.32, 0.41)\n100%\n1.001\n3753.00\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel_parameters(ols_model_centered)\n\nParameter                            | Coefficient |   SE |        95% CI | t(144) |      p\n-------------------------------------------------------------------------------------------\n(Intercept)                          |        1.98 | 0.47 | [ 1.05, 2.91] |   4.22 | &lt; .001\nPetal Width c                        |        0.55 | 0.49 | [-0.42, 1.52] |   1.12 | 0.267 \nSpecies [versicolor]                 |        2.04 | 0.47 | [ 1.10, 2.98] |   4.31 | &lt; .001\nSpecies [virginica]                  |        3.03 | 0.50 | [ 2.05, 4.02] |   6.10 | &lt; .001\nPetal Width c × Species [versicolor] |        1.32 | 0.56 | [ 0.23, 2.42] |   2.38 | 0.019 \nPetal Width c × Species [virginica]  |        0.10 | 0.52 | [-0.94, 1.14] |   0.19 | 0.848 \n\n\nConvergence metrics looking good, we can proceed to interpret the posterior.\n\nVisualization\nVisualizing a Bayesian regression model can be done in several ways:\n\nVisualizing regression parameters themselves\nLike we saw in Part 1, we can directly inspect the marginal posteriors of the coefficients themselves. First we will extract them into a data.frame (and rename interaction terms for convenience):\n\nposterior_iris &lt;- spread_draws(bayes_model_iris, b_Intercept, b_Petal.Width_c, b_Speciesversicolor, b_Speciesvirginica, !!sym(\"b_Petal.Width_c:Speciesversicolor\"), !!sym(\"b_Petal.Width_c:Speciesvirginica\"), sigma) |&gt;\n  rename(b_Petal.WidthXSpeciesversicolor = !!sym(\"b_Petal.Width_c:Speciesversicolor\"),\n         b_Petal.WidthXSpeciesvirginica = !!sym(\"b_Petal.Width_c:Speciesvirginica\"))\n\n\nhead(posterior_iris)\n\n# A tibble: 6 × 10\n  .chain .iteration .draw b_Intercept b_Petal.Width_c b_Speciesversicolor\n   &lt;int&gt;      &lt;int&gt; &lt;int&gt;       &lt;dbl&gt;           &lt;dbl&gt;               &lt;dbl&gt;\n1      1          1     1        1.82           0.393                2.18\n2      1          2     2        1.56           0.160                2.39\n3      1          3     3        1.61           0.102                2.34\n4      1          4     4        1.69           0.231                2.30\n5      1          5     5        1.67           0.243                2.29\n6      1          6     6        1.71           0.250                2.26\n# ℹ 4 more variables: b_Speciesvirginica &lt;dbl&gt;,\n#   b_Petal.WidthXSpeciesversicolor &lt;dbl&gt;,\n#   b_Petal.WidthXSpeciesvirginica &lt;dbl&gt;, sigma &lt;dbl&gt;\n\n\nIn the output of spread_draws we get 10 columns:\n* .chain = the index of the MCMC chain (4 total in our case).\n* .iteration = the index of the draw within it’s chain (each chain is 500 samples long).\n* .draw = the overall index of the draw (2000 samples in the posterior overall).\n* parameter_name - draw’s value for each parameter.\n\nPlotting the marginal posteriors:\n\nposterior_iris |&gt;\n  pivot_longer(cols = c(b_Intercept, b_Petal.Width_c, b_Speciesversicolor, b_Speciesvirginica, b_Petal.WidthXSpeciesversicolor, b_Petal.WidthXSpeciesvirginica),\n               names_to = \"variable\") |&gt;\n  ggplot(aes(x = value, y = variable, fill = after_stat(x &gt; 0))) +\n  stat_halfeye() +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  scale_fill_manual(values = c(\"#d1495b\", \"#00798c\"), labels = c(\"Negative\", \"Positive\")) +\n  scale_x_continuous(breaks = seq(-4, 4, 0.5), labels = seq(-4, 4, 0.5)) +\n  labs(fill = \"Direction of Effect\") +\n  blog_theme\n\n\n\n\n\n\n\n\n\n\nVisualizing the regression line(s)\nMarginal posteriors are nice, but are not a visualization of the model itself. Like in the frequentist model, we would like to see the regression line itself. How to do it?\nThe regression line is a line of conditional means:\n\\[\n\\mu|X=\\beta X\n\\]/\nBut, we have 4000 samples from the posterior containing possible values for each parameter \\(\\beta_i\\) - a.k.a a distribution. So for each possible row in the data set we will have a distribution (4000 values) of predicted conditional means \\(\\mu|X\\):\n\n\n\nUncertainty\nWhy not summarizing the MCMC chains when inferring or visualizing a Bayesian model?\nA key aspect of statistical modeling and visualization is the measurement of uncertainty in the data. In the frequentist world, this is represented with standard errors and confidence intervals.\nIn the Bayesian world uncertainty is represented by the MCMC samples themselves, creating the posterior distribution. For that reason we will make calculations and visualizations with the MCMC samples and not with their summaries.\nCreating a data.frame of independent variables (a design matrix) for creating predictions from the posterior:\n\nnew_data &lt;- expand_grid(Petal.Width_c = seq(min(iris$Petal.Width_c), max(iris$Petal.Width_c), length=200),\n                        Species = unique(iris$Species))\n\nhead(new_data)\n\n# A tibble: 6 × 2\n  Petal.Width_c Species   \n          &lt;dbl&gt; &lt;fct&gt;     \n1         -1.10 setosa    \n2         -1.10 versicolor\n3         -1.10 virginica \n4         -1.09 setosa    \n5         -1.09 versicolor\n6         -1.09 virginica \n\n\nWe actually need to remove some rows from this data frame. This is because not all values of Petal.Width appear in all species. The range of observed widths is different between different species.\n\nrange_setosa &lt;- c(min(iris$Petal.Width_c[iris$Species == \"setosa\"]), max(iris$Petal.Width_c[iris$Species == \"setosa\"]))\nrange_versicolor &lt;- c(min(iris$Petal.Width_c[iris$Species == \"versicolor\"]), max(iris$Petal.Width_c[iris$Species == \"versicolor\"]))\nrange_virginica &lt;- c(min(iris$Petal.Width_c[iris$Species == \"virginica\"]), max(iris$Petal.Width_c[iris$Species == \"virginica\"]))\n\n\nnew_data &lt;- new_data |&gt;\n  mutate(extrapolation = case_when((Species == \"setosa\" & Petal.Width_c &lt; range_setosa[1]) | (Species == \"setosa\" & Petal.Width_c &gt; range_setosa[2]) ~ TRUE,\n                                   (Species == \"versicolor\" & Petal.Width_c &lt; range_versicolor[1]) | (Species == \"versicolor\" & Petal.Width_c &gt; range_versicolor[2]) ~ TRUE,\n                                   (Species == \"virginica\" & Petal.Width_c &lt; range_virginica[1]) | (Species == \"virginica\" & Petal.Width_c &gt; range_virginica[2]) ~ TRUE,\n                                   .default = FALSE)) |&gt;\n  filter(!extrapolation)\n\nCalculating the distribution of conditional means for the second row: \\(Petal.Widthc=-1.099333; \\ Species=versicolor\\):\n\nposterior_iris |&gt;\n  mutate(conditional_mean = b_Intercept + b_Petal.Width_c * -1.099333 + b_Speciesversicolor + b_Petal.WidthXSpeciesversicolor * -1.099333) |&gt;\n  head()\n\n# A tibble: 6 × 11\n  .chain .iteration .draw b_Intercept b_Petal.Width_c b_Speciesversicolor\n   &lt;int&gt;      &lt;int&gt; &lt;int&gt;       &lt;dbl&gt;           &lt;dbl&gt;               &lt;dbl&gt;\n1      1          1     1        1.82           0.393                2.18\n2      1          2     2        1.56           0.160                2.39\n3      1          3     3        1.61           0.102                2.34\n4      1          4     4        1.69           0.231                2.30\n5      1          5     5        1.67           0.243                2.29\n6      1          6     6        1.71           0.250                2.26\n# ℹ 5 more variables: b_Speciesvirginica &lt;dbl&gt;,\n#   b_Petal.WidthXSpeciesversicolor &lt;dbl&gt;,\n#   b_Petal.WidthXSpeciesvirginica &lt;dbl&gt;, sigma &lt;dbl&gt;, conditional_mean &lt;dbl&gt;\n\n\n\nposterior_iris |&gt;\n  mutate(conditional_mean = b_Intercept + b_Petal.Width_c * -1.099333 + b_Speciesversicolor + b_Petal.WidthXSpeciesversicolor * -1.099333) |&gt;\n  ggplot(aes(y = conditional_mean)) +\n  stat_slab(fill = \"#d1495b\", color = \"gray34\") +\n  labs(y = expression(mu)) +\n  blog_theme +\n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank())\n\n\n\n\n\n\n\n\nCalculating this for several rows gives us several more distributions of conditional means. I here use values realistic for each species to avoid extrapolation.\n\nwidth_values_setosa &lt;- c(-1, -0.9, -0.7)\nwidth_values_versicolor &lt;- c(0, 0.1, 0.3)\nwidth_values_virginica &lt;- c(0.2, 0.8, 1.1)\n\nposterior_iris |&gt;\n  mutate(conditionalmean_1_setosa = b_Intercept + b_Petal.Width_c * width_values_setosa[1],\n         conditionalmean_2_setosa = b_Intercept + b_Petal.Width_c * width_values_setosa[2],\n         conditionalmean_3_setosa = b_Intercept + b_Petal.Width_c * width_values_setosa[3],\n         conditionalmean_1_versicolor = b_Intercept + b_Petal.Width_c * width_values_versicolor[1] + b_Speciesversicolor + b_Petal.WidthXSpeciesversicolor * width_values_versicolor[1],\n         conditionalmean_2_versicolor = b_Intercept + b_Petal.Width_c * width_values_versicolor[2] + b_Speciesversicolor + b_Petal.WidthXSpeciesversicolor * width_values_versicolor[2],\n         conditionalmean_3_versicolor = b_Intercept + b_Petal.Width_c * width_values_versicolor[3] + b_Speciesversicolor + b_Petal.WidthXSpeciesversicolor * width_values_versicolor[3],\n         conditionalmean_1_virginica = b_Intercept + b_Petal.Width_c * width_values_virginica[1] + b_Speciesvirginica + b_Petal.WidthXSpeciesvirginica * width_values_virginica[1],\n         conditionalmean_2_virginica = b_Intercept + b_Petal.Width_c * width_values_virginica[2] + b_Speciesvirginica + b_Petal.WidthXSpeciesvirginica * width_values_virginica[2],\n         conditionalmean_3_virginica = b_Intercept + b_Petal.Width_c * width_values_virginica[3] + b_Speciesvirginica + b_Petal.WidthXSpeciesvirginica * width_values_virginica[3]) |&gt;\n  pivot_longer(cols = c(conditionalmean_1_setosa, conditionalmean_2_setosa, conditionalmean_3_setosa,\n                        conditionalmean_1_versicolor, conditionalmean_2_versicolor, conditionalmean_3_versicolor,\n                        conditionalmean_1_virginica, conditionalmean_2_virginica, conditionalmean_3_virginica),\n               names_to = c(\"junk\", \"index\", \"Species\"),\n               names_sep = \"_\") |&gt;\n  mutate(index = case_when(index == 1 & Species == \"setosa\" ~ width_values_setosa[1],\n                           index == 2 & Species == \"setosa\" ~ width_values_setosa[2],\n                           index == 3 & Species == \"setosa\" ~ width_values_setosa[3],\n                           index == 1 & Species == \"versicolor\" ~ width_values_versicolor[1],\n                           index == 2 & Species == \"versicolor\" ~ width_values_versicolor[2],\n                           index == 3 & Species == \"versicolor\" ~ width_values_versicolor[3],\n                           index == 1 & Species == \"virginica\" ~ width_values_virginica[1],\n                           index == 2 & Species == \"virginica\" ~ width_values_virginica[2],\n                           index == 3 & Species == \"virginica\" ~ width_values_virginica[3])) |&gt;\n  ggplot(aes(x = index, y = value, fill = Species)) +\n    stat_slab(color = \"gray33\", linewidth = 0.5) +\n    scale_fill_manual(values = c(\"#edae49\", \"#d1495b\", \"#00798c\")) +\n    labs(x = \"Petal.Width\", y = expression(mu)) +\n    blog_theme\n\n\n\n\n\n\n\n\nWe can use the add_linpred_draws() function from the awesome tidybayes package (Kay 2020) in order to do this thing for all values of X, and finally plot the regression line with the credible interval around it:\n\nnew_data |&gt;\n  add_linpred_draws(bayes_model_iris) |&gt;\n  ggplot(aes(x = Petal.Width_c, y = .linpred, fill = Species, color = Species)) +\n  geom_point(data = iris, aes(y = Petal.Length, color = Species), show.legend = F) +\n  stat_lineribbon(.width = c(0.80, 0.85, 0.97), alpha = 0.7) +\n  scale_fill_manual(values = c(\"#edae49\", \"#d1495b\", \"#00798c\")) +\n  scale_color_manual(values = c(\"#edae49\", \"#d1495b\", \"#00798c\")) +\n  labs(y = expression(mu), fill = \"Species\", title = \"80%, 85% and 97% Credible Intervals\") +\n  guides(color = \"none\") +\n  blog_theme\n\n\n\n\n\n\n\n\nThis is similar to the frequentist confidence interval, just remember that these interval actually represents 80%, 85%, and 97% of regression lines.\nI think that a better way of visualizing uncertainty is by plotting the individual line themselves. Each draw represents a different regression line, plotting them all will give a nice uncertainty visualization:\n\nnew_data &lt;- new_data |&gt;\n  select(-extrapolation) |&gt;\n  add_linpred_draws(bayes_model_iris, ndraws = 44, seed = 14)\n\nWe can look at individual draws:\n\nnew_data |&gt;\n  filter(.draw == 14) |&gt;\n  ggplot(aes(x = Petal.Width_c, y = .linpred, color = Species)) +\n  geom_line(linewidth = 1) +\n  geom_point(data = iris, aes(x = Petal.Width_c, y = Petal.Length, color = Species), inherit.aes = F, show.legend = F) +\n  scale_color_manual(values = c(\"#edae49\", \"#d1495b\", \"#00798c\")) +\n  labs(y = \"Petal.Length\", title = \"Draw number 14\") +\n  blog_theme\n\n\n\n\n\n\n\n\nOr we can group by draw index and look at all the lines:\n\nnew_data |&gt;\n  ggplot(aes(x = Petal.Width_c, y = .linpred, group = interaction(.draw, Species), color = Species)) +\n  geom_point(data = iris, aes(x = Petal.Width_c, y = Petal.Length, color = Species), inherit.aes = F, show.legend = F) +\n  geom_line(alpha = 0.4) +\n  scale_fill_manual(values = c(\"#edae49\", \"#d1495b\", \"#00798c\")) +\n  scale_color_manual(values = c(\"#edae49\", \"#d1495b\", \"#00798c\")) +\n  labs(y = \"Petal.Length\") +\n  blog_theme"
  },
  {
    "objectID": "posts/bayes102/index.html#maximum-likelihood-estimation",
    "href": "posts/bayes102/index.html#maximum-likelihood-estimation",
    "title": "Bayesian Modeling for Psychologists, Part 2",
    "section": "Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation\nFitting this model with the frequentist ML method:\n\nsleep_model_ml &lt;- lmer(Reaction ~ 1 + Days + (1 + Days | Subject),\n                       data = sleep)\n\nUsing the sjPlot::tab_model() function to produce a nice summarized statistics of mixed models:\n\nsjPlot::tab_model(sleep_model_ml)\n\n\n\n \nReaction\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n251.41\n237.94 – 264.87\n&lt;0.001\n\n\nDays\n10.47\n7.42 – 13.52\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n654.94\n\n\n\nτ00 Subject\n612.10\n\n\nτ11 Subject.Days\n35.07\n\n\nρ01 Subject\n0.07\n\n\nICC\n0.72\n\n\nN Subject\n18\n\nObservations\n180\n\n\nMarginal R2 / Conditional R2\n0.279 / 0.799"
  },
  {
    "objectID": "posts/bayes102/index.html#bayesian-estimation",
    "href": "posts/bayes102/index.html#bayesian-estimation",
    "title": "Bayesian Modeling for Psychologists, Part 2",
    "section": "Bayesian Estimation",
    "text": "Bayesian Estimation\nHow many parameters? we have 2 fixed effects (intercept and slope), 3 random effects (random intercept, random slope and their correlation), and sigma - A total of 6 parameters.\n\nget_prior(formula = Reaction ~ 1 + Days + (1 + Days | Subject),\n          data = sleep,\n          family = gaussian())\n\n                     prior     class      coef   group resp dpar nlpar lb ub\n                    (flat)         b                                        \n                    (flat)         b      Days                              \n                    lkj(1)       cor                                        \n                    lkj(1)       cor           Subject                      \n student_t(3, 288.7, 59.3) Intercept                                        \n     student_t(3, 0, 59.3)        sd                                    0   \n     student_t(3, 0, 59.3)        sd           Subject                  0   \n     student_t(3, 0, 59.3)        sd      Days Subject                  0   \n     student_t(3, 0, 59.3)        sd Intercept Subject                  0   \n     student_t(3, 0, 59.3)     sigma                                    0   \n       source\n      default\n (vectorized)\n      default\n (vectorized)\n      default\n      default\n (vectorized)\n (vectorized)\n (vectorized)\n      default\n\n\n\nPrior elicitation\n\nprior_sleep &lt;- set_prior(\"normal(10, 4)\", coef = \"Days\") + # RT should increase with continued sleep deprivation\n  set_prior(\"exponential(1)\", class = \"sd\") + # setting a prior on all random variances at once\n  set_prior(\"exponential(0.01)\", class = \"sigma\")\n\n\n\nModel estimation\n\nbayes_model_sleep &lt;- brm(formula = Reaction ~ 1 + Days + (1 + Days | Subject),\n                         data = sleep,\n                         family = gaussian(),\n                         prior = prior_sleep,\n                         chains = 4,\n                         cores = 4,\n                         iter = 2000,\n                         backend = \"cmdstanr\")\n\nAs our models get more complicated, the posterior distribution gets more hard for the MCMC sampler to sample from. That can result in low ECC and/or high Rhat. One solution can be simply increasing the length of each MCMC chain! this change to model definition doesn’t require compiling the Stan code again, use the update() function instead:\n\nbayes_model_sleep &lt;- update(bayes_model_sleep, iter = 9000)\n\n\nmodel_parameters(bayes_model_sleep, centrality = \"all\", effects = \"all\") |&gt; insight::print_html()\n\n\n\n\n\n\n\nModel Summary\n\n\nParameter\nMedian\nMean\nMAP\n95% CI\npd\nRhat\nESS\n\n\n\n\nFixed Effects\n\n\n(Intercept)\n251.44\n251.44\n251.58\n(243.45, 259.61)\n100%\n1.000\n18190.00\n\n\nDays\n10.38\n10.37\n10.47\n(7.53, 13.22)\n100%\n1.000\n12965.00\n\n\nSigma\n\n\nsigma\n28.66\n28.72\n28.59\n(25.51, 32.32)\n100%\n1.000\n14323.00\n\n\nRandom Effects Variances\n\n\nSD (Intercept: Subject)\n2.58\n3.57\n0.16\n(0.06, 11.71)\n100%\n1.000\n4650.00\n\n\nSD (Days: Subject)\n5.75\n5.82\n5.64\n(3.94, 8.08)\n100%\n1.000\n6902.00\n\n\nCor (Intercept~Days: Subject)\n0.59\n0.46\n0.96\n(-0.74, 0.98)\n84.40%\n1.001\n1453.00\n\n\n\n\n\n\n\n\n\n\n\n\nExtracting MCMC draws:\n\nposterior_sleep &lt;- spread_draws(bayes_model_sleep, b_Intercept, b_Days, sd_Subject__Intercept, sd_Subject__Days, cor_Subject__Intercept__Days)\n\n\nThe parameters themselves\n\nposterior_sleep |&gt;\n  ggplot(aes(x = b_Intercept, fill = \"#d1495b\")) +\n  stat_slab(color = \"gray34\") +\n  guides(color = \"none\", fill = \"none\") +\n  labs(title = \"Intercept\", y = NULL, x = NULL) +\n  scale_x_continuous(breaks = seq(230, 270, 5), labels = seq(230, 270, 5)) +\n  blog_theme +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())\n\n\n\n\n\n\n\n\n\nposterior_sleep |&gt;\n  ggplot(aes(x = b_Days, fill = \"#d1495b\")) +\n  stat_slab(color = \"gray34\") +\n  guides(color = \"none\", fill = \"none\") +\n  labs(title = \"b_Days\", y = NULL, x = NULL) +\n  scale_x_continuous(breaks = seq(5, 17, 1), labels = seq(5, 17, 1)) +\n  blog_theme +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())\n\n\n\n\n\n\n\n\n\nposterior_sleep |&gt;\n  select(.draw, sd_Subject__Intercept, sd_Subject__Days) |&gt;\n  pivot_longer(cols = c(sd_Subject__Intercept, sd_Subject__Days),\n               names_to = \"variable\") |&gt;\n  ggplot(aes(x = value, y = variable)) +\n  stat_slab(fill = \"#d1495b\", color = \"gray34\") +\n  scale_x_continuous(breaks = seq(0, 24, 2), labels = seq(0, 24, 2)) +\n  labs(title = \"SD of random effects\") +\n  blog_theme\n\n\n\n\n\n\n\n\nThese are actually the error terms of the random effects. We saw that they should be correlated:\n\nposterior_sleep |&gt;\n  ggplot(aes(x = cor_Subject__Intercept__Days, fill = after_stat(x &lt; 0))) +\n  stat_halfeye() +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  scale_fill_manual(values = c(\"#d1495b\", \"#00798c\"), labels = c(\"Negative\", \"Positive\")) +\n  scale_x_continuous(breaks = seq(-1, 1, 0.25), labels = seq(-1, 1, 0.25)) +\n  labs(title = \"Correlation between random effects\", fill = \"Direction of Correlation\", y = NULL, x = expression(r)) +\n  blog_theme\n\n\n\n\n\n\n\n\n\np_direction(bayes_model_sleep, effects = \"random\", parameters = \"cor*\")\n\nProbability of Direction SD/Cor: Subject\n\nParameter        |     pd\n-------------------------\nIntercept ~ Days | 84.40%\n\n\n\n\nThe line\nGenerating an empty design matrix:\n\nnew_data_sleep &lt;- expand_grid(Subject = factor(unique(sleep$Subject)),\n                              Days = c(0:9)) |&gt;\n  add_linpred_draws(bayes_model_sleep, ndraws = 45, seed = 14)\n\n\nnew_data_sleep |&gt;\n  ggplot(aes(x = Days, y = .linpred)) +\n  stat_lineribbon(.width = c(0.80, 0.85, 0.97), alpha = 0.7) +\n  geom_point(data = sleep, aes(y = Reaction), color = \"#5D001E\") +\n  scale_fill_manual(values = c(\"#edae49\", \"#d1495b\", \"#00798c\")) +\n  scale_color_manual(values = c(\"#edae49\", \"#d1495b\", \"#00798c\")) +\n  scale_x_continuous(breaks = c(0:9), labels = c(0:9)) +\n  scale_y_continuous(breaks = seq(200, 600, 50), labels = seq(200, 600, 50)) +\n  labs(y = expression(mu), fill = \"% Credible Interval\", title = \"80%, 85% and 97% Credible Intervals\") +\n  guides(color = \"none\") +\n  blog_theme\n\n\n\n\n\n\n\n\nIndividual lines:\n\nnew_data_sleep |&gt;\n  ggplot(aes(x = Days, y = .linpred, group = interaction(.draw, Subject))) +\n  geom_line(color = \"#d1495b\") +\n  geom_point(data = sleep, aes(x = Days, y = Reaction, group = Subject), inherit.aes = F, color = \"#5D001E\") +\n  scale_x_continuous(breaks = c(0:9), labels = c(0:9)) +\n  scale_y_continuous(breaks = seq(200, 600, 50), labels = seq(200, 600, 50)) +\n  labs(y = \"Reaction Time (ms)\") +\n  guides(color = \"none\") +\n  blog_theme\n\n\n\n\n\n\n\n\nFaceting by Subject\n\nnew_data_sleep |&gt;\n  ggplot(aes(x = Days, y = .linpred)) +\n  stat_lineribbon(.width = c(0.80, 0.85, 0.99), alpha = 0.7) +\n  geom_point(data = sleep, aes(y = Reaction)) +\n  facet_wrap(~Subject) +\n  scale_fill_manual(values = c(\"#edae49\", \"#d1495b\", \"#00798c\")) +\n  scale_color_manual(values = c(\"#edae49\", \"#d1495b\", \"#00798c\")) +\n  scale_x_continuous(breaks = c(0:9), labels = c(0:9)) +\n  scale_y_continuous(breaks = seq(200, 600, 100), labels = seq(200, 600, 100)) +\n  labs(y = expression(mu), fill = \"% Credible Interval\", title = \"80%, 85% and 99% Credible Intervals\") +\n  guides(color = \"none\") +\n  blog_theme\n\n\n\n\n\n\n\n\nIndividual lines:\n\nnew_data_sleep |&gt;\n  ggplot(aes(x = Days, y = .linpred, group = .draw)) +\n  geom_line(color = \"#d1495b\") +\n  geom_point(data = sleep, aes(x = Days, y = Reaction), color = \"#5D001E\",  inherit.aes = F) +\n  scale_x_continuous(breaks = c(0:9), labels = c(0:9)) +\n  scale_y_continuous(breaks = seq(200, 600, 100), labels = seq(200, 600, 100)) +\n  labs(y = \"Reation Time (ms)\") +\n  facet_wrap(~Subject) +\n  blog_theme\n\n\n\n\n\n\n\n\nAnother way of visualizing the uncertainty in Mixed effects models is:\n\nnew_data_sleep |&gt;\n  ggplot(aes(x = Days, y = .linpred, group = interaction(.draw, Subject))) +\n  geom_line(aes(color = Subject)) +\n  geom_point(data = sleep, aes(x = Days, y = Reaction, group = Subject), inherit.aes = F, color = \"#5D001E\") +\n  scale_x_continuous(breaks = c(0:9), labels = c(0:9)) +\n  scale_y_continuous(breaks = seq(200, 600, 100), labels = seq(200, 600, 100)) +\n  scale_color_brewer(type = \"div\", palette = 9) +\n  labs(y = \"Reaction Time (ms)\", title = \"Subjects represented with colors\") +\n  guides(color = \"none\") +\n  blog_theme\n\n\n\n\n\n\n\n\nIn this kind of plot we can also observe the random effects: low variability in intercepts + moderate variability in slopes."
  },
  {
    "objectID": "posts/bayes102/index.html#not-so-professional-intro-to-glms",
    "href": "posts/bayes102/index.html#not-so-professional-intro-to-glms",
    "title": "Bayesian Modeling for Psychologists, Part 2",
    "section": "Not-so-professional intro to GLMs",
    "text": "Not-so-professional intro to GLMs\n\nLikelihood Function\nOften, our interesting research questions demand yet more complex models. Recall that ordinary regression model assumes that the dependent variable \\(y\\) is following a normal distribution with conditional mean \\(\\beta_0+\\beta_1 x_1+...+\\beta_n x_n\\) and variance \\(\\sigma^2\\). Generalized linear models can assume other distributions for the outcome variable \\(y\\), for example: if our outcome is binary we can assume it follows a Bernoulli distribution: \\(y \\sim Bernoulli(p)\\). In this kind of model the parameter that is modeled is \\(p\\).\nIn part 1 we called this the assumption of a Data Generating Process (DGP).\n\n\nLink Function\nSometimes modeling other parameters creates other complexities: for example: the parameter \\(p\\) is a probability, strictly bounded in \\([0;1]\\), but the linear predictor \\(\\beta_0+\\beta_1 x_1+...+\\beta_n x_n\\) should be allowed to be any value in \\([-\\infty;+\\infty]\\). Somehow, the parameter \\(p\\) should be mapped from it’s \\([0;1]\\) interval to the \\([-\\infty;+\\infty]\\) interval. This is done with a Link Function:\n\\[\nf(p)=\\beta_0+\\beta_1 x_1+...+\\beta_n x_n\n\\]\nFor Bernoulli models this link function if often the logit function: \\(logit(p)=ln(\\frac{p}{1-p})\\) - giving Logistic regression it’s name.\nThe term \\(f(p)\\) is often not easy or intuitive for interpretation, so the equation will be back-transformed1:\n\\[\n\\frac{p}{1-p} =e^{(\\beta_0+\\beta_1 x_1+...+\\beta_n x_n)}=e^{\\beta_0} \\cdot e^{\\beta_1x_1} \\cdot... \\cdot e^{\\beta_nx_n}\n\\]"
  },
  {
    "objectID": "posts/bayes102/index.html#logistic-regression",
    "href": "posts/bayes102/index.html#logistic-regression",
    "title": "Bayesian Modeling for Psychologists, Part 2",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nWe will use the Smarket dataset from the package ISLR containing information about daily percentage returns for the S&P500 index between 2001-2005.\n\ndata &lt;- ISLR::Smarket\n\nhead(data)\n\n  Year   Lag1   Lag2   Lag3   Lag4   Lag5 Volume  Today Direction\n1 2001  0.381 -0.192 -2.624 -1.055  5.010 1.1913  0.959        Up\n2 2001  0.959  0.381 -0.192 -2.624 -1.055 1.2965  1.032        Up\n3 2001  1.032  0.959  0.381 -0.192 -2.624 1.4112 -0.623      Down\n4 2001 -0.623  1.032  0.959  0.381 -0.192 1.2760  0.614        Up\n5 2001  0.614 -0.623  1.032  0.959  0.381 1.2057  0.213        Up\n6 2001  0.213  0.614 -0.623  1.032  0.959 1.3491  1.392        Up\n\n\n\nMaximum Likelihood Estimation\nPredicting the direction of change Direction from returns the previous day Lag1:\n\nsp500 &lt;- data |&gt;\n  mutate(Direction = factor(Direction, levels = c(\"Down\", \"Up\")),\n         Lag1 = scale(Lag1, scale = F)[,1])\n\nlogistic_model &lt;- glm(Direction ~ Lag1,\n                      data = sp500,\n                      family = binomial(link = \"logit\")) # 'binomial' is the DGP/likelihood function, 'link' is the link function\n\n\nmodel_parameters(logistic_model) |&gt; insight::print_html()\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nz\np\n\n\n\n\n(Intercept)\n0.07\n0.06\n(-0.04, 0.18)\n1.30\n0.193\n\n\nLag1\n-0.07\n0.05\n(-0.17, 0.03)\n-1.40\n0.160\n\n\n\n\n\n\n\n\n\n\n\n\nBack-transforming the parameters gives:\n\nmodel_parameters(logistic_model, exponentiate = T) |&gt; insight::print_html()\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nz\np\n\n\n\n\n(Intercept)\n1.08\n0.06\n(0.96, 1.20)\n1.30\n0.193\n\n\nLag1\n0.93\n0.05\n(0.84, 1.03)\n-1.40\n0.160\n\n\n\n\n\n\n\n\n\n\n\n\nAnd this is the logit function fitted to the data, the negative relationship between Lag1 and market direction today can be seen:\n\nnew_data_sp500 &lt;- expand_grid(Lag1 = seq(-100, 100, 0.1))\n\nnew_data_sp500$prob &lt;- predict(logistic_model, new_data_sp500, type = \"response\")\n\nnew_data_sp500 |&gt;\n  ggplot(aes(x = Lag1, y = prob)) +\n  geom_smooth(method = \"glm\", method.args = list(family = \"binomial\"),\n              color = \"#5D001E\") +\n  scale_x_continuous(breaks = seq(-100, 100, 10), labels = seq(-100, 100, 10)) +\n  labs(x = \"Market return yesterday (%)\", y = \"Predicted Probability\", title = \"Predicted probability of market going Up today\") +\n  blog_theme\n\n\n\n\n\n\n\n\nWhen the market doesn’t change - \\(Lag1=0\\), the probability of the market going Up is \\(1.08\\) greater then the probability of it going down. The market tends to go up!\nAnd, for an increase in one unit of Lag1, the odds ratio \\(\\frac {p(Direction=Up)}{p(Direction=Down)}\\) decreases by a factor of \\(0.93\\). Let’s estimate the posterior!\n\n\nBayesian Estimation\n\nPrior elicitation\nThinking about the priors in a model with a link function can be tricky. So we will define them on the response level (in this case: odds ratio), and transform them to the link level (logit).\n\nget_prior(Direction ~ Lag1,\n          data = sp500,\n          family = bernoulli(link = \"logit\"))\n\n                prior     class coef group resp dpar nlpar lb ub       source\n               (flat)         b                                       default\n               (flat)         b Lag1                             (vectorized)\n student_t(3, 0, 2.5) Intercept                                       default\n\n\nLet’s guess that a positive return yesterday predicts a positive return today. Meaning a coefficient greater then \\(1\\) for Lag1. If the odds ratio is greater then \\(1\\), the log-odds ratio is greater then \\(ln(1)=0\\). Furthermore, the overall direction of the stock market is positive, therefore the intercept should also be greater then \\(1\\). We define a normal marginal prior with \\(\\mu=1; \\sigma=3\\) for each parameter:\n\nprior_sp500 &lt;- set_prior(\"normal(1, 3)\", coef = \"Lag1\") +\n  set_prior(\"normal(1, 3)\", class = \"Intercept\")\n\nLet’s fit!\n\nbayes_model_sp500 &lt;- brm(Direction ~ Lag1,\n                         data = sp500,\n                         family = bernoulli(link = \"logit\"),\n                         prior = prior_sp500,\n                         iter = 2000,\n                         backend = \"cmdstanr\")\n\n\nmodel_parameters(bayes_model_sp500, exponentiate = T, centrality = \"all\") |&gt; insight::print_html()\n\n\n\n\n\n\n\nModel Summary\n\n\nParameter\nMedian\nMean\nMAP\n95% CI\npd\nRhat\nESS\n\n\n\n\n(Intercept)\n1.08\n1.08\n1.08\n(0.96, 1.20)\n89.72%\n1.000\n3739.00\n\n\nLag1\n0.93\n0.93\n0.93\n(0.85, 1.03)\n91.92%\n0.999\n3351.00\n\n\n\n\n\n\n\n\n\n\n\n\nConvergence measures looking good and the parameter estimations are very similar to the Maximum Likelihood (frequentist) estimations.\n\n\nThe parameters themselves\nchains_logistic &lt;- spread_draws(bayes_model_sp500, b_Intercept, b_Lag1) |&gt;\n  mutate(across(c(b_Intercept, b_Lag1), exp)) # transforming the distributions\n\nchains_logistic |&gt;\n  ggplot(aes(x = b_Intercept, fill = after_stat(x &lt; 1))) +\n  stat_slab(color = \"gray34\") +\n  scale_fill_manual(values = c(\"#00798c\", \"#d1495b\"), labels = c(\"Positive\", \"Negative\")) +\n  geom_vline(xintercept = 1, linetype = \"dashed\") +\n  labs(x = \"Intercept\", y = NULL, fill = \"Direction of effect\") +\n  blog_theme\nchains_logistic |&gt;\n  ggplot(aes(x = b_Lag1, fill = after_stat(x &lt; 1))) +\n  stat_slab(color = \"gray34\") +\n  scale_fill_manual(values = c(\"#00798c\", \"#d1495b\"), labels = c(\"Positive\", \"Negative\")) +\n  geom_vline(xintercept = 1, linetype = \"dashed\") +\n  labs(x = \"Lag1\", y = NULL, fill = \"Direction of effect\") +\n  blog_theme\n\n\n\n\n\n\n\n\n\n\n\n\nThe line\n\nnew_data_sp500 |&gt;\n  add_linpred_draws(bayes_model_sp500, ndraws = 40, seed = 140) |&gt;\n  mutate(.linpred = exp(.linpred) / (1 + exp(.linpred))) |&gt;\n  ggplot(aes(x = Lag1, y = .linpred)) +\n  stat_lineribbon(.width = c(0.80, 0.85, 0.90), alpha = 0.7) +\n  scale_x_continuous(breaks = seq(-100, 100, 20), labels = seq(-100, 100, 20)) +\n  scale_y_continuous(breaks = seq(0, 1, 0.1), labels = seq(0, 1, 0.1)) +\n  scale_fill_brewer() +\n  labs(y = \"Probability of market going Up\", x = \"Market return yesterday\", fill = \"Credible Interval %\") +\n  blog_theme\n\n\n\n\n\n\n\n\nWe will now use the tidybayes::add_epred_draws() function in order to extract (40) individual predictions from the posterior distribution.\n\npredicted_direction &lt;- add_epred_draws(newdata = new_data_sp500, object = bayes_model_sp500, ndraws = 40, seed = 140)\n\npredicted_direction |&gt;\n  mutate(.draw = factor(.draw)) |&gt;\n  ggplot(aes(x = Lag1, y = .epred, group = .draw)) +\n  geom_line(color = \"#d1495b\") +\n  scale_x_continuous(breaks = seq(-100, 100, 10), labels = seq(-100, 100, 10)) +\n  labs(color = NULL, fill = NULL, x = \"Market return yesterday (%)\", y = \"Predicted Probability\", title = \"Posterior Predicted probability of market going Up\") +\n  guides(color = \"none\", fill = \"none\") +\n  blog_theme\n\n\n\n\n\n\n\n\nThe negative relationship is seen again, with a few odd outlier trends."
  },
  {
    "objectID": "posts/bayes102/index.html#poisson-regression---glmer",
    "href": "posts/bayes102/index.html#poisson-regression---glmer",
    "title": "Bayesian Modeling for Psychologists, Part 2",
    "section": "Poisson Regression - GLMer",
    "text": "Poisson Regression - GLMer\nFor the final example, we’ll look at a Generalized Linear Mixed effects model. That is a GLM with a hierarchical structure. Let’s look at the epilepsy dataset from brms.\n\nepilepsy &lt;- epilepsy\n\nhead(epilepsy)\n\n  Age Base Trt patient visit count obs       zAge      zBase\n1  31   11   0       1     1     5   1  0.4249950 -0.7571728\n2  30   11   0       2     1     3   2  0.2652835 -0.7571728\n3  25    6   0       3     1     2   3 -0.5332740 -0.9444033\n4  36    8   0       4     1     4   4  1.2235525 -0.8695111\n5  22   66   0       5     1     7   5 -1.0124085  1.3023626\n6  29   27   0       6     1     5   6  0.1055720 -0.1580352\n\n\nThe dataset contains information about patients in a randomized study for epilepsy treatment. Each patient’s number of seizures was recorded 4 times during a 8-week period. We will try to predict the number of seizures from session number (should decrease overtime), and the experimental group (0 = control, 1 = Treatment).\nBecause the response variable in this case is the count of seizures, we will fit a Poisson model. In this model the estimated parameter is the rate of the Poisson process - \\(\\lambda\\).\n\nepilepsy |&gt;\n  ggplot(aes(x = count)) +\n  geom_histogram(bins = 50, color = \"gray30\", fill = \"#d1495b\") +\n  labs(x = \"Number of seizures\", y = \"Count\", title = \"Looks Poisson-ish\") +\n  scale_x_continuous(breaks = seq(0, 100, 10), labels = seq(0, 100, 10)) +\n  blog_theme\n\n\n\n\n\n\n\n\n\nThe model\nBecause the rate of the Poisson process is strictly positive, the \\(log\\) link function is used.\nWe start by assuming that number of seizures is Poisson distributed:\n\\[\nCount \\sim Poisson(\\lambda)\n\\]\nLevel 1: The (log)rate of seizures for patient \\(i\\) in trial \\(j\\) is:\n\\[\nln(\\lambda_{ij})=\\beta_0+\\beta_1 \\cdot Session\n\\]\nLevel 2:\n\\[\n\\beta_{0i}=\\gamma_{00}+\\gamma_{01} \\cdot Treatment_i + \\tau_{0i}\n\\]\n\\[\n\\beta_{1i}=\\gamma_{10}+\\gamma_{11} \\cdot Treatment_i + \\tau_{1i}\n\\]\n\\[\n\\begin{bmatrix}\n      \\beta_0 \\cr\n      \\beta_1\n\\end{bmatrix} \\sim MVN(\\begin{bmatrix}\n                       \\gamma_{00} \\cr\n                       \\gamma_{10}\n                       \\end{bmatrix},\\begin{bmatrix}\n                                      \\tau_0^2 & \\tau_0\\tau_1\\rho_{01} \\cr\n                                      \\tau_1\\tau_0\\rho_{01} & \\tau_1^2\n                                      \\end{bmatrix})\n\\]\nAnd \\(\\rho_{01}\\) is the correlation coefficient between random intercepts and slopes.\n\n\nMaximum Likelihood Estimation\n\npois_model_freq &lt;- glmer(count ~ visit * Trt + (visit | patient),\n                         data = epilepsy,\n                         family = poisson(link = \"log\"))\n\n\nsjPlot::tab_model(pois_model_freq, transform = \"exp\")\n\n\n\n \ncount\n\n\nPredictors\nIncidence Rate Ratios\nCI\np\n\n\n(Intercept)\n6.46\n4.23 – 9.87\n&lt;0.001\n\n\nvisit\n0.96\n0.87 – 1.05\n0.331\n\n\nTrt [1]\n0.78\n0.43 – 1.39\n0.398\n\n\nvisit × Trt [1]\n0.99\n0.87 – 1.12\n0.836\n\n\nRandom Effects\n\n\n\nσ2\n0.19\n\n\n\nτ00 patient\n1.03\n\n\nτ11 patient.visit\n0.02\n\n\nρ01 patient\n-0.39\n\n\nICC\n0.83\n\n\nN patient\n59\n\nObservations\n236\n\n\nMarginal R2 / Conditional R2\n0.021 / 0.833\n\n\n\n\n\n\n\n\nBayesian Estimation\n\nget_prior(formula = count ~ visit * Trt + (visit | patient),\n          data = epilepsy,\n          family = poisson(link = \"log\"))\n\n                  prior     class       coef   group resp dpar nlpar lb ub\n                 (flat)         b                                         \n                 (flat)         b       Trt1                              \n                 (flat)         b      visit                              \n                 (flat)         b visit:Trt1                              \n                 lkj(1)       cor                                         \n                 lkj(1)       cor            patient                      \n student_t(3, 1.4, 2.5) Intercept                                         \n   student_t(3, 0, 2.5)        sd                                     0   \n   student_t(3, 0, 2.5)        sd            patient                  0   \n   student_t(3, 0, 2.5)        sd  Intercept patient                  0   \n   student_t(3, 0, 2.5)        sd      visit patient                  0   \n       source\n      default\n (vectorized)\n (vectorized)\n (vectorized)\n      default\n (vectorized)\n      default\n      default\n (vectorized)\n (vectorized)\n (vectorized)\n\n\n\nPrior elicitation\nWe assume that seizure rate will decrease as the study progresses (negative effect of visit). We can also assume that this decrease will be greater for patients in the treatment group - a cross-levels interaction effect.\nIf each increase of one-unit in visit lead to a decrease of 90% in seizure count - \\(\\beta_{visit}=ln(0.9)=-0.11\\). And if this effect will be 90% smaller in the treatment group then - \\(\\beta_{visitXtreatment}=ln(0.9)=-0.11\\).\n\nprior_epilepsy &lt;- set_prior(\"normal(-0.11, 3)\", coef = \"visit\") +\n  set_prior(\"normal(-0.11,3)\", coef = \"visit:Trt1\") +\n  set_prior(\"exponential(1)\", class = \"sd\") # setting prior on random effects' variance\n\n\n\nModel estimation\n\nbayes_model_epilepsy &lt;- brm(formula = count ~ visit * Trt + (visit | patient),\n                            data = epilepsy,\n                            family = poisson(link = \"log\"),\n                            prior = prior_epilepsy,\n                            chains = 4,\n                            cores = 4,\n                            iter = 3000,\n                            backend = \"cmdstanr\")\n\n\nmodel_parameters(bayes_model_epilepsy, exponentiate = T, centrality = \"all\", effects = \"all\") |&gt; insight::print_html()\n\n\n\n\n\n\n\nModel Summary\n\n\nParameter\nMedian\nMean\nMAP\n95% CI\npd\nRhat\nESS\n\n\n\n\nFixed Effects\n\n\n(Intercept)\n6.50\n6.48\n6.58\n(4.17, 9.91)\n100%\n1.002\n1473.00\n\n\nvisit\n0.95\n0.95\n0.95\n(0.87, 1.04)\n84.00%\n1.001\n3563.00\n\n\nTrt1\n0.78\n0.78\n0.79\n(0.42, 1.47)\n79.17%\n1.002\n1461.00\n\n\nvisit:Trt1\n0.99\n0.98\n0.99\n(0.87, 1.12)\n59.13%\n1.000\n3207.00\n\n\nRandom Effects\n\n\nSD (Intercept: patient)\n2.83\n2.86\n2.79\n(2.31, 3.79)\n100%\n1.000\n2415.00\n\n\nSD (visit: patient)\n1.17\n1.17\n1.16\n(1.10, 1.26)\n100%\n1.001\n2123.00\n\n\nCor (Intercept~visit: patient)\n0.70\n0.71\n0.68\n(0.52, 1.06)\n95.60%\n1.000\n3899.00\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe parameters themselves\n\nposterior_epilepsy &lt;- spread_draws(bayes_model_epilepsy, b_Intercept, b_visit, b_Trt1, !!sym(\"b_visit:Trt1\"), sd_patient__Intercept, sd_patient__visit, cor_patient__Intercept__visit) |&gt;\n  mutate(across(contains(\"b_\"), exp)) # back-transforming the regression coefficients\n\n\nRegression coefficients\n\nposterior_epilepsy |&gt;\n  ggplot(aes(x = b_Intercept, fill = \"#d1495b\")) +\n  stat_slab(color = \"gray34\") +\n  guides(color = \"none\", fill = \"none\") +\n  labs(title = \"Intercept\", y = NULL, x = NULL) +\n  scale_x_continuous(breaks = seq(0, 15, 1), labels = seq(0, 15, 1)) +\n  blog_theme +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())\n\n\n\n\n\n\n\n\n\nposterior_epilepsy |&gt;\n  select(.draw, b_visit, b_Trt1, b_visitXTrt1 = !!sym(\"b_visit:Trt1\")) |&gt;\n  pivot_longer(cols = !.draw,\n               names_to = \"variable\") |&gt;\n  ggplot(aes(x = value, y = variable, fill = after_stat(x &gt; 1))) +\n  stat_slab(color = \"gray34\") +\n  geom_vline(xintercept = 1, linetype = \"dashed\") +\n  guides(color = \"none\") +\n  scale_fill_manual(values = c(\"#d1495b\", \"#00798c\"), labels = c(\"Negative\", \"Positive\")) +\n  scale_x_continuous(breaks = seq(0, 3, 0.25), labels = seq(0, 3, 0.25)) +\n  labs(title = \"Regression coefficients\", y = NULL, x = \"Change in rate of seizures\", fill = \"Direction of effect\") +\n  blog_theme\n\n\n\n\n\n\n\n\n\np_direction(bayes_model_epilepsy, parameters = \"b_\")\n\nProbability of Direction\n\nParameter   |     pd\n--------------------\n(Intercept) |   100%\nvisit       | 84.00%\nTrt1        | 79.17%\nvisit:Trt1  | 59.13%\n\n\n\n\nRandom effects variances\n\nposterior_epilepsy |&gt;\n  select(.draw, sd_patient__Intercept, sd_patient__visit) |&gt;\n  pivot_longer(cols = !.draw,\n               names_to = \"variable\") |&gt;\n  ggplot(aes(x = value, y = variable, fill = \"#d1495b\")) +\n  stat_slab(color = \"gray34\") +\n  guides(color = \"none\", fill = \"none\") +\n  scale_x_continuous(breaks = seq(0, 2, 0.25), labels = seq(0, 2, 0.25)) +\n  labs(title = \"Random effects variances\", y = NULL, x = \"SD\") +\n  blog_theme\n\n\n\n\n\n\n\n\n\nposterior_epilepsy |&gt;\n  ggplot(aes(x = cor_patient__Intercept__visit, fill = after_stat(x &gt; 0))) +\n  stat_slab(color = \"gray34\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  guides(color = \"none\") +\n  scale_fill_manual(values = c(\"#d1495b\", \"#00798c\"), labels = c(\"Negative\", \"Positive\")) +\n  scale_x_continuous(breaks = seq(-1, 1, 0.2), labels = seq(-1, 1, 0.2)) +\n  labs(title = \"Correlation between random effects\", y = NULL, x = expression(r), fill = \"Direction of correlation\") +\n  blog_theme +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())\n\n\n\n\n\n\n\n\n\np_direction(bayes_model_epilepsy, effects = \"random\", parameters = \"cor*\")\n\nProbability of Direction SD/Cor: patient\n\nParameter         |     pd\n--------------------------\nIntercept ~ visit | 95.60%\n\n\n\n\n\nThe regression line\n\nnew_data_epilepsy &lt;- epilepsy |&gt;\n  select(patient, Trt, visit) |&gt;\n  distinct() |&gt;\n  add_epred_draws(bayes_model_epilepsy, ndraws = 40, seed = 14)\n\n\n\nCreating a 59-color & 14-color palettes\nP59 &lt;- Polychrome::createPalette(59, c(\"#543005\", \"#F5F5F5\", \"#003C30\"), range = c(30, 80))\nnames(P59) &lt;- NULL\n\nP14 &lt;- Polychrome::createPalette(14, c(\"#543005\", \"#F5F5F5\", \"#003C30\"), range = c(30, 80))\nnames(P14) &lt;- NULL\n\n\nLooking at the posterior predicted count of seizures (coloring by patient):\n\nnew_data_epilepsy |&gt;\n  mutate(Trt = case_when(Trt == 0 ~ \"Control\",\n                         Trt == 1 ~ \"Treatment\",\n                         .default = NA)) |&gt;\n  ggplot(aes(x = visit, y = .epred, color = patient, group = interaction(patient, .draw))) +\n  geom_line(show.legend = FALSE) +\n  scale_color_manual(values = P59) +\n  guides(color = \"none\") +\n  labs(y = \"Predicted count of seizures\", x = \"Visit\") +\n  scale_y_continuous(breaks = seq(0, 100, 10), labels = seq(0, 100, 10)) +\n  facet_wrap(~Trt) +\n  blog_theme\n\n\n\n\n\n\n\n\nKinda messy… let’s choose 14 random patients and plot them:\n\nset.seed(14)\n\ntreatment_patients &lt;- sample(unique(epilepsy$patient[epilepsy$Trt==1]), size = 7, replace = F)\ncontrol_patients &lt;- sample(unique(epilepsy$patient[epilepsy$Trt==0]), size = 7, replace = F)\n\nnew_data_epilepsy |&gt;\n  filter(patient %in% c(treatment_patients, control_patients)) |&gt;\n  mutate(Trt = case_when(Trt == 0 ~ \"Control\",\n                         Trt == 1 ~ \"Treatment\",\n                         .default = NA)) |&gt;\n  ggplot(aes(x = visit, y = .epred, color = patient, group = interaction(patient, .draw))) +\n  geom_line(show.legend = FALSE) +\n  scale_color_manual(values = P14) +\n  guides(color = \"none\") +\n  labs(y = \"Predicted count of seizures\", x = \"Visit\") +\n  scale_y_continuous(breaks = seq(0, 100, 10), labels = seq(0, 100, 10)) +\n  facet_wrap(~Trt) +\n  blog_theme\n\n\n\n\n\n\n\n\nThe slight overall decrease in seizure count can be seen, as well as the not-so-interesting main effect of Treatment. No visually significant interaction. Random variability in slopes of visit and intercept is also visible."
  },
  {
    "objectID": "posts/bayes102/index.html#footnotes",
    "href": "posts/bayes102/index.html#footnotes",
    "title": "Bayesian Modeling for Psychologists, Part 2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLogistic regression parameters are usually interpreted on the odds ratio of \\(p\\) instead of \\(p\\) itself.↩︎"
  },
  {
    "objectID": "posts/elections2024/index.html",
    "href": "posts/elections2024/index.html",
    "title": "US Presidential Elections - A Bayesian Perspective",
    "section": "",
    "text": "library(tidyverse) # As always\nlibrary(brms)      # For Bayesian modeling\nlibrary(tidybayes) # For visualization\nlibrary(patchwork)"
  },
  {
    "objectID": "posts/elections2024/index.html#prior-distributions",
    "href": "posts/elections2024/index.html#prior-distributions",
    "title": "US Presidential Elections - A Bayesian Perspective",
    "section": "Prior Distributions",
    "text": "Prior Distributions\nThe need to explicitly define a prior distribution for any Bayesian model is a point of contention. Some (rightfully) argue that this definition can’t be objective, and it is practically impossible to represent the existing body of knowledge in a single probability distribution. Others (rightfully) argue that every statistical model have prior assumptions which are rarely checked, and can influence the model as much as a prior distribution.1\nOne reason the incorporation of past studies into the prior distribution is hard, is that it depends on the specific parameters of the model. While this make things hard in many cases, election polling is one area where it should be relatively easy! every poll is essentially the same statistical model (how many votes for red, and how many votes for blue), but with different data every time. This is the ideal scenario for a Bayesian statistician as the posterior distribution (the result) of any poll can be used as the prior distribution for the next poll!"
  },
  {
    "objectID": "posts/elections2024/index.html#posterior-predictive-distributions",
    "href": "posts/elections2024/index.html#posterior-predictive-distributions",
    "title": "US Presidential Elections - A Bayesian Perspective",
    "section": "Posterior Predictive Distributions",
    "text": "Posterior Predictive Distributions\nOne tool of Bayesian statistical inference is the Posterior Predictive Distributions - PPD. This is the prediction of the model. For example, one possible PPD from a Bayesian election poll could be the expected electoral college result for each candidate, or the expected number of seats in the parliament."
  },
  {
    "objectID": "posts/elections2024/index.html#eliciting-a-custom-prior",
    "href": "posts/elections2024/index.html#eliciting-a-custom-prior",
    "title": "US Presidential Elections - A Bayesian Perspective",
    "section": "Eliciting a custom prior",
    "text": "Eliciting a custom prior\nFor the last model we used brms’ default prior. In order to elicit our own, For simplicity, I’ll start with the 2020 vote share between Biden and Trump:\n\n\n\n2020 Results - Nevada (from Wikipedia)\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis part contain some math. Don’t skip it, you can understand it! But if you want to skip it, go to the ‘Modeling with a custom prior’ section.\n\n\n\nprior_trump &lt;- 0.4767\nprior_harris &lt;- 0.5006\n\nThese percentages need to be transformed to the logistic model’s scale, now using the inverse-softmax transformation:\n\\[\n0.5006=softmax(Intercept_{Harris})=\\frac{e^{Intercept_{Harris}}}{\\sum_{k=1}^K e^{Intercept_k}}\n\\]\nAnd:\n\\[\n0.4767=softmax(Intercept_{Trump})=\\frac{e^{Intercept_{Trump}}}{\\sum_{k=1}^K e^{Intercept_k}}\n\\]\nBut with Trump’s intercept fixed to \\(0\\) as the factor’s reference level, it’s possible to calculate the sum in the denominator:\n\\[\n0.4767=\\frac{1}{\\sum_{k=1}^K e^{Intercept_k}}\n\\]\nQuick algebra gives:\n\\[\n\\sum_{k=1}^K e^{Intercept_k}=\\frac{1}{0.4767}=2.097755\n\\]\nNow it’s possible to calculate Harris’ intercept:\n\\[\n0.5006=\\frac{e^{Intercept_{Harris}}}{2.097755}\n\\]\n\\[\ne^{Intercept_{Harris}}=0.5006 \\cdot 2.097755=1.050136\n\\]\n\\[\nIntercept_{Harris}=ln(1.050136)=0.04891968\n\\]\nA quicker way to derive these intercepts is to notice that for every candidate \\(j\\) other than the ‘reference’ candidate \\(r\\), the intercept is:\n\\[\nIntercept_j=ln(\\frac{P(j)}{P(r)})\n\\]\nWhere \\(P(j)\\) and \\(P(r)\\) are the assumed vote shares of candidate \\(j\\) and \\(r\\) respectively."
  },
  {
    "objectID": "posts/elections2024/index.html#footnotes",
    "href": "posts/elections2024/index.html#footnotes",
    "title": "US Presidential Elections - A Bayesian Perspective",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor a more deep discussion on prior distributions, see this post.↩︎\nOr as a factor with random levels, but I don’t think it’s theoretically correct in the US presidential elections.↩︎"
  },
  {
    "objectID": "posts/micrograd/index.html",
    "href": "posts/micrograd/index.html",
    "title": "What’s Artificial Intelligence all about?",
    "section": "",
    "text": "library(tidyverse) # As always\nlibrary(MASS)      # Sampling from a multivariate distributions\nlibrary(plotly)    # For 3D plots\n\nOK, but what is AI actually about? Over the past two summers, I taught a statistics and research methods course to psychology students. Generally speaking, these students tend to be a little intimidated by this field, and as always, I tried over the summer to both empower them and spark an interest in them in the beauty and ‘coolness’ of statistics.\nOne idea they liked was when I told them: If you understand linear regression, you understand Chat GPT. While this is obviously a simplification, I managed to convince them it is not far from the truth. If simple linear modeling is about finding the best two parameters (a slope and an intercept) to approximate a function, AI is about finding the best millions to billions of parameters to approximate a function. In this blog post I will explain one of the basic and most important concepts of modern AI - Gradient Descent using simple (as possible) terms and some R code."
  },
  {
    "objectID": "posts/micrograd/index.html#optimal-minimum-loss-intro-to-loss-functions",
    "href": "posts/micrograd/index.html#optimal-minimum-loss-intro-to-loss-functions",
    "title": "What’s Artificial Intelligence all about?",
    "section": "Optimal = Minimum loss (intro to loss functions)",
    "text": "Optimal = Minimum loss (intro to loss functions)\nNow we can calculate the loss for every model we fit, therefore the loss is actually a function of the model and is called the Loss Function.\n\n\n\n\n\n\nNote\n\n\n\nModels can differ in two main ways: in their structure (for example: a linear function and a 5-th degree polynomial), or in the values of their parameters (for example: these two linear functions: \\(y=2x-4\\), \\(y=3x+5\\)). With the model structure usually chosen a-priori (or through a separate experiment), we will focus on the loss as a function of the parameters’ values.\n\n\nSo our task is to find the set of parameters \\(\\beta\\) that minimize the loss function. Let’s formalize it for the linear model case: Given a linear model: \\(\\hat{y}=\\beta_0+\\beta_1x\\), our goal is to find the minimum of:\n\\[\nloss(\\beta_0,\\beta_1)=\\frac{1}{N} \\sum_{i=1}^{N}(y_i-\\beta_0-\\beta_1x_i)\n\\]\nBut, there is the catch - this function does not have a minimum point! This is because the lines we saw before can be as negative as we want, just make \\(\\beta_0\\) and \\(\\beta_1\\) negative enough. We can also notice that this function is linear with respect to \\(\\beta_0\\) and \\(\\beta1\\), therefore it has no minimum or maximum points.\nIn order to solve this, we simply define the loss function to be the mean Square distance from the line - also known as the Variance.\n\\[\nloss(\\beta_0,\\beta_1)=\\frac{1}{N} \\sum_{i=1}^{N}(y_i-\\hat{y_i})^2=\\frac{1}{N} \\sum_{i=1}^{N}(y_i-\\beta_0-\\beta_1x_i)^2\n\\]"
  },
  {
    "objectID": "posts/micrograd/index.html#derivatives",
    "href": "posts/micrograd/index.html#derivatives",
    "title": "What’s Artificial Intelligence all about?",
    "section": "Derivatives",
    "text": "Derivatives\nThe derivative of a function (with respect to some variable \\(x\\)) at some point is the slope of the tangent line to the function at this point, In other words, how much the function “goes up (or down)” at this point.\nConsider this simple function:\n\nf &lt;- function(x) {\n  x^2 - 3*x + 2\n}\n\nIt’s graph looks like this:\n\nx &lt;- seq(-10, 10, 0.1)\ny &lt;- f(x)\n\nggplot(data.frame(x = x, y = y), aes(x, y)) +\n  geom_point() +\n  theme_classic()\n\n\n\n\n\n\n\n\nWhat is the derivative of the function at the point \\(x=6\\)?\n\ndf &lt;- data.frame(x = x, y = y)\n\nggplot(df, aes(x, y)) +\n  geom_point() +\n  geom_point(data = df[df$x == 6,], aes(x, y), color = \"red\", size = 3) +\n  theme_classic()\n\n\n\n\n\n\n\n\nThe function goes up at this point, therefore the derivative is positive. But exactly How positive? Or in other words, if we move a little bit to the right of the red point, by how much does the function increase?\n\nf6 &lt;- f(6) # the value of the function at x = 6\nh &lt;- 0.0001 # \"a little bit\"\nfh &lt;- f(6 + h)\n\n(fh - f6) / h # standardizing by the amount we increased\n\n[1] 9.0001\n\n\nThe slope of the function at \\(x=6\\) is \\(9\\).\nWhat is the derivative of the function at \\(x=-5\\)?\n\n(f(-5 + h) - f(-5)) / h\n\n[1] -12.9999\n\n\nAs expected and as we can see in the graph, this derivative is negative.\nWhy do we care? imagine that we stand on the point \\(x=6\\) and we want to “go up”, in what direction we should advance? If the function is increasing at this point - The derivative is positive - then we want to go right (increase \\(X\\)). But, if the function is decreasing at this point - The derivative is negative - then we want to go left (decrease \\(X\\)). Notice that in both scenarios we want to go In the direction of the derivative.\nSo, what if we stand on the point \\(x=6\\) and we want to “go down”? Simple, just go In the opposite direction of the derivative! Let’s see how that works:\nAgain, we stand on \\(x=6\\):\n\n\n\n\n\n\n\n\n\nCalculating the derivative:\n\nx0 &lt;- 6\nfx0 &lt;- f(x0) # the value of the function at x = 6\nh &lt;- 0.0001 # \"a little bit\"\nfh &lt;- f(x0 + h)\n\n(fh - fx0) / h # standardizing by the amount we increased\n\n[1] 9.0001\n\n\nStepping in the Opposite direction:\n\nstep_size &lt;- 0.1 # controlling the size of our step\nx1 &lt;- x0 + -9*step_size\nx1\n\n[1] 5.1\n\n\n\n\n\n\n\n\n\n\n\nAgain!\ncalculating the derivative:\n\nfx1 &lt;- f(x1) # the value of the function at x = 5.1\nh &lt;- 0.0001 # \"a little bit\"\nfh &lt;- f(x1 + h)\n\n(fh - fx1) / h # standardizing by the amount we increased\n\n[1] 7.2001\n\n\nStepping in the opposite direction:\n\nstep_size &lt;- 0.1 # controlling the size of our step\nx2 &lt;- x1 + -7.2*step_size\nx2\n\n[1] 4.38\n\n\nWhere we landed?\n\n\n\n\n\n\n\n\n\nAnd this is Stochastic Gradient Descent (SGD) - Descending down the gradient (derivative) in a loop (in a stochastic manner).\nWhat about multivariate functions? for example: \\(f(x_1, x_2)=3x_1-4x_2+x_1x_2-5\\). In this case we can calculate the derivative of each variable with respect to the outcome, also called the partial derivatives: \\[\n\\frac{\\partial y}{\\partial x_1}, \\frac{\\partial y}{\\partial x_2}\n\\]\nFor example, let’s calculate the partial derivative of \\(x_1\\) with respect to \\(y\\) in the point \\(x_1=-2, x_2=3\\):\n\ng &lt;- function(x1, x2) {\n  return(3*x1 - 4*x2 + x1*x2 - 5)\n}\n\ng0 &lt;- g(-2, 3)\ng0\n\n[1] -29\n\n\nWhat happens when we increase \\(x_1\\) by a little?\n\nh &lt;- 0.0001 # a little\ngh &lt;- g(-2 + h, 3) # notice how x2 remains constant\n\n(gh - g0) / h\n\n[1] 6\n\n\nThe partial derivative of \\(x_1\\) at the point \\(x_1=-2, x_2=3\\) is \\(6\\). Now we just need to calculate the partial derivative of \\(x_2\\) at the same point and descend the gradient!"
  },
  {
    "objectID": "posts/micrograd/index.html#simple-linear-regression",
    "href": "posts/micrograd/index.html#simple-linear-regression",
    "title": "What’s Artificial Intelligence all about?",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\nReturning to the case of a simple linear regression, we see that the whole thing is just about some function with two parameters that we need to minimize.\n\\[\nloss(\\beta_0,\\beta_1)=\\frac{1}{N} \\sum_{i=1}^{N}(y_i-\\beta_0-\\beta_1x_i)^2\n\\]\nLet’s estimate the partial derivatives of \\(\\beta_0\\) and \\(\\beta_1\\) with respect to the loss at the point \\(\\beta_0=2, \\beta_1=0.5\\):\n\n\n\n\n\n\nNote\n\n\n\nThe need to calculate the derivative of the loss function justifies it being the mean of Square, as opposed to Absolute, errors as they easier to differentiate.\n\n\n\nbeta0 &lt;- 2\nbeta1 &lt;- 0.5\n\nloss0 &lt;- mean((d$X2 - beta0 - beta1*d$X1)^2)\nloss0\n\n[1] 0.6821639\n\n\nCalculating partial derivatives:\n\n# Partial derivative of beta0\nloss1 &lt;- mean((d$X2 - (beta0 + 0.001) - beta1*d$X1)^2)\n\nbeta0_grad &lt;- (loss1 - loss0) / 0.001\n\n# Partial derivative of beta1\nloss1 &lt;- mean((d$X2 - beta0 - (beta1 + 0.001)*d$X1)^2)\n\nbeta1_grad &lt;- (loss1 - loss0) / 0.001\n\nTaking a step:\n\nstep_size &lt;- 0.1\n\nbeta0 &lt;- beta0 - step_size*beta0_grad\nbeta1 &lt;- beta1 - step_size*beta1_grad\n\nCalculating the loss again:\n\nmean((d$X2 - beta0 - beta1*d$X1)^2)\n\n[1] 0.6442667\n\n\nThe loss went down!\nAutomating the process in a loop:\n\nset.seed(14)\n\nX &lt;- d$X1\nY &lt;- d$X2\n\n# Hyper Parameters\nlearning_rate &lt;- 0.1 # formerly known as step_size\nh &lt;- 0.001           # small change in parameter for calculating derivatives\nepochs &lt;- 80         # Number of iterations\n\n# Initialing random values for parameters\nbeta0 &lt;- rnorm(1)\nbeta1 &lt;- rnorm(1)\n\nfor (i in c(1:epochs)) {\n  \n  # Forward Pass (calculating model predictions)\n  Y_hat &lt;- beta0 + beta1*X\n  \n  # Calculating Loss\n  loss &lt;- mean((Y - Y_hat)^2)\n  \n  # Printing loss every 10th iteration\n  if (i %% 5 == 0) {\n    print(glue::glue(\"Epoch {i}: Loss: {loss}\"))\n  }\n  \n  # Calculating Gradients\n  beta0_grad &lt;- (mean((Y - (beta0 + h) - beta1*X)^2) - loss) / h\n  beta1_grad &lt;- (mean((Y - beta0 - (beta1 + h)*X)^2) - loss) / h\n  \n  # Updating Parameters\n  beta0 &lt;- beta0 - learning_rate*beta0_grad\n  beta1 &lt;- beta1 - learning_rate*beta1_grad\n}\n\nEpoch 5: Loss: 1.80038967350815\nEpoch 10: Loss: 1.01512495422098\nEpoch 15: Loss: 0.74136306380543\nEpoch 20: Loss: 0.635632474184838\nEpoch 25: Loss: 0.594395955672709\nEpoch 30: Loss: 0.578298619591311\nEpoch 35: Loss: 0.572014177360529\nEpoch 40: Loss: 0.569560667340541\nEpoch 45: Loss: 0.568602778731536\nEpoch 50: Loss: 0.568228797188623\nEpoch 55: Loss: 0.568082782199662\nEpoch 60: Loss: 0.568025770526887\nEpoch 65: Loss: 0.568003508715428\nEpoch 70: Loss: 0.567994814993631\nEpoch 75: Loss: 0.5679914192996\nEpoch 80: Loss: 0.567990092591508\n\n\nWhat are the final parameters?\n\nprint(beta0)\n\n[1] 1.61333\n\nprint(beta1)\n\n[1] 0.6348897\n\n\nWhat are the “real” parameters?\n\ncoef(lm(Y ~ X))\n\n(Intercept)           X \n  1.6145082   0.6342556 \n\n\nAlmost the same."
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Tomer Zipori’s resume",
    "section": "",
    "text": "Tomer Zipori\n\n\n\n\n\n tomerzip@post.bgu.ac.il\n Github\n Twitter\n LinkedIn\n Personal Blog\n 054-2520843\n\n\n\n\n\nExperience in Frequentist and Bayesian statistical analysis, statistical learning models, and data visualization methods.\nExperience and knowledge in many Machine Learning methods: Classification, Regression and Natural Language Processing."
  },
  {
    "objectID": "cv.html#contact",
    "href": "cv.html#contact",
    "title": "Tomer Zipori’s resume",
    "section": "",
    "text": "tomerzip@post.bgu.ac.il\n Github\n Twitter\n LinkedIn\n Personal Blog\n 054-2520843"
  },
  {
    "objectID": "cv.html#skills",
    "href": "cv.html#skills",
    "title": "Tomer Zipori’s resume",
    "section": "",
    "text": "Experience in Frequentist and Bayesian statistical analysis, statistical learning models, and data visualization methods.\nExperience and knowledge in many Machine Learning methods: Classification, Regression and Natural Language Processing."
  },
  {
    "objectID": "cv.html#title",
    "href": "cv.html#title",
    "title": "Tomer Zipori’s resume",
    "section": "Tomer Zipori",
    "text": "Tomer Zipori\n\nGeneral Information\nMA in Cognitive Psychology and Data-science. I have expertise and enthusiasm in everything Social science and data science. As a quick and independent learner, I have acquired many data science skills during my time in academia such as: Data visualization, Machine learning, data communication and more."
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Tomer Zipori’s resume",
    "section": "Education",
    "text": "Education\n\nBen-Gurion University of the Negev\nM.A. in Cognitive Psychology and Data Science\nBe’er Sheva, Israel\n2022 - Now\nThesis: Effects of Social-Status and Ethnicity on face individuation.\nSupervisor: Dr. Niv Reggev\n\n\nHebrew University in Jerusalem\nB.A. in Psychology and Philosophy\nJerusalem, Israel\n2018 - 2021\nThesis: Inter-personal differences in the effect of parental modeling on sharing behavior during middle childhood.\nSupervisor: Prof. Ariel Knafo-Noam\nGraduated Magna Cum Laude"
  },
  {
    "objectID": "cv.html#research-experience",
    "href": "cv.html#research-experience",
    "title": "Tomer Zipori’s resume",
    "section": "Research Experience",
    "text": "Research Experience\n\nComputational Social Psychology (SCP)\nBen-Gurion University of the Negev\nBe’er Sheva, Israel\n2024 - Now\n\nInvestigated how different psychological interventions improved the ability of participants at detecting fake-news.\nPerformed statistical analysis, visualizations and inferences for a meta-analysis.\nSelf-learning various statistical methods including Ordinal Regression, Frequentist and Bayesian machine learning and meta-analyses of effect sizes.\n\n\n\nSocial, Cognition, Motivation and Brain Lab (SCMB)\nBen-Gurion University of the Negev\nBe’er Sheva, Israel\n2022 - 2024\n\nInvestigated how social-status of faces affects their identification by others.\nCreated and implemented protocols for data organization and data sharing via documentation and the use of Github.\nManaging a team of research assistants.\nGuiding students in their research projects in the lab.\n\n\n\nGraduate Research Assistant\nThe Social Development Lab, Hebrew University in Jerusalem\nJerusalem, Israel\n2019 - 2020\n\nParticipation in the “Young Researchers” and writing an extended seminar term paper."
  },
  {
    "objectID": "cv.html#professional-experience",
    "href": "cv.html#professional-experience",
    "title": "Tomer Zipori’s resume",
    "section": "Professional Experience",
    "text": "Professional Experience\n\nData Quality Specialist\nPlanck\nIsrael\n2024 - Now\n\n\nWas part of the data tagging and validation\n\n\n\n\nContent Writer\nClassit\nIsrael\n2021 - 2023\n\n\nWrote high-school level math questions and teaching units."
  },
  {
    "objectID": "cv.html#teaching-experience",
    "href": "cv.html#teaching-experience",
    "title": "Tomer Zipori’s resume",
    "section": "Teaching Experience",
    "text": "Teaching Experience\n\nIntroduction to Statistics\nTeaching Assistant in the Intro to statistics course for BA students.\nBe’er Sheva, Israel\n2022 - 2024\n\n\nMethodology in experimental Psychology\nTeaching Assistant in the Methodology in experimental Psychology. course for BA students. As part of the course I have guided groups of students in forming research questions, planning and conducting experiments, statistically analyzing results, and forming conclusions.\nBe’er Sheva, Israel\n2022 - 2023\n\n\nMITAM Instructor\nOfek MITAM\nIsrael\n2023 - Now\nTeaching a class of the Ofek MITAM course. The MITAM is the entrance test for advanced degrees in psychology - equivalent to the GRE."
  }
]