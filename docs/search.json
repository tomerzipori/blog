[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog posts",
    "section": "",
    "text": "Netflix trends\n\n\n\n\n\n\n\ncode\n\n\ntext analysis\n\n\nvisualization\n\n\nNLP\n\n\n\n\nText data visualization and analysis\n\n\n\n\n\n\nJun 13, 2023\n\n\nTomer Zipori\n\n\n\n\n\n\n  \n\n\n\n\nReal Vs. Fake news in football\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nMachine Learning\n\n\nNLP\n\n\n\n\nExperiment in NLP and some text classification\n\n\n\n\n\n\nMay 12, 2023\n\n\nTomer Zipori\n\n\n\n\n\n\n  \n\n\n\n\nWhat makes FIFA 23 players good?\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nMachine Learning\n\n\n\n\nExperimentation with Elastic -net regression and decision tree boosting.\n\n\n\n\n\n\nMay 6, 2023\n\n\nTomer Zipori\n\n\n\n\n\n\n  \n\n\n\n\nWho is the leading US state in UFO reports? probably not what you thought\n\n\n\n\n\n\n\ncode\n\n\nvisualization\n\n\nggplot2\n\n\ntidytuesday\n\n\n\n\nVisualizing UFO reports per state capita\n\n\n\n\n\n\nApr 16, 2023\n\n\nTomer Zipori\n\n\n\n\n\n\n  \n\n\n\n\nCage-free vs. caged hens in the US\n\n\n\n\n\n\n\ncode\n\n\nvisualization\n\n\nggplot2\n\n\ntidytuesday\n\n\n\n\n\n\n\n\n\n\n\nApr 12, 2023\n\n\nTomer Zipori\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tomer Zipori",
    "section": "",
    "text": "Data science & data visualization nerd. I’m studying for my MA in Cognitive Psychology at Ben-Gurion University of the Negev. I hope to record and share my journey in studying and applying cool data-science stuff!"
  },
  {
    "objectID": "posts/eggs/index.html",
    "href": "posts/eggs/index.html",
    "title": "Cage-free vs. caged hens in the US",
    "section": "",
    "text": "Some time ago, when the fall semester began I’ve enrolled on a course called Data science lab as part of the ‘Data Science for the Social Sciences’ program. One of the assignments was to make some TidyTuesday contribution and present it in class. So this is my submission and what I consider as my first respectable attempts at visualizing data."
  },
  {
    "objectID": "posts/eggs/index.html#libraries",
    "href": "posts/eggs/index.html#libraries",
    "title": "Cage-free vs. caged hens in the US",
    "section": "Libraries",
    "text": "Libraries\n\nlibrary(tidytuesdayR) # for easy data loading\nlibrary(tidyverse)    # for data pre-processing and wrangling\nlibrary(lubridate)    # makes dealing with date format much easier\nlibrary(showtext)     # fonts"
  },
  {
    "objectID": "posts/eggs/index.html#loading-fonts",
    "href": "posts/eggs/index.html#loading-fonts",
    "title": "Cage-free vs. caged hens in the US",
    "section": "Loading fonts",
    "text": "Loading fonts\nshowtext is an awesome package that allows to load installed fonts into R and use it in ggplot2 plots (for example). For a really helpful video that I used see this video from the Riffomonas Project.\n\nfont_add(family = \"Stencil\", regular = \"STENCIL.TTF\")\nshowtext_auto()"
  },
  {
    "objectID": "posts/eggs/index.html#taking-a-peak",
    "href": "posts/eggs/index.html#taking-a-peak",
    "title": "Cage-free vs. caged hens in the US",
    "section": "Taking a peak",
    "text": "Taking a peak\n\nhead(eggproduction)\n\n# A tibble: 6 × 6\n  observed_month prod_type     prod_process   n_hens     n_eggs source          \n  &lt;date&gt;         &lt;chr&gt;         &lt;chr&gt;           &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;           \n1 2016-07-31     hatching eggs all          57975000 1147000000 ChicEggs-09-23-…\n2 2016-08-31     hatching eggs all          57595000 1142700000 ChicEggs-10-21-…\n3 2016-09-30     hatching eggs all          57161000 1093300000 ChicEggs-11-22-…\n4 2016-10-31     hatching eggs all          56857000 1126700000 ChicEggs-12-23-…\n5 2016-11-30     hatching eggs all          57116000 1096600000 ChicEggs-01-24-…\n6 2016-12-31     hatching eggs all          57750000 1132900000 ChicEggs-02-28-…\n\n\neggproduction holds monthly data about number of hens and produced eggs in the US. prod_type specifies the type of egg produced, it has 2 levels:\n\nunique(eggproduction$prod_type)\n\n[1] \"hatching eggs\" \"table eggs\"   \n\n\nFor the current mini-project, I’ll stay with “table eggs” only.\nThe variable prod_process specifies the type of housing of the egg-producing hens. It has 3 levels:\n\nunique(eggproduction$prod_process)\n\n[1] \"all\"                     \"cage-free (non-organic)\"\n[3] \"cage-free (organic)\"    \n\n\nI’ll leave data of all hens for now.\n\nPre-processing 1\nFiltering out irrelevant data and renaming some variables.\n\negg_clean &lt;- eggproduction %&gt;%\n  filter(prod_type != \"hatching eggs\" & prod_process == \"all\") %&gt;% # Leave only eggs meant for eating and general data\n  select(-source, -prod_type, -prod_process, n_hens_all = n_hens, n_eggs_all = n_eggs) # Irrelevant columns\n\n\n\nTaking a peak 2\n\nhead(cagefreepercentages)\n\n# A tibble: 6 × 4\n  observed_month percent_hens percent_eggs source                             \n  &lt;date&gt;                &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;                              \n1 2007-12-31              3.2           NA Egg-Markets-Overview-2019-10-19.pdf\n2 2008-12-31              3.5           NA Egg-Markets-Overview-2019-10-19.pdf\n3 2009-12-31              3.6           NA Egg-Markets-Overview-2019-10-19.pdf\n4 2010-12-31              4.4           NA Egg-Markets-Overview-2019-10-19.pdf\n5 2011-12-31              5.4           NA Egg-Markets-Overview-2019-10-19.pdf\n6 2012-12-31              6             NA Egg-Markets-Overview-2019-10-19.pdf\n\n\nThis data-frame also holds monthly data. The variable percent_hens specifies observed or computed percentage of cage-free hens relative to all table-egg-laying hens (from the Github repo). We’ll select these variables and use them to merge with the first data-frame.\n\negg_clean2 &lt;- cagefreepercentages %&gt;%\n  drop_na(percent_eggs) %&gt;%                                                # droping rows with missing percent_eggs data\n  select(-source, -percent_eggs, cagefree_percent_hens = percent_hens) %&gt;% # Irrelevant\\to many NA's columns + renaming\n  inner_join(egg_clean, by = \"observed_month\", multiple = \"all\") %&gt;%       # joining with the first data-frame\n  mutate(Cagefree = (cagefree_percent_hens * n_hens_all) / 100) %&gt;%        # calculating number of cage-free hens\n  mutate(Traditional = n_hens_all - Cagefree) %&gt;%                          # calculating number of traditional housing hens\n  select(observed_month, Cagefree, Traditional) %&gt;%\n  pivot_longer(cols = c(\"Cagefree\", \"Traditional\"), names_to = \"housing\", values_to = \"n_hens\")"
  },
  {
    "objectID": "posts/eggs/index.html#gameplan",
    "href": "posts/eggs/index.html#gameplan",
    "title": "Cage-free vs. caged hens in the US",
    "section": "Gameplan",
    "text": "Gameplan\nFew hours deep, I’ve decided it would be interesting to see the change in number of cage-free hens compared to caged hens during the time period we have data about. Because I wanted to plot only certain points of data along the time axis, I needed to create a subset of the big data-frame that holds the data for the points I wanted to plot.\n\nIdentitfying the dates of interest\nFirst thing, I found 6 dates that are equally spaced between the start and end points. The repetitive code below is quite ugly, and lubridate probably has a nice and elegant solution, I didn’t want to spend to much time on it.\n\ndates_for_plot &lt;- seq.Date(egg_clean2$observed_month[1], egg_clean2$observed_month[nrow(egg_clean2)], length.out = 6)\ndates_for_plot[2] &lt;- as.Date(\"2017-07-31\")\ndates_for_plot[3] &lt;- as.Date(\"2018-05-31\")\ndates_for_plot[4] &lt;- as.Date(\"2019-04-30\")\ndates_for_plot[5] &lt;- as.Date(\"2020-03-31\")\n\n\n\nCreating the subset\nI will use this subset in order to plot the 6 points on the general data. I will only need to specify data = subset_for_points in the relevant geom object.\n\nsubset_for_points &lt;- egg_clean2 %&gt;%\n  filter((housing == \"Cagefree\" & observed_month %in% dates_for_plot) |\n           (housing == \"Traditional\" & (observed_month == dates_for_plot[1] | observed_month == dates_for_plot[6]))) %&gt;%\n  inner_join(select(egg_clean, observed_month, n_hens = n_hens_all), by = \"observed_month\") %&gt;%\n  mutate(n_hens = case_when(housing == \"Cagefree\" ~ n_hens.x,\n                            housing == \"Traditional\" ~ n_hens.y)) %&gt;%\n  select(-n_hens.x, -n_hens.y)"
  },
  {
    "objectID": "posts/eggs/index.html#actually-plotting",
    "href": "posts/eggs/index.html#actually-plotting",
    "title": "Cage-free vs. caged hens in the US",
    "section": "Actually plotting",
    "text": "Actually plotting\nI went with a simple stacked density plot. Watch how the number of cage-free goes from\n\n\nCode\nplot &lt;- egg_clean2 %&gt;%\n  ggplot(aes(x = observed_month, y = n_hens/1000000, fill = factor(housing, levels = c(\"Traditional\", \"Cagefree\")),\n             color = factor(housing, levels = c(\"Traditional\", \"Cagefree\")),\n             label = n_hens/1000000)) +\n  geom_density(position = 'stack', stat = 'identity') +\n  geom_point(data = subset_for_points) +\n  geom_text(data = subset_for_points, aes(label = round(n_hens/1000000)), hjust = 0.5, vjust = -1, size = 6, family = \"serif\") +\n  scale_fill_manual(values = c(\"#ffefd5\", \"#e1bf92\")) +\n  scale_color_manual(values = c(\"#e1bf92\", \"#83502e\")) +\n  annotate(geom = \"text\", x = as_date(\"2019/1/1\"), y = 30, label = \"Cage-Free\", color = \"#808080\",\n           family = \"Stencil\", angle = 2.5, size = 15, alpha = 0.5) +\n  annotate(geom = \"text\", x = as_date(\"2019/1/1\"), y = 190, label = \"Caged\", color = \"#808080\",\n           family = \"Stencil\", angle = 2.5, size = 15, alpha = 0.5) +\n  scale_x_date(breaks = dates_for_plot) +\n  xlab(\"\") +\n  ylab(\"Number of egg-producing hens (millions)\") +\n  labs(fill = \"Housing\", title = \"Number of Cage-free hens in the US is constatly rising\",\n       subtitle = \"Relative number of Cage-free hens in the US in the years 2016-2021\",\n       caption = \"Tomer Zipori | #TidyTuesday | Source: The Humane League's US Egg Production dataset\") +\n  guides(color = \"none\", fill = \"none\") +\n  theme_classic() +\n  theme(axis.title = element_text(size = 16, color = \"#83502e\"),\n        axis.text.x = element_text(size = 13, color = \"#83502e\"),\n        axis.text.y = element_text(size = 13, color = \"#83502e\"),\n        plot.title = element_text(hjust = 0.5, size = 18, color = \"#83502e\"),\n        plot.subtitle = element_text(hjust = 0.5, size = 13, family = \"serif\", color = \"#83502e\"),\n        plot.caption = element_text(family = \"serif\", color = \"#83502e\"),\n        plot.margin = margin(0.5,0.5,0.5,0.7, \"cm\"),\n        plot.background = element_rect(fill = \"#fffaf0\"),\n        panel.background = element_rect(fill = \"#fffaf0\"))\nplot"
  },
  {
    "objectID": "posts/fifa23/index.html",
    "href": "posts/fifa23/index.html",
    "title": "What makes FIFA 23 players good?",
    "section": "",
    "text": "The current Data is an upload to Kaggle by Babatunde Zenith, and it includes information about players in the popular FIFA 23 video game. Information includes: name, age, nationality, position, various football ratings and contract deals.\nThe current notebook is an attempt at:\n      1. Accurately and efficiently predicting player’s overall rating.\n      2. Identifying important variables (features) for this prediction.\nBoth goals will be achieved using two methods: Elastic-net regression and Decision tree Boosting. Data pre-processing will be done with tidyverse, Model fitting and evaluation will be done with the caret and gbm packages."
  },
  {
    "objectID": "posts/fifa23/index.html#re-naming-columns",
    "href": "posts/fifa23/index.html#re-naming-columns",
    "title": "What makes FIFA 23 players good?",
    "section": "Re-naming columns",
    "text": "Re-naming columns\nReplacing spaces with underscores for ease.\n\nnames(players) &lt;- str_replace_all(names(players), pattern = \" \", replacement = \"_\")"
  },
  {
    "objectID": "posts/fifa23/index.html#non-numeric-variables",
    "href": "posts/fifa23/index.html#non-numeric-variables",
    "title": "What makes FIFA 23 players good?",
    "section": "non-numeric variables",
    "text": "non-numeric variables\nFirst we’ll look at potential garbage variables.\n\nnames(select(players, where(is.character)))\n\n [1] \"Known_As\"                    \"Full_Name\"                  \n [3] \"Positions_Played\"            \"Best_Position\"              \n [5] \"Nationality\"                 \"Image_Link\"                 \n [7] \"Club_Name\"                   \"Club_Position\"              \n [9] \"Contract_Until\"              \"Club_Jersey_Number\"         \n[11] \"On_Loan\"                     \"Preferred_Foot\"             \n[13] \"National_Team_Name\"          \"National_Team_Image_Link\"   \n[15] \"National_Team_Position\"      \"National_Team_Jersey_Number\"\n[17] \"Attacking_Work_Rate\"         \"Defensive_Work_Rate\"        \n\n\nAlmost all garbage data. Since I’ve noted that Work Rate variables are ordered (low-medium-high) We’ll re-code them:\n\nplayers &lt;- players %&gt;%\n  mutate(Attacking_Work_Rate = case_when(Attacking_Work_Rate == \"Low\" ~ 1,\n                                         Attacking_Work_Rate == \"Medium\" ~ 2,\n                                         Attacking_Work_Rate == \"High\" ~ 3),\n         Defensive_Work_Rate = case_when(Defensive_Work_Rate == \"Low\" ~ 1,\n                                         Defensive_Work_Rate == \"Medium\" ~ 2,\n                                         Defensive_Work_Rate == \"High\" ~ 3)) %&gt;%\n  select(-Known_As, -Full_Name, -Positions_Played, -Nationality, -Image_Link, -Club_Name, -Contract_Until, -Club_Jersey_Number, -National_Team_Name, -National_Team_Image_Link, -National_Team_Jersey_Number, -On_Loan) %&gt;% # getting rid of garbage variables\n  mutate(across(where(is.character), ~na_if(., \"-\"))) # replacing all \"-\" with NA"
  },
  {
    "objectID": "posts/fifa23/index.html#searching-for-variables-with-large-number-of-nas",
    "href": "posts/fifa23/index.html#searching-for-variables-with-large-number-of-nas",
    "title": "What makes FIFA 23 players good?",
    "section": "Searching for variables with large number of NA’s",
    "text": "Searching for variables with large number of NA’s\n\ncolSums(is.na(players))\n\n                 Overall                Potential           Value(in_Euro) \n                       0                        0                        0 \n           Best_Position                      Age            Height(in_cm) \n                       0                        0                        0 \n           Weight(in_kg)               TotalStats                BaseStats \n                       0                        0                        0 \n           Wage(in_Euro)           Release_Clause            Club_Position \n                       0                        0                       86 \n               Joined_On           Preferred_Foot         Weak_Foot_Rating \n                       0                        0                        0 \n             Skill_Moves International_Reputation   National_Team_Position \n                       0                        0                    16746 \n     Attacking_Work_Rate      Defensive_Work_Rate               Pace_Total \n                       0                        0                        0 \n          Shooting_Total            Passing_Total          Dribbling_Total \n                       0                        0                        0 \n         Defending_Total        Physicality_Total                 Crossing \n                       0                        0                        0 \n               Finishing         Heading_Accuracy            Short_Passing \n                       0                        0                        0 \n                 Volleys                Dribbling                    Curve \n                       0                        0                        0 \n       Freekick_Accuracy              LongPassing              BallControl \n                       0                        0                        0 \n            Acceleration             Sprint_Speed                  Agility \n                       0                        0                        0 \n               Reactions                  Balance               Shot_Power \n                       0                        0                        0 \n                 Jumping                  Stamina                 Strength \n                       0                        0                        0 \n              Long_Shots               Aggression            Interceptions \n                       0                        0                        0 \n             Positioning                   Vision                Penalties \n                       0                        0                        0 \n               Composure                  Marking          Standing_Tackle \n                       0                        0                        0 \n          Sliding_Tackle        Goalkeeper_Diving      Goalkeeper_Handling \n                       0                        0                        0 \n       GoalkeeperKicking   Goalkeeper_Positioning      Goalkeeper_Reflexes \n                       0                        0                        0 \n               ST_Rating                LW_Rating                LF_Rating \n                       0                        0                        0 \n               CF_Rating                RF_Rating                RW_Rating \n                       0                        0                        0 \n              CAM_Rating                LM_Rating                CM_Rating \n                       0                        0                        0 \n               RM_Rating               LWB_Rating               CDM_Rating \n                       0                        0                        0 \n              RWB_Rating                LB_Rating                CB_Rating \n                       0                        0                        0 \n               RB_Rating                GK_Rating \n                       0                        0 \n\n\nNational team position seems sparse, we’ll have to get rid of club_position as well for the model fitting. We’ll also get rid of best_position because it creates so much dummy vars. I’ll analyzed it in another day…\n\nplayers &lt;- select(players, -National_Team_Position, -Club_Position, -Best_Position)"
  },
  {
    "objectID": "posts/fifa23/index.html#data-splitting",
    "href": "posts/fifa23/index.html#data-splitting",
    "title": "What makes FIFA 23 players good?",
    "section": "Data splitting",
    "text": "Data splitting\n\nset.seed(14)\ntrain_id &lt;- createDataPartition(y = players$Overall, p = 0.7, list = F)\n\nplayers_train &lt;- players[train_id,]\nplayers_test &lt;- players[-train_id,]"
  },
  {
    "objectID": "posts/fifa23/index.html#tuning-grid-for-hyper-parameters",
    "href": "posts/fifa23/index.html#tuning-grid-for-hyper-parameters",
    "title": "What makes FIFA 23 players good?",
    "section": "Tuning grid for hyper-parameters",
    "text": "Tuning grid for hyper-parameters\n\ntg &lt;- expand.grid(alpha = c(seq(0, 1, length.out = 25)),\n                  lambda = c(2 ^ seq(10, -10, length = 100)))\n\nSetting a relatively large range of hyper-parameters because elastic-net regression is not super expansive computationally."
  },
  {
    "objectID": "posts/fifa23/index.html#training",
    "href": "posts/fifa23/index.html#training",
    "title": "What makes FIFA 23 players good?",
    "section": "Training",
    "text": "Training\n\nelastic_reg &lt;- train(Overall ~ ., \n                    data = players_train,\n                    method = \"glmnet\",\n                    preProcess = c(\"center\", \"scale\"), # for better interpatation of coefficients\n                    tuneGrid = tg,\n                    trControl =  trainControl(method = \"cv\", number = 10)) # 10-fold Cross-Validation"
  },
  {
    "objectID": "posts/fifa23/index.html#best-hyper-parameters",
    "href": "posts/fifa23/index.html#best-hyper-parameters",
    "title": "What makes FIFA 23 players good?",
    "section": "Best hyper-parameters",
    "text": "Best hyper-parameters\n\nelastic_reg$bestTune\n\n     alpha       lambda\n1501 0.625 0.0009765625"
  },
  {
    "objectID": "posts/fifa23/index.html#traincv-error",
    "href": "posts/fifa23/index.html#traincv-error",
    "title": "What makes FIFA 23 players good?",
    "section": "Train/CV error",
    "text": "Train/CV error\n\nplot(elastic_reg, xTrans = log, digits = 3)\n\n\n\nelastic_reg$results[elastic_reg$results$RMSE == min(elastic_reg$results$RMSE, na.rm = T),]\n\n     alpha       lambda     RMSE  Rsquared     MAE     RMSESD  RsquaredSD\n1501 0.625 0.0009765625 1.601089 0.9440492 1.24766 0.04469899 0.003446449\n          MAESD\n1501 0.02690937\n\n\nAll mixes of \\(\\alpha\\) and \\(\\lambda\\) hyper-parameters converge in the end."
  },
  {
    "objectID": "posts/fifa23/index.html#model-coefficients",
    "href": "posts/fifa23/index.html#model-coefficients",
    "title": "What makes FIFA 23 players good?",
    "section": "Model coefficients",
    "text": "Model coefficients\n\nelasnet_coeffs &lt;- coef(elastic_reg$finalModel, s = elastic_reg$bestTune$lambda)\nplot(elasnet_coeffs, ylab = \"Coefficient\")\n\n\n\nround(elasnet_coeffs, 4)\n\n73 x 1 sparse Matrix of class \"dgCMatrix\"\n                              s1\n(Intercept)              65.9420\nPotential                 2.3510\n`Value(in_Euro)`          0.6037\nAge                       2.0174\n`Height(in_cm)`          -0.1111\n`Weight(in_kg)`           0.0979\nTotalStats               -2.7794\nBaseStats                 0.0004\n`Wage(in_Euro)`           0.2468\nRelease_Clause           -0.2689\nJoined_On                 0.0710\nPreferred_FootRight      -0.0694\nWeak_Foot_Rating         -0.0397\nSkill_Moves               0.3644\nInternational_Reputation -0.2143\nAttacking_Work_Rate      -0.0729\nDefensive_Work_Rate      -0.1001\nPace_Total                0.6887\nShooting_Total            0.4371\nPassing_Total             0.6242\nDribbling_Total           1.4907\nDefending_Total          -0.0937\nPhysicality_Total         0.9278\nCrossing                  0.3602\nFinishing                -0.4256\nHeading_Accuracy          0.8768\nShort_Passing             0.2001\nVolleys                   0.0255\nDribbling                -1.3137\nCurve                     0.0000\nFreekick_Accuracy         0.1731\nLongPassing              -0.6509\nBallControl               0.1516\nAcceleration              0.0289\nSprint_Speed             -0.1612\nAgility                  -0.1264\nReactions                 1.1084\nBalance                  -0.0032\nShot_Power               -0.0565\nJumping                   0.0664\nStamina                   0.0582\nStrength                 -0.1455\nLong_Shots               -0.3293\nAggression               -0.1703\nPositioning              -1.1545\nVision                   -0.7001\nPenalties                 0.1108\nComposure                 0.4431\nMarking                   0.6215\nStanding_Tackle           0.2082\nSliding_Tackle            0.2331\nGoalkeeper_Diving         0.1745\nGoalkeeper_Handling      -0.0091\nGoalkeeperKicking         0.0767\nGoalkeeper_Positioning   -0.0696\nGoalkeeper_Reflexes      -0.0828\nST_Rating                 2.5495\nLW_Rating                -0.0648\nLF_Rating                 0.0000\nCF_Rating                 0.0000\nRF_Rating                 0.0000\nRW_Rating                 .     \nCAM_Rating               -0.1751\nLM_Rating                 0.5492\nCM_Rating                 1.9575\nRM_Rating                 0.0129\nLWB_Rating                .     \nCDM_Rating                1.3227\nRWB_Rating                .     \nLB_Rating                -0.0021\nCB_Rating                -0.9501\nRB_Rating                 .     \nGK_Rating                 0.6368\n\n\nThe intercept is quite large. Let’s look at the variables in a more informative scale.\n\nplot(elasnet_coeffs[-1,], ylab = \"Coefficient\")"
  },
  {
    "objectID": "posts/fifa23/index.html#test-error",
    "href": "posts/fifa23/index.html#test-error",
    "title": "What makes FIFA 23 players good?",
    "section": "Test error",
    "text": "Test error\n\nelasticreg_pred &lt;- predict(elastic_reg, newdata = players_test) # calculating model's prediction for test set\n\nTest error and effect size\n\\(RMSE=1.60955711701293\\)\n\\(R^2=0.944839845538989\\)\nVery nice!"
  },
  {
    "objectID": "posts/fifa23/index.html#training-control",
    "href": "posts/fifa23/index.html#training-control",
    "title": "What makes FIFA 23 players good?",
    "section": "Training control",
    "text": "Training control\nWe’ll use adaptive cross-validation in order to make the hyper-parameter search more efficient.\nFor further explanation on implementation in R see. For further reading on theory see.\n\ntr &lt;- trainControl(method = \"adaptive_cv\",\n                   number = 10, repeats = 10,\n                   adaptive = list(min = 5, alpha = 0.05, \n                                   method = \"BT\", complete = TRUE),\n                   search = \"random\")"
  },
  {
    "objectID": "posts/fifa23/index.html#training-1",
    "href": "posts/fifa23/index.html#training-1",
    "title": "What makes FIFA 23 players good?",
    "section": "Training",
    "text": "Training\n\nset.seed(14)\nboost_model &lt;- train(Overall ~ ., \n                   data = players_train,\n                   method = \"gbm\",\n                   trControl = tr, # No explicit tuning grid is needed\n                   verbose = T)"
  },
  {
    "objectID": "posts/fifa23/index.html#traincv-error-1",
    "href": "posts/fifa23/index.html#traincv-error-1",
    "title": "What makes FIFA 23 players good?",
    "section": "Train/CV error",
    "text": "Train/CV error\nGetting the results of the best tuning parameters found.\n\nboost_model$results[boost_model$results$RMSE == min(boost_model$results$RMSE, na.rm = T),5:10]\n\n       RMSE  Rsquared       MAE      RMSESD   RsquaredSD       MAESD\n2 0.7146858 0.9893686 0.5457707 0.005980442 0.0002966158 0.002603466\n\n\nSeems quite optimized, but is it overfitted?"
  },
  {
    "objectID": "posts/fifa23/index.html#test-error-1",
    "href": "posts/fifa23/index.html#test-error-1",
    "title": "What makes FIFA 23 players good?",
    "section": "Test error",
    "text": "Test error\n\nboost_pred &lt;- predict(boost_model, players_test)\n\nTest error and effect size\n\\(RMSE=0.700272523649518\\)\n\\(R^2=0.989645490170188\\)\nVery Very nice!"
  },
  {
    "objectID": "posts/fifa23/index.html#variable-importance",
    "href": "posts/fifa23/index.html#variable-importance",
    "title": "What makes FIFA 23 players good?",
    "section": "Variable importance",
    "text": "Variable importance\n\nvarimp &lt;- caret::varImp(boost_model, scale = T)\n\nvarimp\n\ngbm variable importance\n\n  only 20 most important variables shown (out of 72)\n\n                        Overall\n`Value(in_Euro)`       100.0000\nReactions               50.2939\nBaseStats               16.4047\nAge                      7.5742\n`Wage(in_Euro)`          3.8700\nPotential                3.6002\nCB_Rating                2.3274\nDefending_Total          1.0533\nGoalkeeper_Positioning   0.6619\nCrossing                 0.3162\nTotalStats               0.2916\nShooting_Total           0.2559\nStrength                 0.2359\nPositioning              0.2113\nStanding_Tackle          0.1991\nLF_Rating                0.1927\nRelease_Clause           0.1814\nDribbling_Total          0.1650\nLB_Rating                0.1578\nHeading_Accuracy         0.1497\n\n\n\nPlotting variable importance\n\n\nShow the plot’s code\n# data preparation\nvarimp$importance %&gt;%\n  rownames_to_column(var = \"Feature\") %&gt;%\n  dplyr::rename(Importance = Overall) %&gt;%\n  filter(Importance != 0) %&gt;% # Only features that have an above 0 importance\n  \n  # Plotting\n  ggplot(aes(x = reorder(Feature, -Importance), y = Importance)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip(ylim = c(0, 100)) +\n  scale_y_continuous(limits = c(0,100), expand = c(0, 0)) +\n  labs(x = \"Feature\", y = \"Importance\", title = \"Variable importance in boosted model\", caption = \"Tomer Zipori | FIFA 23 Player Research by Babatunde Zenith | Kaggle\") +\n  theme_classic() +\n  theme(axis.text.y = element_text(size = 7),\n        plot.title = element_text(size = 16, hjust = 0.5),\n        plot.margin = unit(c(1,1,1,1), \"cm\"),\n        plot.caption = element_text(size = 6, hjust = 0.5, vjust = -5))\n\n\n\n\n\nPlayer value is the strongest predictor by far, with a few interesting ones right behind it (CB_rating?)."
  },
  {
    "objectID": "posts/football_fake_news/index.html",
    "href": "posts/football_fake_news/index.html",
    "title": "Real Vs. Fake news in football",
    "section": "",
    "text": "As part of my data-science courses this fall semester, I have started also to get into Natural Language Processing (NLP, but of the good kind). This is the first time I have ever tried it, so let’s see how it went.\nBrief overview of the post:\n      1. Basic preprocessing and creation of Data Feature Matrix (DFM), using packages stringr and quanteda.\n      2. Classification with Naive-Bayes method.\n      3. Classification with Logistic regression, and feature importance plot."
  },
  {
    "objectID": "posts/football_fake_news/index.html#adding-labels",
    "href": "posts/football_fake_news/index.html#adding-labels",
    "title": "Real Vs. Fake news in football",
    "section": "Adding labels",
    "text": "Adding labels\n\nreal$label &lt;- \"real\"\nfake$label &lt;- \"fake\""
  },
  {
    "objectID": "posts/football_fake_news/index.html#combining-data-frames",
    "href": "posts/football_fake_news/index.html#combining-data-frames",
    "title": "Real Vs. Fake news in football",
    "section": "Combining data frames",
    "text": "Combining data frames\n\ntweets &lt;- rbind(real, fake) |&gt;\n  drop_na() |&gt; # dropping NA rows\n  mutate(tweet = str_trim(tweet) |&gt; str_squish()) # removing white-spaces in the start and beginning of tweet, and between words"
  },
  {
    "objectID": "posts/football_fake_news/index.html#creating-dfms-cleaning-text",
    "href": "posts/football_fake_news/index.html#creating-dfms-cleaning-text",
    "title": "Real Vs. Fake news in football",
    "section": "Creating DFMs & cleaning text",
    "text": "Creating DFMs & cleaning text\nCreating a count-based DFM (frequency of each word in each document). Before creating the DFM I employ 3 steps of normalization:\n      1. Removing special characters and expressions (punctuation, urls…).\n      2. Converting words to their stem-form (the word example converts to exampl). This step should decrease       the number of unique tokens, so that our DFM would be less sparse.\n      3. Converting every character to lower-case letters.\n\ndata_feature_mat &lt;- corp |&gt;\n  tokens(remove_punct = T, remove_numbers = T, remove_url = T, remove_separators = T, remove_symbols = T) |&gt;\n  tokens_wordstem() |&gt;\n  dfm(tolower = T)\n\nhead(data_feature_mat)\n\nDocument-feature matrix of: 6 documents, 18,869 features (99.89% sparse) and 1 docvar.\n       features\ndocs    sun down technic director al-ah respect us and play to\n  text1   1    1       1        1     1       1  1   1    1  1\n  text2   0    0       0        0     0       0  0   0    0  2\n  text3   0    0       1        1     0       0  0   0    0  0\n  text4   0    0       0        0     0       0  0   1    0  0\n  text5   0    0       0        0     0       0  0   0    0  1\n  text6   0    0       0        0     0       0  0   0    0  0\n[ reached max_nfeat ... 18,859 more features ]"
  },
  {
    "objectID": "posts/football_fake_news/index.html#naive-bayes-classifier",
    "href": "posts/football_fake_news/index.html#naive-bayes-classifier",
    "title": "Real Vs. Fake news in football",
    "section": "Naive-Bayes classifier",
    "text": "Naive-Bayes classifier\nA Naive-Bayes classifier predicts the class (in this case of a document) in the following way:\n\n\nThe likelihood of each token to appear in documents of each class is calculated from the training data. For example, if the word manager appeared in \\(11\\)% of the documents in the first class, and in \\(2\\)% of the documents in the second class, the likelihood of it in each class is:\n\\[\n\\displaylines{p(manager\\ |\\ class\\ 1)=0.11\\\\p(manager\\ |\\ class\\ 2)=0.02}\n\\]\nThe prior probability of each document to be classified to each class is also learned from the training data, and it is the base-rate frequencies of the two classes.\n\nFor each document in the test set, the likelihood of it given it is from each of the classes is calculated by multiplying the likelihoods of the tokens appearing in it. So if a document is for example the sentence I like turtles, then the likelihood of it to belong class 1 is:\n\\[\n\\displaylines{p(I\\ |\\ class\\ 1)\\cdotp(like\\ |\\ class\\ 1)\\cdotp(turtles\\ |\\ class\\ 1)}\n\\]\nMore formally, if a document belong to a certain class \\(k\\), then it’s likelihood of being comprised of a set of tokens \\(t\\) is:\n\\[\n\\prod_{i=1}^{n}p(t_i\\ |\\ class\\ k)\n\\]\nAccording to Bayes theorem, the probability of the document to belong to class \\(k\\) - the posterior probability - is proportional to the product of the likelihood of it’s tokens given this class and the prior probability of any document to belong to this class:\n\\[\np(class\\ k\\ |\\ t) \\propto p(t\\ |\\ class\\ k)\\cdotp(class\\ k)\n\\]\nBecause the Naive-Bayes classifier is comparing between classes, the standardizing term is not needed. The class that has the largest product of prior and likelihood is the class the document will be classified to."
  },
  {
    "objectID": "posts/football_fake_news/index.html#fitting-the-model",
    "href": "posts/football_fake_news/index.html#fitting-the-model",
    "title": "Real Vs. Fake news in football",
    "section": "Fitting the model",
    "text": "Fitting the model\n\nnb_model &lt;- textmodel_nb(train_dfm, y = docvars(train_dfm, \"label\"))"
  },
  {
    "objectID": "posts/football_fake_news/index.html#test-performance",
    "href": "posts/football_fake_news/index.html#test-performance",
    "title": "Real Vs. Fake news in football",
    "section": "Test performance",
    "text": "Test performance\n\npred_nb &lt;- predict(nb_model, newdata = test_dfm)\n\n(conmat_nb &lt;- table(pred_nb, docvars(test_dfm, \"label\")))\n\n       \npred_nb fake real\n   fake 3893  380\n   real  143 3956\n\n\nSeems nice, let’s look at some metrics.\nConfusion matrix\n\ncaret::confusionMatrix(conmat_nb, mode = \"everything\", positive = \"real\")\n\nConfusion Matrix and Statistics\n\n       \npred_nb fake real\n   fake 3893  380\n   real  143 3956\n                                          \n               Accuracy : 0.9375          \n                 95% CI : (0.9321, 0.9426)\n    No Information Rate : 0.5179          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.8752          \n                                          \n Mcnemar's Test P-Value : &lt; 2.2e-16       \n                                          \n            Sensitivity : 0.9124          \n            Specificity : 0.9646          \n         Pos Pred Value : 0.9651          \n         Neg Pred Value : 0.9111          \n              Precision : 0.9651          \n                 Recall : 0.9124          \n                     F1 : 0.9380          \n             Prevalence : 0.5179          \n         Detection Rate : 0.4725          \n   Detection Prevalence : 0.4896          \n      Balanced Accuracy : 0.9385          \n                                          \n       'Positive' Class : real            \n                                          \n\n\nNice results!"
  },
  {
    "objectID": "posts/football_fake_news/index.html#logistic-regression",
    "href": "posts/football_fake_news/index.html#logistic-regression",
    "title": "Real Vs. Fake news in football",
    "section": "Logistic regression",
    "text": "Logistic regression\nThe nice thing about the textmodel_lr method from the quanteda.textmodels package is that it does the Cross-Validation for us!\n\nlr_model &lt;- textmodel_lr(x = train_dfm, y = docvars(train_dfm, \"label\"))"
  },
  {
    "objectID": "posts/football_fake_news/index.html#test-performance-1",
    "href": "posts/football_fake_news/index.html#test-performance-1",
    "title": "Real Vs. Fake news in football",
    "section": "Test performance",
    "text": "Test performance\n\npred_lr &lt;- predict(lr_model, newdata = test_dfm)\n\n(conmat_lr &lt;- table(pred_lr, docvars(test_dfm, \"label\")))\n\n       \npred_lr fake real\n   fake 3795  188\n   real  241 4148\n\n\nAlso seems nice.\nConfusion matrix\n\ncaret::confusionMatrix(conmat_lr, mode = \"everything\", positive = \"real\")\n\nConfusion Matrix and Statistics\n\n       \npred_lr fake real\n   fake 3795  188\n   real  241 4148\n                                          \n               Accuracy : 0.9488          \n                 95% CI : (0.9438, 0.9534)\n    No Information Rate : 0.5179          \n    P-Value [Acc &gt; NIR] : &lt; 2e-16         \n                                          \n                  Kappa : 0.8973          \n                                          \n Mcnemar's Test P-Value : 0.01205         \n                                          \n            Sensitivity : 0.9566          \n            Specificity : 0.9403          \n         Pos Pred Value : 0.9451          \n         Neg Pred Value : 0.9528          \n              Precision : 0.9451          \n                 Recall : 0.9566          \n                     F1 : 0.9508          \n             Prevalence : 0.5179          \n         Detection Rate : 0.4955          \n   Detection Prevalence : 0.5242          \n      Balanced Accuracy : 0.9485          \n                                          \n       'Positive' Class : real            \n                                          \n\n\nSlight improvement…"
  },
  {
    "objectID": "posts/football_fake_news/index.html#plot-important-words-for-classification",
    "href": "posts/football_fake_news/index.html#plot-important-words-for-classification",
    "title": "Real Vs. Fake news in football",
    "section": "Plot important words for classification",
    "text": "Plot important words for classification\n\n\nCode\nlr_summary &lt;- summary(lr_model) # summarizing the model\n\ncoefs &lt;- data.frame(lr_summary$estimated.feature.scores) # extracting coefficients\n\n\ncol_vec &lt;- c(\"#fc7753\", \"#e3e3e3\", \"#66d7d1\", \"black\")\n\ncoefs |&gt;\n  \n  # preparing df for plot\n  rownames_to_column(var = \"Token\") |&gt;\n  rename(Coefficient = real) |&gt;\n  filter(Coefficient != 0 & Token != \"(Intercept)\") |&gt;\n  mutate(bigger_then_0 = Coefficient &gt; 0) |&gt;\n  \n  # ggplotting\n  ggplot(aes(x = Token, y = Coefficient, color = bigger_then_0)) +\n  geom_point() +\n  scale_color_manual(values = c(col_vec[1], col_vec[3])) +\n  scale_y_continuous(n.breaks = 10) +\n  labs(title = \"Most important words for classifying if a tweet is fake news\",\n       x = \"\") +\n  theme_classic() +\n  geom_hline(yintercept = 0, color = \"black\", linetype = 2, linewidth = 1, show.legend = F) +\n  theme(plot.background = element_rect(color = col_vec[2], fill = col_vec[2]),\n        panel.background = element_rect(color = col_vec[2], fill = col_vec[2]),\n        axis.line = element_line(color = col_vec[4]),\n        axis.title = element_text(color = col_vec[4]),\n        axis.text = element_text(color = col_vec[4]),\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1, size = 12, face = \"bold\"),\n        legend.position = \"none\",\n        plot.title = element_text(size = 16, color = col_vec[4], hjust = .5, family = \"serif\", face = \"bold\"))"
  },
  {
    "objectID": "posts/netflix/index.html",
    "href": "posts/netflix/index.html",
    "title": "Netflix trends",
    "section": "",
    "text": "This post will be dedicated yet again to some experimentation with text analysis. I will be using mostly basic and classic NLP tools like word counting and DFM’s (Document-Feature matrix).\nI hope that in the next NLP post I will be using some more advanced methods like transformers (go BERT!).\nI will be using the popular Netflix dataset from Kaggle. It contains data of movies and TV shows added to Netflix’s streaming service since 2008.\nIt’s going to be a relatively short post, the first part will include some nice visualization (word clouds), and in the second part I will try and build a mini show-recommendation function."
  },
  {
    "objectID": "posts/netflix/index.html#text-analysis-of-show-description",
    "href": "posts/netflix/index.html#text-analysis-of-show-description",
    "title": "Netflix trends",
    "section": "Text analysis of show description",
    "text": "Text analysis of show description\nHere I’m creating the DFM. It is basically a huge matrix with each document (show description) being a row, and each unique word being a column. Values represent the frequency of each word in each document.\n\ndata_clean_dfm &lt;- data_clean |&gt;\n  mutate(description = str_remove_all(description, pattern = \"[[:punct:]]\")) |&gt;\n  corpus(text_field = \"description\") |&gt;\n  tokens(remove_separators = T) |&gt;\n  tokens_remove(stopwords()) |&gt;\n  dfm()\n\ndata_clean_dfm\n\nDocument-feature matrix of: 8,807 documents, 20,824 features (99.93% sparse) and 13 docvars.\n       features\ndocs    father nears end life filmmaker kirsten johnson stages death inventive\n  text1      1     1   1    1         1       1       1      1     1         1\n  text2      0     0   0    0         0       0       0      0     0         0\n  text3      0     0   0    0         0       0       0      0     0         0\n  text4      0     0   0    0         0       0       0      0     0         0\n  text5      0     0   0    1         0       0       0      0     0         0\n  text6      0     0   0    0         0       0       0      0     0         0\n[ reached max_ndoc ... 8,801 more documents, reached max_nfeat ... 20,814 more features ]\n\n\nFor example: The word ‘father’ appear once in the first document, and doesn’t appear in documents 2 to 6."
  },
  {
    "objectID": "posts/netflix/index.html#tf-idf-term-frequency---inverse-document-frequency",
    "href": "posts/netflix/index.html#tf-idf-term-frequency---inverse-document-frequency",
    "title": "Netflix trends",
    "section": "TF-IDF (Term Frequency - Inverse Document Frequency)",
    "text": "TF-IDF (Term Frequency - Inverse Document Frequency)\n\nTF - Term Frequency\nUp until now I have used the most basic DFM - the count DFM. It is basically counts the number of times each word appear in the document, and is sometimes also called ‘Bag of Words’.\nOne problem that arises in our inner statistician is that longer documents contain more words! therefore, different term frequencies can be hard to compare. The simple solution is to calculate Proportional frequency in the following way:\n\\[\nterm \\ frequency= \\frac{N \\ times \\ a \\ word \\ appear \\ in \\ the \\ document}{Total \\ number \\ of \\ words \\ in \\ the \\ document}\n\\]\nThis gives us the Term frequency part of the tf-idf method.\n\n\nInverse Document Frequency\nSo we have the term frequency of every token (word). We can start and make show recommendations, right? One problem that still exist is that some words could be prevalent in every document. For example, if the word ‘Life’ appears in a large number of descriptions, it is not so useful in identifying someones taste in TV shows and movies. If, on the other hand, the word ‘Father’ appear only in a small number of descriptions, it will be a lot more helpful to us. Therefore, we want to represent term Uniqueness. How can we do that?\nFor each token (word) the Inverse-Document-Frequency is calculated:\n\\[\nidf=log( \\frac{Total \\ number \\ of \\ documents}{N \\ documents \\ containing \\ the \\ term})\n\\]\nThis number is getting close to zero the more documents containing the word, and is getting larger the less documents contain it (and the more documents we have).\nTo get the final tf-idf value for every term, we simply multiply the Term-Frequency with the Inverse-Document-Frequency:\n\\[\ntfidf(word)=tf(word) \\cdot idf(word)\n\\]\nIn creating the tf-idf DFM, I have also converted every word to lower case and to it’s stem.\n\nnetflix_tf_idf &lt;- data_clean_dfm |&gt;\n  dfm_tolower() |&gt;\n  dfm_wordstem() |&gt;\n  dfm_tfidf()"
  },
  {
    "objectID": "posts/netflix/index.html#cosine-similarity",
    "href": "posts/netflix/index.html#cosine-similarity",
    "title": "Netflix trends",
    "section": "Cosine Similarity",
    "text": "Cosine Similarity\nNow that we have the DFM, we have a vector representation of every description of every show. Every word in our dataset is a dimension and it’s tf-idf in each document is the vector component. For example, the vector representation of the description of “Attack on Titan” is:\n\ndfm_subset(netflix_tf_idf, subset = title == \"Attack on Titan\")\n\nDocument-feature matrix of: 1 document, 15,097 features (99.90% sparse) and 13 docvars.\n         features\ndocs      father near end life filmmak kirsten johnson stage death invent\n  text779      0    0   0    0       0       0       0     0     0      0\n[ reached max_nfeat ... 15,087 more features ]\n\n\nNaturally these vector are mostly zeros…\nBut now that we have vectors we can do all the cool things that we can do with vectors! For example, we can recommend TV shows and movies based on how they are similar to a previously watched show.\nIf we return to Physics class, we can think of vectors as arrows with length and direction.\n\n\n\nTwo vectors\n\n\nIt is natural to think of the similarity between two vectors as the measure to which they point in the same direction. In other words, the angle between two vectors. In two dimensions, it is possible to calculate the cosine between two vectors the following way:\n\\[\ncos \\theta= \\frac{ \\vec{a} \\cdot \\vec{b}}{|\\vec{a}| \\cdot |\\vec{b}|}\n\\]\nWithout turning this into a real Physics class, in the numerator is the dot-product of the vectors, and in the denominator is the product of lengths of the vectors. For example, the cosine between the vectors \\(\\vec{a}=(1,1)\\) and \\(\\vec{b}=(-4,3)\\) is:\n\\[\ncos \\theta= \\frac{1 \\cdot (-4)+1 \\cdot (3)}{ \\sqrt{1^2+1^2} \\cdot \\sqrt{(-4)^2+3^2}}=\\frac{-1}{5 \\sqrt{2}}\n\\]\nBut what is the angle between ~15,000 dimensions vectors?! fortunately, this calculation remains the same in any dimension.\nWhat is the relation between the angle and it’s cosine? without digging too deep, the larger the angle, the smaller the cosine. So the larger the cosine, the greater the similarity.\n\n\n\n\n\n\nNote\n\n\n\nNice insight: the cosine similarity is nothing but the un-standardized pearson correlation coefficient! Let \\(x\\) and \\(y\\) be vectors of Z scores. The correlation coefficient is:\n\\[\nr_{xy}= \\frac{\\sum_{i=1}^{N}{x_i \\cdot y_i}}{N}\n\\] While the cosine of the angle between them is:\n\\[\ncos \\theta= \\frac{\\vec{x} \\cdot \\vec{y}}{|\\vec{x}| \\cdot |\\vec{y}|}\n\\] The length of a vector of Z scores is \\(\\sqrt{N}\\), therefore the denominator is always: \\(\\sqrt{N} \\cdot \\sqrt{N}=N\\).\nThe nominator is the dot product of the vectors, which is exactly the sum of the by-component products. Finally we get:\n\\[\nr_{xy}=cos \\ \\theta_{xy}\n\\]\n\n\n\nFunctions\nThe main idea of the recommendation generator is simple. If you liked a TV show or a movie, you will probably like shows with similar description. Not any similarity, cosine similarity!\n\nget_recommendation &lt;- function(show, liked = T) {\n  library(dplyr)\n  library(quanteda)\n  \n  features &lt;- netflix_tf_idf@docvars\n  show_id &lt;- features$docname_[tolower(features$title) %in% tolower(show)]\n  show_id &lt;- as.integer(str_remove_all(show_id, pattern = \"text\"))\n  \n  simil_mat &lt;- textstat_simil(netflix_tf_idf[show_id,], netflix_tf_idf, method = \"cosine\")\n  \n  if (liked) {\n  simil_df &lt;- data.frame(shows = simil_mat@Dimnames[[2]],\n                      simil = simil_mat@x) |&gt;\n  arrange(-simil) |&gt;\n  inner_join(select(netflix_tf_idf@docvars, docname_, title),\n             by = join_by(shows == docname_)) |&gt;\n    select(-shows, match = simil) |&gt;\n    mutate(match = (match-min(match))/(max(match)-min(match))) |&gt;\n    head(11)\n  }\n  if (!liked){\n    simil_df &lt;- data.frame(shows = simil_mat@Dimnames[[2]],\n                      simil = simil_mat@x) |&gt;\n  arrange(simil) |&gt;\n  inner_join(select(netflix_tf_idf@docvars, docname_, title),\n             by = join_by(shows == docname_)) |&gt;\n    select(-shows, match = simil) |&gt;\n    mutate(match = 1-(match-min(match))/(max(match)-min(match))) |&gt;\n    head(11)\n  }\n  \n  return(simil_df[-1,])\n}\n\nget_recommendation_plot &lt;- function(show) {\n  library(ggplot2)\n  \n  plot_df &lt;- get_recommendation(show)\n  \n  ggplot(plot_df, aes(reorder(title, -match, identity), match, fill = match)) +\n  geom_col() +\n  labs(x = \"Show\", y = \"Match\") +\n  scale_fill_gradient(low = \"#000000\", high = \"#990011FF\") +\n  theme_classic() +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),\n        legend.position = \"none\")\n}\n\n\n\nGetting show recommendations\nLet’s use an example. Maybe I watched “Attack on Titan”, what are the top 10 recommended shows for me?\n\nwatched &lt;- \"Attack on Titan\"\n\nget_recommendation(watched)\n\n       match                               title\n2  0.1570728 Little Singham aur Kaal ka Mahajaal\n3  0.1444591                             The BFG\n4  0.1367644                    Sym-Bionic Titan\n5  0.1339566                           The Giant\n6  0.1335654                               Agent\n7  0.1227649                Pup Star: World Tour\n8  0.1207346              Power Battle Watch Car\n9  0.1159523                         Rising High\n10 0.1149643           League of Legends Origins\n11 0.1136349                A Sort of Homecoming\n\n\nWhat if I did not like a show? no problem, you will be recommended shows and movies that are not similar.\n\nget_recommendation(\"I Am Sam\", liked = FALSE)\n\n   match                               title\n2      1                           Ganglands\n3      1               Jailbirds New Orleans\n4      1                        Kota Factory\n5      1    My Little Pony: A New Generation\n6      1                             Sankofa\n7      1       The Great British Baking Show\n8      1                        The Starling\n9      1 Vendetta: Truth, Lies and The Mafia\n10     1                    Bangkok Breaking\n11     1    Confessions of an Invisible Girl\n\n\n\nIn a plot\n\nget_recommendation_plot(watched)"
  },
  {
    "objectID": "posts/ufo/index.html",
    "href": "posts/ufo/index.html",
    "title": "Who is the leading US state in UFO reports? probably not what you thought",
    "section": "",
    "text": "As I told in another post, I’ve recently enrolled to a course about Data-Viz (and maybe some NLP later-on, stay tuned). One of the assignments in the course was to make some TidyTuesday contribution and present it in class. Although this is not what I’ve presented in class, this was made shortly after and I think it came out quite nice :)"
  },
  {
    "objectID": "posts/ufo/index.html#initial-pre-processing",
    "href": "posts/ufo/index.html#initial-pre-processing",
    "title": "Who is the leading US state in UFO reports? probably not what you thought",
    "section": "Initial pre-processing",
    "text": "Initial pre-processing\n\nufo_sightings &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-06-25/ufo_sightings.csv\")\n\nusa &lt;- map_data(\"state\") # US map\n\nstate_codes &lt;- read_csv(\"state_code.csv\") %&gt;% # converting from state name to 2-letter code and back\n  select(state, code) %&gt;%\n  mutate(state = tolower(state), code = tolower(code))\n\nuspop &lt;- read_excel(\"uspop.xlsx\", col_names = c(\"region\", \"pop_2010\", \"pop_2011\", \"pop_2012\", \"pop_2013\", \"pop_2014\")) %&gt;% # info about state population\n  mutate(region = tolower(str_remove(region, \".\"))) %&gt;%\n  rowwise() %&gt;%\n  mutate(mean_pop = mean(c(pop_2010, pop_2011, pop_2012, pop_2013, pop_2014))) %&gt;%\n  ungroup() %&gt;%\n  select(region, mean_pop)\n\nUS population info taken from the United States Census Bureau."
  },
  {
    "objectID": "posts/ufo/index.html#glimpsing-at-the-data",
    "href": "posts/ufo/index.html#glimpsing-at-the-data",
    "title": "Who is the leading US state in UFO reports? probably not what you thought",
    "section": "Glimpsing at the data",
    "text": "Glimpsing at the data\n\nglimpse(ufo_sightings)\n\nRows: 80,332\nColumns: 11\n$ date_time                  &lt;chr&gt; \"10/10/1949 20:30\", \"10/10/1949 21:00\", \"10…\n$ city_area                  &lt;chr&gt; \"san marcos\", \"lackland afb\", \"chester (uk/…\n$ state                      &lt;chr&gt; \"tx\", \"tx\", NA, \"tx\", \"hi\", \"tn\", NA, \"ct\",…\n$ country                    &lt;chr&gt; \"us\", NA, \"gb\", \"us\", \"us\", \"us\", \"gb\", \"us…\n$ ufo_shape                  &lt;chr&gt; \"cylinder\", \"light\", \"circle\", \"circle\", \"l…\n$ encounter_length           &lt;dbl&gt; 2700, 7200, 20, 20, 900, 300, 180, 1200, 18…\n$ described_encounter_length &lt;chr&gt; \"45 minutes\", \"1-2 hrs\", \"20 seconds\", \"1/2…\n$ description                &lt;chr&gt; \"This event took place in early fall around…\n$ date_documented            &lt;chr&gt; \"4/27/2004\", \"12/16/2005\", \"1/21/2008\", \"1/…\n$ latitude                   &lt;dbl&gt; 29.88306, 29.38421, 53.20000, 28.97833, 21.…\n$ longitude                  &lt;dbl&gt; -97.941111, -98.581082, -2.916667, -96.6458…"
  },
  {
    "objectID": "posts/ufo/index.html#defining-functions",
    "href": "posts/ufo/index.html#defining-functions",
    "title": "Who is the leading US state in UFO reports? probably not what you thought",
    "section": "Defining functions",
    "text": "Defining functions\nSome helper function will help us later, mainly to work with dates.\nconvert_to_date takes a vector of character formatted dates and converts it to lubridate’s date format. floor_decade takes a vector of dates and converts it to a vector of decades.\n\nconvert_to_date &lt;- function(x) { \n  sub_string &lt;- str_sub(x, 1, 10)\n  d &lt;- mdy(sub_string)\n  return(as.numeric(d))\n}\nfloor_decade &lt;- function(x){\n  return(lubridate::year(x) - lubridate::year(x) %% 10)\n  }\n\n\nConverting the dates\n\nufo_sightings &lt;- ufo_sightings %&gt;%\n  mutate(date = as_date(purrr::map_dbl(date_time, ~convert_to_date(.)))) # Convert to 'Date' format. Run only once, its slow af"
  },
  {
    "objectID": "posts/ufo/index.html#globals",
    "href": "posts/ufo/index.html#globals",
    "title": "Who is the leading US state in UFO reports? probably not what you thought",
    "section": "Globals",
    "text": "Globals\nHere I’m loading some images and fonts that will be of use later to beautify the plot.\n\nnightsky_img &lt;- \"nightsky2.jpg\"\n\n#font_files() %&gt;% tibble() %&gt;% filter(str_detect(family, \"Showcard Gothic\"))\nfont_add(family = \"Showcard Gothic\", regular = \"SHOWG.TTF\")\nshowtext_auto()"
  },
  {
    "objectID": "posts/ufo/index.html#data-pre-processing",
    "href": "posts/ufo/index.html#data-pre-processing",
    "title": "Who is the leading US state in UFO reports? probably not what you thought",
    "section": "Data pre-processing",
    "text": "Data pre-processing\nPreparing the data for plotting. I did several things in here:\n      1. Leaving only reports from the US.\n      2. Leaving only reports for continental US.\n      3. Selecting the relevant variables.\n      4. Calculating decades.\n\nufo &lt;- ufo_sightings %&gt;%\n  filter(country == \"us\") %&gt;% # Leaving only sightings in US\n  filter(!(state %in% c(\"ak\", \"pr\", \"hi\"))) %&gt;% # Only mainland US\n  select(date, code = state, description, encounter_length, latitude, longitude) %&gt;%\n  left_join(state_codes, by = \"code\") %&gt;%\n  mutate(decade = as.factor(purrr::map_dbl(date, ~floor_decade(.)))) %&gt;% # Create decade variable\n  drop_na(decade)\n\nAfter some experimenting I’ve decided to make a heat map to visualize the number of UFO reports per state. But first, some more data processing. I first counted the number of cases per state (by_state), and then combined it with the USA map data frame (by_state2).\n\nby_state &lt;- ufo %&gt;%\n    group_by(state, decade, .drop = F) %&gt;%\n    summarise(cases = n(),\n              .groups = \"drop\")\n  \nby_state2 &lt;- left_join(usa, by_state, by = c(\"region\" = \"state\"), multiple = \"all\") %&gt;%\n  filter(decade %in% c(2000, 2010)) %&gt;%\n    left_join(uspop, by = \"region\")\n\nNow I need to summarize the number of cases per state.\n\ncases_per_state &lt;- by_state2 %&gt;%\n  group_by(region) %&gt;%\n  summarise(cases = sum(cases), .groups = \"drop\")\n\nAnd finally merge it back together with the geographic data.\n\nby_state2 &lt;- left_join(by_state2, select(cases_per_state, region, cases_total = cases), by = \"region\")"
  },
  {
    "objectID": "posts/ufo/index.html#heatmap-1",
    "href": "posts/ufo/index.html#heatmap-1",
    "title": "Who is the leading US state in UFO reports? probably not what you thought",
    "section": "Heatmap 1",
    "text": "Heatmap 1\nAnd now for the heat map… drum roll\n\nheatmap &lt;- ggplot(by_state2, aes(x = long, y = lat, fill = cases_total, group = group)) +\n  geom_polygon(color = \"black\", show.legend = T) +\n  scale_fill_gradient(low = \"#ffae00\", high = \"#d90000\", limits = c(0, 2864832), breaks = c(0, 2850000)) +\n  coord_fixed(1.3, clip = \"off\") +\n  theme_minimal()\n\nheatmap\n\n\n\n\nLooking at these results, I thought that of course California, Texas and Florida have the most reports, they also have the most people!\nA per capita measure will probably be more informative.\nCalculating reports per capita.\n\ncases_per_capita &lt;- by_state2 %&gt;%\n  group_by(region) %&gt;%\n  summarise(cases = sum(cases), .groups = \"drop\") %&gt;%\n  left_join(uspop, by = \"region\") %&gt;%\n  mutate(cases_per_capita = cases/mean_pop)\n\nMerging again with the geographical data.\n\nby_state2 &lt;- left_join(by_state2, select(cases_per_capita, region, cases_per_capita, cases_total = cases), by = \"region\")"
  },
  {
    "objectID": "posts/ufo/index.html#heatmap-2",
    "href": "posts/ufo/index.html#heatmap-2",
    "title": "Who is the leading US state in UFO reports? probably not what you thought",
    "section": "Heatmap 2",
    "text": "Heatmap 2\n\nheatmap2 &lt;- ggplot(by_state2, aes(x = long, y = lat, fill = cases_per_capita, group = group)) +\n  geom_polygon(color = \"black\", show.legend = T) +\n  scale_fill_gradient(low = \"#ffae00\", high = \"#d90000\", limits = c(0, 0.2), breaks = seq(0, 0.2, length.out = 6)) +\n  coord_fixed(1.3, clip = \"off\") +\n  theme_minimal()\n\nheatmap2\n\n\n\n\nNice!"
  },
  {
    "objectID": "posts/ufo/index.html#heatmap3",
    "href": "posts/ufo/index.html#heatmap3",
    "title": "Who is the leading US state in UFO reports? probably not what you thought",
    "section": "Heatmap3",
    "text": "Heatmap3\nLet’s add some aesthetics because why not (I’ve only spent 6 hours on Google researching color theory and ggplot2’s internal logic). Unfold the code chunk if you are interested in seeing the monstrosity.\n\n\nCode\nheatmap3 &lt;- ggplot(by_state2, aes(x = long, y = lat, fill = cases_per_capita, group = group)) +\n  geom_polygon(color = \"#00670c\", show.legend = T) +\n  scale_fill_gradient(low = \"black\", high = \"#5dff00\", limits = c(0, 0.2), breaks = seq(0, 0.2, length.out = 6), guide = guide_colorbar(\"Number of reported cases per capita\", \n                                                                               title.position = \"top\",\n                                                                               title.theme = element_text(color = \"#5dff00\", family = \"serif\"),\n                                                                               title.hjust = 0.5,\n                                                                               barwidth = 30,\n                                                                               ticks.colour = NA)) +\n  labs(title = \"15 years of UFO sightings in the US between 2000 and 2014\",\n       caption = \"Tomer Zipori | #TidyTuesday | Source: National UFO Reporting Center\") +\n  coord_fixed(1.3, clip = \"off\") +\n  theme_minimal() +\n  annotate(\"label\", x = -130, y = 45.4, label = \"Washington is spooky!\\n # of cases: 1,228,975\\n Cases per capita: 0.18\",\n           color = \"#5dff00\", fill = \"black\", family = \"serif\", fontface = \"bold\") +\n  geom_curve(aes(x = -127.5, y = 46.4, xend = -124.48, yend = 47.4), color = \"#5dff00\", linewidth = 1, curvature = -0.35,\n             arrow = arrow(type = \"closed\", length = unit(0.02, \"npc\"))) +\n  annotate(\"label\", x = -124, y = 30.4, label = \"Utah has the lowest rate in the US\\n # of cases: 22,715\\n Cases per capita: 0.0079\",\n           color = \"#5dff00\", fill = \"black\", family = \"serif\", fontface = \"bold\") +\n  geom_curve(aes(x = -119.4, y = 31.4, xend = -111.5, yend = 39), color = \"#5dff00\", linewidth = 1, curvature = 0.3,\n             arrow = arrow(type = \"closed\", length = unit(0.02, \"npc\"))) +\n  theme(plot.title = element_text(size = 24, vjust = -4, hjust = 0.5, color = \"#5dff00\", family = \"Showcard Gothic\"),\n        plot.caption = element_text(color = \"#5dff00\", hjust = 1.05, family = \"serif\", size = 9),\n        axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        panel.grid = element_blank(),\n        legend.position = \"bottom\", legend.box = \"horizontal\", legend.text = element_text(color = \"#5dff00\", family = \"mono\", size = 14))\n\nheatmap3 &lt;- ggbackground(heatmap3, nightsky_img)"
  }
]