[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog posts",
    "section": "",
    "text": "Real Vs. Fake news in football\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nMachine Learning\n\n\nNLP\n\n\n\n\nExperiment in NLP and some text classification\n\n\n\n\n\n\nMay 12, 2023\n\n\nTomer Zipori\n\n\n\n\n\n\n  \n\n\n\n\nWhat makes FIFA 23 players good?\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nMachine Learning\n\n\n\n\nExperimentation with Elastic -net regression and decision tree boosting.\n\n\n\n\n\n\nMay 6, 2023\n\n\nTomer Zipori\n\n\n\n\n\n\n  \n\n\n\n\nWho is the leading US state in UFO reports? probably not what you thought\n\n\n\n\n\n\n\ncode\n\n\nvisualization\n\n\nggplot2\n\n\ntidytuesday\n\n\n\n\nVisualizing UFO reports per state capita\n\n\n\n\n\n\nApr 16, 2023\n\n\nTomer Zipori\n\n\n\n\n\n\n  \n\n\n\n\nCage-free vs. caged hens in the US\n\n\n\n\n\n\n\ncode\n\n\nvisualization\n\n\nggplot2\n\n\ntidytuesday\n\n\n\n\n\n\n\n\n\n\n\nApr 12, 2023\n\n\nTomer Zipori\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tomer Zipori",
    "section": "",
    "text": "Data science & data visualization nerd. I’m studying for my MA in Cognitive Psychology at Ben-Gurion University of the Negev. I hope to record and share my journey in studying and applying cool data-science stuff!"
  },
  {
    "objectID": "posts/fifa23/index.html",
    "href": "posts/fifa23/index.html",
    "title": "What makes FIFA 23 players good?",
    "section": "",
    "text": "The current Data is an upload to Kaggle by Babatunde Zenith, and it includes information about players in the popular FIFA 23 video game. Information includes: name, age, nationality, position, various football ratings and contract deals.\nThe current notebook is an attempt at:\n      1. Accurately and efficiently predicting player’s overall rating.\n      2. Identifying important variables (features) for this prediction.\nBoth goals will be achieved using two methods: Elastic-net regression and XGBoost. Data pre-processing will be done with tidyverse, Model fitting and evaluation will be done with the caret and gbm packages."
  },
  {
    "objectID": "posts/fifa23/index.html#re-naming-columns",
    "href": "posts/fifa23/index.html#re-naming-columns",
    "title": "What makes FIFA 23 players good?",
    "section": "Re-naming columns",
    "text": "Re-naming columns\nReplacing spaces with underscores for ease.\n\nnames(players) <- str_replace_all(names(players), pattern = \" \", replacement = \"_\")"
  },
  {
    "objectID": "posts/fifa23/index.html#non-numeric-variables",
    "href": "posts/fifa23/index.html#non-numeric-variables",
    "title": "What makes FIFA 23 players good?",
    "section": "non-numeric variables",
    "text": "non-numeric variables\nFirst we’ll look at potential garbage variables.\n\nnames(select(players, where(is.character)))\n\n [1] \"Known_As\"                    \"Full_Name\"                  \n [3] \"Positions_Played\"            \"Best_Position\"              \n [5] \"Nationality\"                 \"Image_Link\"                 \n [7] \"Club_Name\"                   \"Club_Position\"              \n [9] \"Contract_Until\"              \"Club_Jersey_Number\"         \n[11] \"On_Loan\"                     \"Preferred_Foot\"             \n[13] \"National_Team_Name\"          \"National_Team_Image_Link\"   \n[15] \"National_Team_Position\"      \"National_Team_Jersey_Number\"\n[17] \"Attacking_Work_Rate\"         \"Defensive_Work_Rate\"        \n\n\nAlmost all garbage data. Since I’ve noted that Work Rate variables are ordered (low-medium-high) We’ll re-code them:\n\nplayers <- players %>%\n  mutate(Attacking_Work_Rate = case_when(Attacking_Work_Rate == \"Low\" ~ 1,\n                                         Attacking_Work_Rate == \"Medium\" ~ 2,\n                                         Attacking_Work_Rate == \"High\" ~ 3),\n         Defensive_Work_Rate = case_when(Defensive_Work_Rate == \"Low\" ~ 1,\n                                         Defensive_Work_Rate == \"Medium\" ~ 2,\n                                         Defensive_Work_Rate == \"High\" ~ 3)) %>%\n  select(-Known_As, -Full_Name, -Positions_Played, -Nationality, -Image_Link, -Club_Name, -Contract_Until, -Club_Jersey_Number, -National_Team_Name, -National_Team_Image_Link, -National_Team_Jersey_Number, -On_Loan) %>% # getting rid of garbage variables\n  mutate(across(where(is.character), ~na_if(., \"-\"))) # replacing all \"-\" with NA"
  },
  {
    "objectID": "posts/fifa23/index.html#searching-for-variables-with-large-number-of-nas",
    "href": "posts/fifa23/index.html#searching-for-variables-with-large-number-of-nas",
    "title": "What makes FIFA 23 players good?",
    "section": "Searching for variables with large number of NA’s",
    "text": "Searching for variables with large number of NA’s\n\ncolSums(is.na(players))\n\n                 Overall                Potential           Value(in_Euro) \n                       0                        0                        0 \n           Best_Position                      Age            Height(in_cm) \n                       0                        0                        0 \n           Weight(in_kg)               TotalStats                BaseStats \n                       0                        0                        0 \n           Wage(in_Euro)           Release_Clause            Club_Position \n                       0                        0                       86 \n               Joined_On           Preferred_Foot         Weak_Foot_Rating \n                       0                        0                        0 \n             Skill_Moves International_Reputation   National_Team_Position \n                       0                        0                    16746 \n     Attacking_Work_Rate      Defensive_Work_Rate               Pace_Total \n                       0                        0                        0 \n          Shooting_Total            Passing_Total          Dribbling_Total \n                       0                        0                        0 \n         Defending_Total        Physicality_Total                 Crossing \n                       0                        0                        0 \n               Finishing         Heading_Accuracy            Short_Passing \n                       0                        0                        0 \n                 Volleys                Dribbling                    Curve \n                       0                        0                        0 \n       Freekick_Accuracy              LongPassing              BallControl \n                       0                        0                        0 \n            Acceleration             Sprint_Speed                  Agility \n                       0                        0                        0 \n               Reactions                  Balance               Shot_Power \n                       0                        0                        0 \n                 Jumping                  Stamina                 Strength \n                       0                        0                        0 \n              Long_Shots               Aggression            Interceptions \n                       0                        0                        0 \n             Positioning                   Vision                Penalties \n                       0                        0                        0 \n               Composure                  Marking          Standing_Tackle \n                       0                        0                        0 \n          Sliding_Tackle        Goalkeeper_Diving      Goalkeeper_Handling \n                       0                        0                        0 \n       GoalkeeperKicking   Goalkeeper_Positioning      Goalkeeper_Reflexes \n                       0                        0                        0 \n               ST_Rating                LW_Rating                LF_Rating \n                       0                        0                        0 \n               CF_Rating                RF_Rating                RW_Rating \n                       0                        0                        0 \n              CAM_Rating                LM_Rating                CM_Rating \n                       0                        0                        0 \n               RM_Rating               LWB_Rating               CDM_Rating \n                       0                        0                        0 \n              RWB_Rating                LB_Rating                CB_Rating \n                       0                        0                        0 \n               RB_Rating                GK_Rating \n                       0                        0 \n\n\nNational team position seems sparse, we’ll have to get rid of club_position as well for the model fitting. We’ll also get rid of best_position because it creates so much dummy vars. I’ll analyzed it in another day…\n\nplayers <- select(players, -National_Team_Position, -Club_Position, -Best_Position)"
  },
  {
    "objectID": "posts/fifa23/index.html#data-splitting",
    "href": "posts/fifa23/index.html#data-splitting",
    "title": "What makes FIFA 23 players good?",
    "section": "Data splitting",
    "text": "Data splitting\n\nset.seed(14)\ntrain_id <- createDataPartition(y = players$Overall, p = 0.7, list = F)\n\nplayers_train <- players[train_id,]\nplayers_test <- players[-train_id,]"
  },
  {
    "objectID": "posts/fifa23/index.html#tuning-grid-for-hyper-parameters",
    "href": "posts/fifa23/index.html#tuning-grid-for-hyper-parameters",
    "title": "What makes FIFA 23 players good?",
    "section": "Tuning grid for hyper-parameters",
    "text": "Tuning grid for hyper-parameters\n\ntg <- expand.grid(alpha = c(seq(0, 1, length.out = 25)),\n                  lambda = c(2 ^ seq(10, -10, length = 100)))\n\nSetting a relatively large range of hyper-parameters because elastic-net regression is not super expansive computationally."
  },
  {
    "objectID": "posts/fifa23/index.html#training",
    "href": "posts/fifa23/index.html#training",
    "title": "What makes FIFA 23 players good?",
    "section": "Training",
    "text": "Training\n\nelastic_reg <- train(Overall ~ ., \n                    data = players_train,\n                    method = \"glmnet\",\n                    preProcess = c(\"center\", \"scale\"), # for better interpatation of coefficients\n                    tuneGrid = tg,\n                    trControl =  trainControl(method = \"cv\", number = 10)) # 10-fold Cross-Validation"
  },
  {
    "objectID": "posts/fifa23/index.html#best-hyper-parameters",
    "href": "posts/fifa23/index.html#best-hyper-parameters",
    "title": "What makes FIFA 23 players good?",
    "section": "Best hyper-parameters",
    "text": "Best hyper-parameters\n\nelastic_reg$bestTune\n\n     alpha       lambda\n1501 0.625 0.0009765625"
  },
  {
    "objectID": "posts/fifa23/index.html#traincv-error",
    "href": "posts/fifa23/index.html#traincv-error",
    "title": "What makes FIFA 23 players good?",
    "section": "Train/CV error",
    "text": "Train/CV error\n\nplot(elastic_reg, xTrans = log, digits = 3)\n\n\n\nelastic_reg$results[elastic_reg$results$RMSE == min(elastic_reg$results$RMSE, na.rm = T),]\n\n     alpha       lambda     RMSE  Rsquared     MAE     RMSESD  RsquaredSD\n1501 0.625 0.0009765625 1.601089 0.9440492 1.24766 0.04469899 0.003446449\n          MAESD\n1501 0.02690937\n\n\nAll mixes of \\(\\alpha\\) and \\(\\lambda\\) hyper-parameters converge in the end."
  },
  {
    "objectID": "posts/fifa23/index.html#model-coefficients",
    "href": "posts/fifa23/index.html#model-coefficients",
    "title": "What makes FIFA 23 players good?",
    "section": "Model coefficients",
    "text": "Model coefficients\n\n\n\n\nelasnet_coeffs <- coef(elastic_reg$finalModel, s = elastic_reg$bestTune$lambda)\nplot(elasnet_coeffs, ylab = \"Coefficient\")\n\n\n\nround(elasnet_coeffs, 4)\n\n73 x 1 sparse Matrix of class \"dgCMatrix\"\n                              s1\n(Intercept)              65.9420\nPotential                 2.3510\n`Value(in_Euro)`          0.6037\nAge                       2.0174\n`Height(in_cm)`          -0.1111\n`Weight(in_kg)`           0.0979\nTotalStats               -2.7794\nBaseStats                 0.0004\n`Wage(in_Euro)`           0.2468\nRelease_Clause           -0.2689\nJoined_On                 0.0710\nPreferred_FootRight      -0.0694\nWeak_Foot_Rating         -0.0397\nSkill_Moves               0.3644\nInternational_Reputation -0.2143\nAttacking_Work_Rate      -0.0729\nDefensive_Work_Rate      -0.1001\nPace_Total                0.6887\nShooting_Total            0.4371\nPassing_Total             0.6242\nDribbling_Total           1.4907\nDefending_Total          -0.0937\nPhysicality_Total         0.9278\nCrossing                  0.3602\nFinishing                -0.4256\nHeading_Accuracy          0.8768\nShort_Passing             0.2001\nVolleys                   0.0255\nDribbling                -1.3137\nCurve                     0.0000\nFreekick_Accuracy         0.1731\nLongPassing              -0.6509\nBallControl               0.1516\nAcceleration              0.0289\nSprint_Speed             -0.1612\nAgility                  -0.1264\nReactions                 1.1084\nBalance                  -0.0032\nShot_Power               -0.0565\nJumping                   0.0664\nStamina                   0.0582\nStrength                 -0.1455\nLong_Shots               -0.3293\nAggression               -0.1703\nPositioning              -1.1545\nVision                   -0.7001\nPenalties                 0.1108\nComposure                 0.4431\nMarking                   0.6215\nStanding_Tackle           0.2082\nSliding_Tackle            0.2331\nGoalkeeper_Diving         0.1745\nGoalkeeper_Handling      -0.0091\nGoalkeeperKicking         0.0767\nGoalkeeper_Positioning   -0.0696\nGoalkeeper_Reflexes      -0.0828\nST_Rating                 2.5495\nLW_Rating                -0.0648\nLF_Rating                 0.0000\nCF_Rating                 0.0000\nRF_Rating                 0.0000\nRW_Rating                 .     \nCAM_Rating               -0.1751\nLM_Rating                 0.5492\nCM_Rating                 1.9575\nRM_Rating                 0.0129\nLWB_Rating                .     \nCDM_Rating                1.3227\nRWB_Rating                .     \nLB_Rating                -0.0021\nCB_Rating                -0.9501\nRB_Rating                 .     \nGK_Rating                 0.6368\n\n\nThe intercept is quite large. Let’s look at the variables in a more informative scale.\n\nplot(elasnet_coeffs[-1,], ylab = \"Coefficient\")"
  },
  {
    "objectID": "posts/fifa23/index.html#test-error",
    "href": "posts/fifa23/index.html#test-error",
    "title": "What makes FIFA 23 players good?",
    "section": "Test error",
    "text": "Test error\n\nelasticreg_pred <- predict(elastic_reg, newdata = players_test) # calculating model's prediction for test set\n\n\n\n\nTest error and effect size\n\\(RMSE=1.60955711701293\\)\n\\(R^2=0.944839845538989\\)\nVery nice!"
  },
  {
    "objectID": "posts/fifa23/index.html#training-control",
    "href": "posts/fifa23/index.html#training-control",
    "title": "What makes FIFA 23 players good?",
    "section": "Training control",
    "text": "Training control\nWe’ll use adaptive cross-validation in order to make the hyper-parameter search more efficient.\nFor further explanation on implementation in R see. For further reading on theory see.\n\ntr <- trainControl(method = \"adaptive_cv\",\n                   number = 10, repeats = 10,\n                   adaptive = list(min = 5, alpha = 0.05, \n                                   method = \"BT\", complete = TRUE),\n                   search = \"random\")"
  },
  {
    "objectID": "posts/fifa23/index.html#training-1",
    "href": "posts/fifa23/index.html#training-1",
    "title": "What makes FIFA 23 players good?",
    "section": "Training",
    "text": "Training\n\nset.seed(14)\nxgboost <- train(Overall ~ ., \n                   data = players_train,\n                   method = \"gbm\",\n                   trControl = tr, # No explicit tuning grid is needed\n                   verbose = T)"
  },
  {
    "objectID": "posts/fifa23/index.html#traincv-error-1",
    "href": "posts/fifa23/index.html#traincv-error-1",
    "title": "What makes FIFA 23 players good?",
    "section": "Train/CV error",
    "text": "Train/CV error\nGetting the results of the best tuning parameters found.\n\nxgboost$results[xgboost$results$RMSE == min(xgboost$results$RMSE, na.rm = T),5:10]\n\n       RMSE  Rsquared       MAE      RMSESD   RsquaredSD       MAESD\n2 0.7146858 0.9893686 0.5457707 0.005980442 0.0002966158 0.002603466\n\n\nSeems quite optimized, but is it overfitted?"
  },
  {
    "objectID": "posts/fifa23/index.html#test-error-1",
    "href": "posts/fifa23/index.html#test-error-1",
    "title": "What makes FIFA 23 players good?",
    "section": "Test error",
    "text": "Test error\n\nboost_pred <- predict(xgboost, players_test)\n\n\n\n\nTest error and effect size\n\\(RMSE=0.700272523649518\\)\n\\(R^2=0.989645490170188\\)\nVery Very nice!"
  },
  {
    "objectID": "posts/fifa23/index.html#variable-importance",
    "href": "posts/fifa23/index.html#variable-importance",
    "title": "What makes FIFA 23 players good?",
    "section": "Variable importance",
    "text": "Variable importance\n\nvarimp <- caret::varImp(xgboost, scale = T)\n\nvarimp\n\ngbm variable importance\n\n  only 20 most important variables shown (out of 72)\n\n                        Overall\n`Value(in_Euro)`       100.0000\nReactions               50.2939\nBaseStats               16.4047\nAge                      7.5742\n`Wage(in_Euro)`          3.8700\nPotential                3.6002\nCB_Rating                2.3274\nDefending_Total          1.0533\nGoalkeeper_Positioning   0.6619\nCrossing                 0.3162\nTotalStats               0.2916\nShooting_Total           0.2559\nStrength                 0.2359\nPositioning              0.2113\nStanding_Tackle          0.1991\nLF_Rating                0.1927\nRelease_Clause           0.1814\nDribbling_Total          0.1650\nLB_Rating                0.1578\nHeading_Accuracy         0.1497\n\n\n\nPlotting variable importance\n\n\nShow the plot’s code\n# data preparation\nvarimp$importance %>%\n  rownames_to_column(var = \"Feature\") %>%\n  dplyr::rename(Importance = Overall) %>%\n  filter(Importance != 0) %>% # Only features that have an above 0 importance\n  \n  # Plotting\n  ggplot(aes(x = reorder(Feature, -Importance), y = Importance)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip(ylim = c(0, 100)) +\n  scale_y_continuous(limits = c(0,100), expand = c(0, 0)) +\n  labs(x = \"Feature\", y = \"Importance\", title = \"Variable importance in boosted model\", caption = \"Tomer Zipori | FIFA 23 Player Research by Babatunde Zenith | Kaggle\") +\n  theme_classic() +\n  theme(axis.text.y = element_text(size = 7),\n        plot.title = element_text(size = 16, hjust = 0.5),\n        plot.margin = unit(c(1,1,1,1), \"cm\"),\n        plot.caption = element_text(size = 6, hjust = 0.5, vjust = -5))\n\n\n\n\n\nPlayer value is the strongest predictor by far, with a few interesting ones right behind it (CB_rating?)."
  },
  {
    "objectID": "posts/ufo/index.html",
    "href": "posts/ufo/index.html",
    "title": "Who is the leading US state in UFO reports? probably not what you thought",
    "section": "",
    "text": "As I told in another post, I’ve recently enrolled to a course about Data-Viz (and maybe some NLP later-on, stay tuned). One of the assignments in the course was to make some TidyTuesday contribution and present it in class. Although this is not what I’ve presented in class, this was made shortly after and I think it came out quite nice :)"
  },
  {
    "objectID": "posts/ufo/index.html#initial-pre-processing",
    "href": "posts/ufo/index.html#initial-pre-processing",
    "title": "Who is the leading US state in UFO reports? probably not what you thought",
    "section": "Initial pre-processing",
    "text": "Initial pre-processing\n\nufo_sightings <- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-06-25/ufo_sightings.csv\")\n\nusa <- map_data(\"state\") # US map\n\nstate_codes <- read_csv(\"state_code.csv\") %>% # converting from state name to 2-letter code and back\n  select(state, code) %>%\n  mutate(state = tolower(state), code = tolower(code))\n\nuspop <- read_excel(\"uspop.xlsx\", col_names = c(\"region\", \"pop_2010\", \"pop_2011\", \"pop_2012\", \"pop_2013\", \"pop_2014\")) %>% # info about state population\n  mutate(region = tolower(str_remove(region, \".\"))) %>%\n  rowwise() %>%\n  mutate(mean_pop = mean(c(pop_2010, pop_2011, pop_2012, pop_2013, pop_2014))) %>%\n  ungroup() %>%\n  select(region, mean_pop)\n\nUS population info taken from the United States Census Bureau."
  },
  {
    "objectID": "posts/ufo/index.html#glimpsing-at-the-data",
    "href": "posts/ufo/index.html#glimpsing-at-the-data",
    "title": "Who is the leading US state in UFO reports? probably not what you thought",
    "section": "Glimpsing at the data",
    "text": "Glimpsing at the data\n\nglimpse(ufo_sightings)\n\nRows: 80,332\nColumns: 11\n$ date_time                  <chr> \"10/10/1949 20:30\", \"10/10/1949 21:00\", \"10…\n$ city_area                  <chr> \"san marcos\", \"lackland afb\", \"chester (uk/…\n$ state                      <chr> \"tx\", \"tx\", NA, \"tx\", \"hi\", \"tn\", NA, \"ct\",…\n$ country                    <chr> \"us\", NA, \"gb\", \"us\", \"us\", \"us\", \"gb\", \"us…\n$ ufo_shape                  <chr> \"cylinder\", \"light\", \"circle\", \"circle\", \"l…\n$ encounter_length           <dbl> 2700, 7200, 20, 20, 900, 300, 180, 1200, 18…\n$ described_encounter_length <chr> \"45 minutes\", \"1-2 hrs\", \"20 seconds\", \"1/2…\n$ description                <chr> \"This event took place in early fall around…\n$ date_documented            <chr> \"4/27/2004\", \"12/16/2005\", \"1/21/2008\", \"1/…\n$ latitude                   <dbl> 29.88306, 29.38421, 53.20000, 28.97833, 21.…\n$ longitude                  <dbl> -97.941111, -98.581082, -2.916667, -96.6458…"
  },
  {
    "objectID": "posts/ufo/index.html#defining-functions",
    "href": "posts/ufo/index.html#defining-functions",
    "title": "Who is the leading US state in UFO reports? probably not what you thought",
    "section": "Defining functions",
    "text": "Defining functions\nSome helper function will help us later, mainly to work with dates.\nconvert_to_date takes a vector of character formatted dates and converts it to lubridate’s date format. floor_decade takes a vector of dates and converts it to a vector of decades.\n\nconvert_to_date <- function(x) { \n  sub_string <- str_sub(x, 1, 10)\n  d <- mdy(sub_string)\n  return(as.numeric(d))\n}\nfloor_decade <- function(x){\n  return(lubridate::year(x) - lubridate::year(x) %% 10)\n  }\n\n\nConverting the dates\n\nufo_sightings <- ufo_sightings %>%\n  mutate(date = as_date(purrr::map_dbl(date_time, ~convert_to_date(.)))) # Convert to 'Date' format. Run only once, its slow af"
  },
  {
    "objectID": "posts/ufo/index.html#globals",
    "href": "posts/ufo/index.html#globals",
    "title": "Who is the leading US state in UFO reports? probably not what you thought",
    "section": "Globals",
    "text": "Globals\nHere I’m loading some images and fonts that will be of use later to beautify the plot.\n\nnightsky_img <- \"nightsky2.jpg\"\n\n#font_files() %>% tibble() %>% filter(str_detect(family, \"Showcard Gothic\"))\nfont_add(family = \"Showcard Gothic\", regular = \"SHOWG.TTF\")\nshowtext_auto()"
  },
  {
    "objectID": "posts/ufo/index.html#data-pre-processing",
    "href": "posts/ufo/index.html#data-pre-processing",
    "title": "Who is the leading US state in UFO reports? probably not what you thought",
    "section": "Data pre-processing",
    "text": "Data pre-processing\nPreparing the data for plotting. I did several things in here:\n      1. Leaving only reports from the US.\n      2. Leaving only reports for continental US.\n      3. Selecting the relevant variables.\n      4. Calculating decades.\n\nufo <- ufo_sightings %>%\n  filter(country == \"us\") %>% # Leaving only sightings in US\n  filter(!(state %in% c(\"ak\", \"pr\", \"hi\"))) %>% # Only mainland US\n  select(date, code = state, description, encounter_length, latitude, longitude) %>%\n  left_join(state_codes, by = \"code\") %>%\n  mutate(decade = as.factor(purrr::map_dbl(date, ~floor_decade(.)))) %>% # Create decade variable\n  drop_na(decade)\n\nAfter some experimenting I’ve decided to make a heat map to visualize the number of UFO reports per state. But first, some more data processing. I first counted the number of cases per state (by_state), and then combined it with the USA map data frame (by_state2).\n\nby_state <- ufo %>%\n    group_by(state, decade, .drop = F) %>%\n    summarise(cases = n(),\n              .groups = \"drop\")\n  \nby_state2 <- left_join(usa, by_state, by = c(\"region\" = \"state\"), multiple = \"all\") %>%\n  filter(decade %in% c(2000, 2010)) %>%\n    left_join(uspop, by = \"region\")\n\nNow I need to summarize the number of cases per state.\n\ncases_per_state <- by_state2 %>%\n  group_by(region) %>%\n  summarise(cases = sum(cases), .groups = \"drop\")\n\nAnd finally merge it back together with the geographic data.\n\nby_state2 <- left_join(by_state2, select(cases_per_state, region, cases_total = cases), by = \"region\")"
  },
  {
    "objectID": "posts/ufo/index.html#heatmap-1",
    "href": "posts/ufo/index.html#heatmap-1",
    "title": "Who is the leading US state in UFO reports? probably not what you thought",
    "section": "Heatmap 1",
    "text": "Heatmap 1\nAnd now for the heat map… drum roll\n\nheatmap <- ggplot(by_state2, aes(x = long, y = lat, fill = cases_total, group = group)) +\n  geom_polygon(color = \"black\", show.legend = T) +\n  scale_fill_gradient(low = \"#ffae00\", high = \"#d90000\", limits = c(0, 2864832), breaks = c(0, 2850000)) +\n  coord_fixed(1.3, clip = \"off\") +\n  theme_minimal()\n\nheatmap\n\n\n\n\nLooking at these results, I thought that of course California, Texas and Florida have the most reports, they also have the most people!\nA per capita measure will probably be more informative.\nCalculating reports per capita.\n\ncases_per_capita <- by_state2 %>%\n  group_by(region) %>%\n  summarise(cases = sum(cases), .groups = \"drop\") %>%\n  left_join(uspop, by = \"region\") %>%\n  mutate(cases_per_capita = cases/mean_pop)\n\nMerging again with the geographical data.\n\nby_state2 <- left_join(by_state2, select(cases_per_capita, region, cases_per_capita, cases_total = cases), by = \"region\")"
  },
  {
    "objectID": "posts/ufo/index.html#heatmap-2",
    "href": "posts/ufo/index.html#heatmap-2",
    "title": "Who is the leading US state in UFO reports? probably not what you thought",
    "section": "Heatmap 2",
    "text": "Heatmap 2\n\nheatmap2 <- ggplot(by_state2, aes(x = long, y = lat, fill = cases_per_capita, group = group)) +\n  geom_polygon(color = \"black\", show.legend = T) +\n  scale_fill_gradient(low = \"#ffae00\", high = \"#d90000\", limits = c(0, 0.2), breaks = seq(0, 0.2, length.out = 6)) +\n  coord_fixed(1.3, clip = \"off\") +\n  theme_minimal()\n\nheatmap2\n\n\n\n\nNice!"
  },
  {
    "objectID": "posts/ufo/index.html#heatmap3",
    "href": "posts/ufo/index.html#heatmap3",
    "title": "Who is the leading US state in UFO reports? probably not what you thought",
    "section": "Heatmap3",
    "text": "Heatmap3\nLet’s add some aesthetics because why not (I’ve only spent 6 hours on Google researching color theory and ggplot2’s internal logic). Unfold the code chunk if you are interested in seeing the monstrosity.\n\n\nCode\nheatmap3 <- ggplot(by_state2, aes(x = long, y = lat, fill = cases_per_capita, group = group)) +\n  geom_polygon(color = \"#00670c\", show.legend = T) +\n  scale_fill_gradient(low = \"black\", high = \"#5dff00\", limits = c(0, 0.2), breaks = seq(0, 0.2, length.out = 6), guide = guide_colorbar(\"Number of reported cases per capita\", \n                                                                               title.position = \"top\",\n                                                                               title.theme = element_text(color = \"#5dff00\", family = \"serif\"),\n                                                                               title.hjust = 0.5,\n                                                                               barwidth = 30,\n                                                                               ticks.colour = NA)) +\n  labs(title = \"15 years of UFO sightings in the US between 2000 and 2014\",\n       caption = \"Tomer Zipori | #TidyTuesday | Source: National UFO Reporting Center\") +\n  coord_fixed(1.3, clip = \"off\") +\n  theme_minimal() +\n  annotate(\"label\", x = -130, y = 45.4, label = \"Washington is spooky!\\n # of cases: 1,228,975\\n Cases per capita: 0.18\",\n           color = \"#5dff00\", fill = \"black\", family = \"serif\", fontface = \"bold\") +\n  geom_curve(aes(x = -127.5, y = 46.4, xend = -124.48, yend = 47.4), color = \"#5dff00\", linewidth = 1, curvature = -0.35,\n             arrow = arrow(type = \"closed\", length = unit(0.02, \"npc\"))) +\n  annotate(\"label\", x = -124, y = 30.4, label = \"Utah has the lowest rate in the US\\n # of cases: 22,715\\n Cases per capita: 0.0079\",\n           color = \"#5dff00\", fill = \"black\", family = \"serif\", fontface = \"bold\") +\n  geom_curve(aes(x = -119.4, y = 31.4, xend = -111.5, yend = 39), color = \"#5dff00\", linewidth = 1, curvature = 0.3,\n             arrow = arrow(type = \"closed\", length = unit(0.02, \"npc\"))) +\n  theme(plot.title = element_text(size = 24, vjust = -4, hjust = 0.5, color = \"#5dff00\", family = \"Showcard Gothic\"),\n        plot.caption = element_text(color = \"#5dff00\", hjust = 1.05, family = \"serif\", size = 9),\n        axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        panel.grid = element_blank(),\n        legend.position = \"bottom\", legend.box = \"horizontal\", legend.text = element_text(color = \"#5dff00\", family = \"mono\", size = 14))\n\nheatmap3 <- ggbackground(heatmap3, nightsky_img)"
  },
  {
    "objectID": "posts/eggs/index.html",
    "href": "posts/eggs/index.html",
    "title": "Cage-free vs. caged hens in the US",
    "section": "",
    "text": "Some time ago, when the fall semester began I’ve enrolled on a course called Data science lab as part of the ‘Data Science for the Social Sciences’ program. One of the assignments was to make some TidyTuesday contribution and present it in class. So this is my submission and what I consider as my first respectable attempts at visualizing data."
  },
  {
    "objectID": "posts/eggs/index.html#libraries",
    "href": "posts/eggs/index.html#libraries",
    "title": "Cage-free vs. caged hens in the US",
    "section": "Libraries",
    "text": "Libraries\n\nlibrary(tidytuesdayR) # for easy data loading\nlibrary(tidyverse)    # for data pre-processing and wrangling\nlibrary(lubridate)    # makes dealing with date format much easier\nlibrary(showtext)     # fonts"
  },
  {
    "objectID": "posts/eggs/index.html#loading-fonts",
    "href": "posts/eggs/index.html#loading-fonts",
    "title": "Cage-free vs. caged hens in the US",
    "section": "Loading fonts",
    "text": "Loading fonts\nshowtext is an awesome package that allows to load installed fonts into R and use it in ggplot2 plots (for example). For a really helpful video that I used see this video from the Riffomonas Project.\n\nfont_add(family = \"Stencil\", regular = \"STENCIL.TTF\")\nshowtext_auto()"
  },
  {
    "objectID": "posts/eggs/index.html#taking-a-peak",
    "href": "posts/eggs/index.html#taking-a-peak",
    "title": "Cage-free vs. caged hens in the US",
    "section": "Taking a peak",
    "text": "Taking a peak\n\nhead(eggproduction)\n\n# A tibble: 6 × 6\n  observed_month prod_type     prod_process   n_hens     n_eggs source          \n  <date>         <chr>         <chr>           <dbl>      <dbl> <chr>           \n1 2016-07-31     hatching eggs all          57975000 1147000000 ChicEggs-09-23-…\n2 2016-08-31     hatching eggs all          57595000 1142700000 ChicEggs-10-21-…\n3 2016-09-30     hatching eggs all          57161000 1093300000 ChicEggs-11-22-…\n4 2016-10-31     hatching eggs all          56857000 1126700000 ChicEggs-12-23-…\n5 2016-11-30     hatching eggs all          57116000 1096600000 ChicEggs-01-24-…\n6 2016-12-31     hatching eggs all          57750000 1132900000 ChicEggs-02-28-…\n\n\neggproduction holds monthly data about number of hens and produced eggs in the US. prod_type specifies the type of egg produced, it has 2 levels:\n\nunique(eggproduction$prod_type)\n\n[1] \"hatching eggs\" \"table eggs\"   \n\n\nFor the current mini-project, I’ll stay with “table eggs” only.\nThe variable prod_process specifies the type of housing of the egg-producing hens. It has 3 levels:\n\nunique(eggproduction$prod_process)\n\n[1] \"all\"                     \"cage-free (non-organic)\"\n[3] \"cage-free (organic)\"    \n\n\nI’ll leave data of all hens for now.\n\nPre-processing 1\nFiltering out irrelevant data and renaming some variables.\n\negg_clean <- eggproduction %>%\n  filter(prod_type != \"hatching eggs\" & prod_process == \"all\") %>% # Leave only eggs meant for eating and general data\n  select(-source, -prod_type, -prod_process, n_hens_all = n_hens, n_eggs_all = n_eggs) # Irrelevant columns\n\n\n\nTaking a peak 2\n\nhead(cagefreepercentages)\n\n# A tibble: 6 × 4\n  observed_month percent_hens percent_eggs source                             \n  <date>                <dbl>        <dbl> <chr>                              \n1 2007-12-31              3.2           NA Egg-Markets-Overview-2019-10-19.pdf\n2 2008-12-31              3.5           NA Egg-Markets-Overview-2019-10-19.pdf\n3 2009-12-31              3.6           NA Egg-Markets-Overview-2019-10-19.pdf\n4 2010-12-31              4.4           NA Egg-Markets-Overview-2019-10-19.pdf\n5 2011-12-31              5.4           NA Egg-Markets-Overview-2019-10-19.pdf\n6 2012-12-31              6             NA Egg-Markets-Overview-2019-10-19.pdf\n\n\nThis data-frame also holds monthly data. The variable percent_hens specifies observed or computed percentage of cage-free hens relative to all table-egg-laying hens (from the Github repo). We’ll select these variables and use them to merge with the first data-frame.\n\negg_clean2 <- cagefreepercentages %>%\n  drop_na(percent_eggs) %>%                                                # droping rows with missing percent_eggs data\n  select(-source, -percent_eggs, cagefree_percent_hens = percent_hens) %>% # Irrelevant\\to many NA's columns + renaming\n  inner_join(egg_clean, by = \"observed_month\", multiple = \"all\") %>%       # joining with the first data-frame\n  mutate(Cagefree = (cagefree_percent_hens * n_hens_all) / 100) %>%        # calculating number of cage-free hens\n  mutate(Traditional = n_hens_all - Cagefree) %>%                          # calculating number of traditional housing hens\n  select(observed_month, Cagefree, Traditional) %>%\n  pivot_longer(cols = c(\"Cagefree\", \"Traditional\"), names_to = \"housing\", values_to = \"n_hens\")"
  },
  {
    "objectID": "posts/eggs/index.html#gameplan",
    "href": "posts/eggs/index.html#gameplan",
    "title": "Cage-free vs. caged hens in the US",
    "section": "Gameplan",
    "text": "Gameplan\nFew hours deep, I’ve decided it would be interesting to see the change in number of cage-free hens compared to caged hens during the time period we have data about. Because I wanted to plot only certain points of data along the time axis, I needed to create a subset of the big data-frame that holds the data for the points I wanted to plot.\n\nIdentitfying the dates of interest\nFirst thing, I found 6 dates that are equally spaced between the start and end points. The repetitive code below is quite ugly, and lubridate probably has a nice and elegant solution, I didn’t want to spend to much time on it.\n\ndates_for_plot <- seq.Date(egg_clean2$observed_month[1], egg_clean2$observed_month[nrow(egg_clean2)], length.out = 6)\ndates_for_plot[2] <- as.Date(\"2017-07-31\")\ndates_for_plot[3] <- as.Date(\"2018-05-31\")\ndates_for_plot[4] <- as.Date(\"2019-04-30\")\ndates_for_plot[5] <- as.Date(\"2020-03-31\")\n\n\n\nCreating the subset\nI will use this subset in order to plot the 6 points on the general data. I will only need to specify data = subset_for_points in the relevant geom object.\n\nsubset_for_points <- egg_clean2 %>%\n  filter((housing == \"Cagefree\" & observed_month %in% dates_for_plot) |\n           (housing == \"Traditional\" & (observed_month == dates_for_plot[1] | observed_month == dates_for_plot[6]))) %>%\n  inner_join(select(egg_clean, observed_month, n_hens = n_hens_all), by = \"observed_month\") %>%\n  mutate(n_hens = case_when(housing == \"Cagefree\" ~ n_hens.x,\n                            housing == \"Traditional\" ~ n_hens.y)) %>%\n  select(-n_hens.x, -n_hens.y)"
  },
  {
    "objectID": "posts/eggs/index.html#actually-plotting",
    "href": "posts/eggs/index.html#actually-plotting",
    "title": "Cage-free vs. caged hens in the US",
    "section": "Actually plotting",
    "text": "Actually plotting\nI went with a simple stacked density plot. Watch how the number of cage-free goes from\n\n\nCode\nplot <- egg_clean2 %>%\n  ggplot(aes(x = observed_month, y = n_hens/1000000, fill = factor(housing, levels = c(\"Traditional\", \"Cagefree\")),\n             color = factor(housing, levels = c(\"Traditional\", \"Cagefree\")),\n             label = n_hens/1000000)) +\n  geom_density(position = 'stack', stat = 'identity') +\n  geom_point(data = subset_for_points) +\n  geom_text(data = subset_for_points, aes(label = round(n_hens/1000000)), hjust = 0.5, vjust = -1, size = 6, family = \"serif\") +\n  scale_fill_manual(values = c(\"#ffefd5\", \"#e1bf92\")) +\n  scale_color_manual(values = c(\"#e1bf92\", \"#83502e\")) +\n  annotate(geom = \"text\", x = as_date(\"2019/1/1\"), y = 30, label = \"Cage-Free\", color = \"#808080\",\n           family = \"Stencil\", angle = 2.5, size = 15, alpha = 0.5) +\n  annotate(geom = \"text\", x = as_date(\"2019/1/1\"), y = 190, label = \"Caged\", color = \"#808080\",\n           family = \"Stencil\", angle = 2.5, size = 15, alpha = 0.5) +\n  scale_x_date(breaks = dates_for_plot) +\n  xlab(\"\") +\n  ylab(\"Number of egg-producing hens (millions)\") +\n  labs(fill = \"Housing\", title = \"Number of Cage-free hens in the US is constatly rising\",\n       subtitle = \"Relative number of Cage-free hens in the US in the years 2016-2021\",\n       caption = \"Tomer Zipori | #TidyTuesday | Source: The Humane League's US Egg Production dataset\") +\n  guides(color = \"none\", fill = \"none\") +\n  theme_classic() +\n  theme(axis.title = element_text(size = 16, color = \"#83502e\"),\n        axis.text.x = element_text(size = 13, color = \"#83502e\"),\n        axis.text.y = element_text(size = 13, color = \"#83502e\"),\n        plot.title = element_text(hjust = 0.5, size = 18, color = \"#83502e\"),\n        plot.subtitle = element_text(hjust = 0.5, size = 13, family = \"serif\", color = \"#83502e\"),\n        plot.caption = element_text(family = \"serif\", color = \"#83502e\"),\n        plot.margin = margin(0.5,0.5,0.5,0.7, \"cm\"),\n        plot.background = element_rect(fill = \"#fffaf0\"),\n        panel.background = element_rect(fill = \"#fffaf0\"))\nplot"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/football_fake_news/index.html",
    "href": "posts/football_fake_news/index.html",
    "title": "Real Vs. Fake news in football",
    "section": "",
    "text": "As part of my data-science courses this fall semester, I have started also to get into Natural Language Processing (NLP, but of the good kind). This is the first time I have ever tried it, so let’s see how it went.\nBrief overview of the post:\n      1. Basic preprocessing and creation of Data Feature Matrix (DFM), using packages stringr and quanteda.\n      2. Classification with Naive-Bayes method.\n      3. Classification with Logistic regression, and feature importance plot."
  },
  {
    "objectID": "posts/football_fake_news/index.html#adding-labels",
    "href": "posts/football_fake_news/index.html#adding-labels",
    "title": "Real Vs. Fake news in football",
    "section": "Adding labels",
    "text": "Adding labels\n\nreal$label <- \"real\"\nfake$label <- \"fake\""
  },
  {
    "objectID": "posts/football_fake_news/index.html#combining-data-frames",
    "href": "posts/football_fake_news/index.html#combining-data-frames",
    "title": "Real Vs. Fake news in football",
    "section": "Combining data frames",
    "text": "Combining data frames\n\ntweets <- rbind(real, fake) |>\n  drop_na() |> # dropping NA rows\n  mutate(tweet = str_trim(tweet) |> str_squish()) # removing white-spaces in the start and beginning of tweet, and between words"
  },
  {
    "objectID": "posts/football_fake_news/index.html#creating-dfms-cleaning-text",
    "href": "posts/football_fake_news/index.html#creating-dfms-cleaning-text",
    "title": "Real Vs. Fake news in football",
    "section": "Creating DFMs & cleaning text",
    "text": "Creating DFMs & cleaning text\nCreating a count-based DFM (frequency of each word in each document). Before creating the DFM I employ 3 steps of normalization:\n      1. Removing special characters and expressions (punctuation, urls…).\n      2. Converting words to their stem-form (the word example converts to exampl). This step should decrease       the number of unique tokens, so that our DFM would be less sparse.\n      3. Converting every character to lower-case letters.\n\ndata_feature_mat <- corp |>\n  tokens(remove_punct = T, remove_numbers = T, remove_url = T, remove_separators = T, remove_symbols = T) |>\n  tokens_wordstem() |>\n  dfm(tolower = T)\n\nhead(data_feature_mat)\n\nDocument-feature matrix of: 6 documents, 18,869 features (99.89% sparse) and 1 docvar.\n       features\ndocs    sun down technic director al-ah respect us and play to\n  text1   1    1       1        1     1       1  1   1    1  1\n  text2   0    0       0        0     0       0  0   0    0  2\n  text3   0    0       1        1     0       0  0   0    0  0\n  text4   0    0       0        0     0       0  0   1    0  0\n  text5   0    0       0        0     0       0  0   0    0  1\n  text6   0    0       0        0     0       0  0   0    0  0\n[ reached max_nfeat ... 18,859 more features ]"
  },
  {
    "objectID": "posts/football_fake_news/index.html#naive-bayes-classifier",
    "href": "posts/football_fake_news/index.html#naive-bayes-classifier",
    "title": "Real Vs. Fake news in football",
    "section": "Naive-Bayes classifier",
    "text": "Naive-Bayes classifier\nA Naive-Bayes classifier predicts the class (in this case of a document) in the following way:\n\n\nThe likelihood of each token to appear in documents of each class is calculated from the training data. For example, if the word manager appeared in \\(11\\)% of the documents in the first class, and in \\(2\\)% of the documents in the second class, the likelihood of it in each class is:\n\\[\n\\displaylines{p(manager\\ |\\ class\\ 1)=0.11\\\\p(manager\\ |\\ class\\ 2)=0.02}\n\\]\nThe prior probability of each document to be classified to each class is also learned from the training data, and it is the base-rate frequencies of the two classes.\n\nFor each document in the test set, the likelihood of it given it is from each of the classes is calculated by multiplying the likelihoods of the tokens appearing in it. So if a document is for example the sentence I like turtles, then the likelihood of it to belong class 1 is:\n\\[\n\\displaylines{p(I\\ |\\ class\\ 1)\\cdotp(like\\ |\\ class\\ 1)\\cdotp(turtles\\ |\\ class\\ 1)}\n\\]\nMore formally, if a document belong to a certain class \\(k\\), then it’s likelihood of being comprised of a set of tokens \\(t\\) is:\n\\[\n\\prod_{i=1}^{n}p(t_i\\ |\\ class\\ k)\n\\]\nAccording to Bayes theorem, the probability of the document to belong to class \\(k\\) - the posterior probability - is proportional to the product of the likelihood of it’s tokens given this class and the prior probability of any document to belong to this class:\n\\[\np(class\\ k\\ |\\ t) \\propto p(t\\ |\\ class\\ k)\\cdotp(class\\ k)\n\\]\nBecause the Naive-Bayes classifier is comparing between classes, the standardizing term is not needed. The class that has the largest product of prior and likelihood is the class the document will be classified to."
  },
  {
    "objectID": "posts/football_fake_news/index.html#fitting-the-model",
    "href": "posts/football_fake_news/index.html#fitting-the-model",
    "title": "Real Vs. Fake news in football",
    "section": "Fitting the model",
    "text": "Fitting the model\n\nnb_model <- textmodel_nb(train_dfm, y = docvars(train_dfm, \"label\"))"
  },
  {
    "objectID": "posts/football_fake_news/index.html#test-performance",
    "href": "posts/football_fake_news/index.html#test-performance",
    "title": "Real Vs. Fake news in football",
    "section": "Test performance",
    "text": "Test performance\n\npred_nb <- predict(nb_model, newdata = test_dfm)\n\n(conmat_nb <- table(pred_nb, docvars(test_dfm, \"label\")))\n\n       \npred_nb fake real\n   fake 3893  380\n   real  143 3956\n\n\nSeems nice, let’s look at some metrics.\nConfusion matrix\n\ncaret::confusionMatrix(conmat_nb, mode = \"everything\", positive = \"real\")\n\nConfusion Matrix and Statistics\n\n       \npred_nb fake real\n   fake 3893  380\n   real  143 3956\n                                          \n               Accuracy : 0.9375          \n                 95% CI : (0.9321, 0.9426)\n    No Information Rate : 0.5179          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.8752          \n                                          \n Mcnemar's Test P-Value : < 2.2e-16       \n                                          \n            Sensitivity : 0.9124          \n            Specificity : 0.9646          \n         Pos Pred Value : 0.9651          \n         Neg Pred Value : 0.9111          \n              Precision : 0.9651          \n                 Recall : 0.9124          \n                     F1 : 0.9380          \n             Prevalence : 0.5179          \n         Detection Rate : 0.4725          \n   Detection Prevalence : 0.4896          \n      Balanced Accuracy : 0.9385          \n                                          \n       'Positive' Class : real            \n                                          \n\n\nNice results!"
  },
  {
    "objectID": "posts/football_fake_news/index.html#logistic-regression",
    "href": "posts/football_fake_news/index.html#logistic-regression",
    "title": "Real Vs. Fake news in football",
    "section": "Logistic regression",
    "text": "Logistic regression\nThe nice thing about the textmodel_lr method from the quanteda.textmodels package is that it does the Cross-Validation for us!\n\nlr_model <- textmodel_lr(x = train_dfm, y = docvars(train_dfm, \"label\"))"
  },
  {
    "objectID": "posts/football_fake_news/index.html#test-performance-1",
    "href": "posts/football_fake_news/index.html#test-performance-1",
    "title": "Real Vs. Fake news in football",
    "section": "Test performance",
    "text": "Test performance\n\npred_lr <- predict(lr_model, newdata = test_dfm)\n\n(conmat_lr <- table(pred_lr, docvars(test_dfm, \"label\")))\n\n       \npred_lr fake real\n   fake 3795  188\n   real  241 4148\n\n\nAlso seems nice.\nConfusion matrix\n\ncaret::confusionMatrix(conmat_lr, mode = \"everything\", positive = \"real\")\n\nConfusion Matrix and Statistics\n\n       \npred_lr fake real\n   fake 3795  188\n   real  241 4148\n                                          \n               Accuracy : 0.9488          \n                 95% CI : (0.9438, 0.9534)\n    No Information Rate : 0.5179          \n    P-Value [Acc > NIR] : < 2e-16         \n                                          \n                  Kappa : 0.8973          \n                                          \n Mcnemar's Test P-Value : 0.01205         \n                                          \n            Sensitivity : 0.9566          \n            Specificity : 0.9403          \n         Pos Pred Value : 0.9451          \n         Neg Pred Value : 0.9528          \n              Precision : 0.9451          \n                 Recall : 0.9566          \n                     F1 : 0.9508          \n             Prevalence : 0.5179          \n         Detection Rate : 0.4955          \n   Detection Prevalence : 0.5242          \n      Balanced Accuracy : 0.9485          \n                                          \n       'Positive' Class : real            \n                                          \n\n\nSlight improvement…"
  },
  {
    "objectID": "posts/football_fake_news/index.html#plot-important-words-for-classification",
    "href": "posts/football_fake_news/index.html#plot-important-words-for-classification",
    "title": "Real Vs. Fake news in football",
    "section": "Plot important words for classification",
    "text": "Plot important words for classification\n\n\nCode\nlr_summary <- summary(lr_model) # summarizing the model\n\ncoefs <- data.frame(lr_summary$estimated.feature.scores) # extracting coefficients\n\n\ncol_vec <- c(\"#fc7753\", \"#e3e3e3\", \"#66d7d1\", \"black\")\n\ncoefs |>\n  \n  # preparing df for plot\n  rownames_to_column(var = \"Token\") |>\n  rename(Coefficient = real) |>\n  filter(Coefficient != 0 & Token != \"(Intercept)\") |>\n  mutate(bigger_then_0 = Coefficient > 0) |>\n  \n  # ggplotting\n  ggplot(aes(x = Token, y = Coefficient, color = bigger_then_0)) +\n  geom_point() +\n  scale_color_manual(values = c(col_vec[1], col_vec[3])) +\n  scale_y_continuous(n.breaks = 10) +\n  labs(title = \"Most important words for classifying if a tweet is fake news\",\n       x = \"\") +\n  theme_classic() +\n  geom_hline(yintercept = 0, color = \"black\", linetype = 2, linewidth = 1, show.legend = F) +\n  theme(plot.background = element_rect(color = col_vec[2], fill = col_vec[2]),\n        panel.background = element_rect(color = col_vec[2], fill = col_vec[2]),\n        axis.line = element_line(color = col_vec[4]),\n        axis.title = element_text(color = col_vec[4]),\n        axis.text = element_text(color = col_vec[4]),\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1, size = 12, face = \"bold\"),\n        legend.position = \"none\",\n        plot.title = element_text(size = 16, color = col_vec[4], hjust = .5, family = \"serif\", face = \"bold\"))"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Tomer Zipori’s resume",
    "section": "",
    "text": "Tomer Zipori\n\n\n\n\n\n tomerzip@post.bgu.ac.il\n Github\n Twitter\n LinkedIn\n 054-2520843\n\n\n\n\n\nExperienced in statistical analysis, statistical learning models, and data visualization methods methods.\nExperience and knowledge in many Machine Learning methods: Classification, Regression and Natural Language Processing.\nHighly skilled in R, Intermediate level Python.\nVisit my data science blog.\n\n\n\n\nLast updated on 2023-05-14."
  },
  {
    "objectID": "cv.html#title",
    "href": "cv.html#title",
    "title": "Tomer Zipori’s resume",
    "section": "Tomer Zipori",
    "text": "Tomer Zipori\n\nCurrently searching for a PhD student position\nPlease note that this is a real resume, and I’m really looking for a PhD student position at the moment. I made this resume because Yihui asked me if I’d like to test the pagedown package with my resume. If you are interested in my background and skills, please feel free to contact me."
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Tomer Zipori’s resume",
    "section": "Education",
    "text": "Education\n\nHebrew University in Jerusalem\nB.A. in Psychology and Philosophy\nJerusalem, Israel\n2018 - 2021\nThesis: Inter-personal differences in the effect of parental modeling on sharing behavior during middle childhood.\nSupervisor: Prof. Ariel Knafo-Noam\nGraduated Magna Cum Laude\n\n\nBen-Gurion University of the Negev\nM.A. in Cognitive Psychology\nBe’er Sheva, Israel\n2022 - Now\nThesis: Effects of Social-Status on face identification.\nSupervisor: Dr. Niv Reggev"
  },
  {
    "objectID": "cv.html#research-experience",
    "href": "cv.html#research-experience",
    "title": "Tomer Zipori’s resume",
    "section": "Research Experience",
    "text": "Research Experience\n\nGraduate Research Assistant\nThe Social Development Lab, Hebrew University in Jerusalem\nJerusalem, Israel\n2019 - 2020\n\nParticipation in the “Young Researchers” and writing an extended seminar term paper.\n\n\n\nSCMB Lab\nBe’er Sheva, Israel\n2022 - Now\n\nInvestigated how social-status of target faces affects their identification by others.\nManaging a team of research assistants.\nGuiding students in their research projects in the lab."
  },
  {
    "objectID": "cv.html#professional-experience",
    "href": "cv.html#professional-experience",
    "title": "Tomer Zipori’s resume",
    "section": "Professional Experience",
    "text": "Professional Experience\n\nContent Writer\nClassit\nIsrael\n2021 - 2023\n\n\nWrote high-school level math questions and teaching units."
  },
  {
    "objectID": "cv.html#teaching-experience",
    "href": "cv.html#teaching-experience",
    "title": "Tomer Zipori’s resume",
    "section": "Teaching Experience",
    "text": "Teaching Experience\n\nIntroduction to Statistics\nTeaching Assistant in the Intro to statistics course for BA students.\nBe’er Sheva, Israel\n2022 - 2023\n\n\nMethodology in experimental Psychology\nTeaching Assistant in the Methodology in experimental Psychology. course for BA students. As part of the course I have guided groups of students in forming research questions, planning and conducting experiments, statistically analyzing results, and forming conclusions.\nBe’er Sheva, Israel\n2022 - 2023\n\n\nMITAM Instructor\nOfek MITAM\nIsrael\n2023 - Now\nTeaching a class of the Ofek MITAM course. The MITAM is the entrance test for advanced degrees in psychology - equivalent to the GRE."
  }
]