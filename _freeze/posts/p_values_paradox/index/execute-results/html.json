{
  "hash": "d4a206f33232bd732860d7a34d83d08a",
  "result": {
    "markdown": "---\ntitle: \"Rejecting the null hypothesis or not?\"\ndescription: \"When different interpretations of probability disagree on the meaning of p-values\"\nauthor: \"Tomer Zipori\"\ndate: 2023-09-04\ntitle-block-banner: \"#990011FF\"\ntitle-block-banner-color: \"#FCF6F5FF\"\ncategories: [code, theory]\nimage: \"morty.jpg\"\nexecute: \n  warning: false\n  message: false\n  cache: true\nformat:\n  html:\n    theme: cosmo\n    toc: true\n    toc-depth: 3\n    toc-location: right\neditor: visual\n---\n\n::: {.cell hash='index_cache/html/unnamed-chunk-1_4a1f1f8041683a6b38ef203d33a21ddb'}\n\n:::\n\n\nIn the current post I try to explain what appears to be a paradox: given identical data, different schools of statistics disagree about the appropriate conclusion.\n\nTraditionally, there are two ways to interpret Probability:\\\n1. Probability is the relative frequency of events at infinity - the Frequentist school.\\\n2. Probability represents the degree of beliefs - The Bayesian school.\n\nClassic hypothesis testing in the social sciences usually follows the Frequentist school, the probability of errors is calculated as the relative frequency of extreme data under the relevant hypotheses.\n\n# Distribution of p-values when the null hypothesis is true\n\nAn example for Frequentist logic in hypothesis testing is the probability of type I errors, also known as $\\alpha$: rejecting the null hypothesis when it is true.\n\nThe famous (and notorious?) 5% convention, refers to the relative frequency of significant tests when the null is true.\n\nLet's take infinite (140,000) samples of $n=30$ from populations with equal means, and compare each of them with a t-test:\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-2_548b33e77730ca06fd6673c933fdbac2'}\n\n:::\n\n::: {.cell hash='index_cache/html/unnamed-chunk-3_0fa4db21c223acdc037ff64c1a4ef9e7'}\n\n```{.r .cell-code}\nn <- 30\nmu0 <- 0\nsd0 <- 1\nmu1 <- 0\n\np_values_null <- rep(NULL, 140000)\n\nfor (i in c(1:140000)) {\n  group1 <- rnorm(n, mu0, sd0)\n  group2 <- rnorm(n, mu1, sd0)\n  \n  test <- t.test(group1, group2, alternative = \"less\")\n  \n  p_values_null[i] <- test$p.value\n}\n```\n:::\n\n\nHow many tests were significant?\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-4_cb4909299457328effb1cb0948af3f81'}\n\n```{.r .cell-code}\nlength(p_values_null[p_values_null < .05])/length(p_values_null)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.05020714\n```\n:::\n:::\n\n\nWhat if we defined $\\alpha=0.067$?\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-5_1d5b65e4de494cd2931e85af1c9a119b'}\n\n```{.r .cell-code}\nlength(p_values_null[p_values_null < .067])/length(p_values_null)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.06699286\n```\n:::\n:::\n\n\nPutting it in different words, the probability of getting p-values under a certain threshold equals to that threshold. or, **When population means are equal, p-values are distributed uniformly.**\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-6_5d48094300e64b10c5800ab0bf5c87a0'}\n\n```{.r .cell-code}\nhist(p_values_null, breaks = 50, xlab = NULL, freq = F)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n# Distribution of p-values when the null hypothesis is false\n\nWhat happens when population means are *not* equal? The probability of detecting an existing effect (e.g. getting a significant result) is called *Power*. Naturally, statistical Power is larger for larger differences between populations means.\n\nLets simulate yet again infinite (140,000) t-tests, this time between samples from populations with difference of means of 0.65.\n\n## Distribution of p-values with 80% power\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-7_2215717d612e3f0dfc96747742b4ba0e'}\n\n:::\n\n::: {.cell hash='index_cache/html/unnamed-chunk-8_b84ad3681c12cb26f25d7efb8710c468'}\n\n```{.r .cell-code}\nn <- 30\nmu0 <- 0\nsd0 <- 1\nmu1 <- 0.65\n\np_values_80 <- rep(NULL, 140000)\n\nfor (i in c(1:140000)) {\n  group1 <- rnorm(n, mu0, sd0)\n  group2 <- rnorm(n, mu1, sd0)\n  \n  test <- t.test(group1, group2, alternative = \"less\")\n  \n  p_values_80[i] <- test$p.value\n}\n```\n:::\n\n\nWhat is the probability of getting a significant result?\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-9_d2652adefe2dd3a09b883fb7ced87431'}\n\n```{.r .cell-code}\nlength(p_values_80[p_values_80 < .05])/length(p_values_80)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.80015\n```\n:::\n:::\n\n\nHow does the distribution of p-values look when there is 80% Power (probability of 0.8 of getting a significant result)?\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-10_19f05b78389aea7d7801e35033c3e135'}\n\n```{.r .cell-code}\nhist(p_values_80, breaks = 50, xlab = NULL, freq = F)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nSmaller p-values are more probable, and the uniform distribution from before \"gets skewed\".\n\n# Distribution of p-values with 99% power\n\nIncreasing the difference between population means causes the p-values distribution to get more and more skewed.\n\n\n::: {.cell hash='index_cache/html/theoretical distributions_26c72fa5e5500d4373ef718e3d574972'}\n\n:::\n\n::: {.cell hash='index_cache/html/unnamed-chunk-11_4e0ebc3a0b0668ea36a4c41dfd9bff2c'}\n\n:::\n\n::: {.cell hash='index_cache/html/unnamed-chunk-12_2a075f0edfecfdcf394d8c185a18acaa'}\n\n```{.r .cell-code}\nn <- 30\nmu0 <- 0\nsd0 <- 1\nmu1 <- 1.0376218\n\np_values_99 <- rep(NULL, 140000)\n\nfor (i in c(1:140000)) {\n  group1 <- rnorm(n, mu0, sd0)\n  group2 <- rnorm(n, mu1, sd0)\n  \n  test <- t.test(group1, group2, alternative = \"less\")\n  \n  p_values_99[i] <- test$p.value\n}\n\nlength(p_values_99[p_values_99 < .05])/length(p_values_99)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9899\n```\n:::\n:::\n\n\nAnd the distribution of p-values:\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-13_e0b9cac8b18b96898f9132835bda7ab9'}\n\n```{.r .cell-code}\nhist(p_values_99, breaks = 100, freq = F, xlab = NULL)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n# Reversing the conditionality\n\n![All about the Bayes](bayes.png)\n\nAs we said before, p-values represent the probability of getting certain data when the null hypothesis is true. What if we looked at it from the other side? What is the probability that the null hypothesis is true, given a certain p-value?\n\nIn the Frequentist school, this question seems weird: what is the relative frequency at infinity of the null hypothesis being true? That is, out of infinite worlds, in how many of them the null is true?\n\nSwitching our perspective to be Bayesian, assigning probabilities to hypotheses becomes less weird. That is because Bayesians see probability as a measure of confidence in some statement/model/possible world.\n\nAs it's name suggests, the Bayesian interpretation of probability is rooted in the Bayes theorem for conditional probabilities:\n\n$$\np(A/B)= \\frac {p(A) \\cdot p(B/A)}{p(B)}\n$$\n\nConfidence in $A$ given the occurrence of $B$ is proportional to the product of the prior confidence in $A$ and the conditional probability of $A$ on $B$ - also called the **Likelihood** of $A$.\n\n::: callout-important\n## Important\n\nBayes' theorem is **not** a matter of interpretation. it can be derived from the basic probability axioms. It is linked with the interpretation of probability as a measure of confidence in statements because it defines the procedure of belief updating.\n:::\n\nRe-phrasing the theorem, it is possible to formulate the posterior probability of a hypothesis given some observed data:\n\n$$\np(hypothesis/data)= \\frac {p(hypothesis) \\cdot p(data/hypothesis)}{p(data)}\n$$\n\nAnother possibility of conceptualize the Bayes mindset is seeing the posterior probability of the hypothesis as an *update* to the prior probability of it.\n\nBayes theorem also enable the comparison of two hypotheses using the ratio between the posteriors:/ $$\n\\frac{p(hypothesis \\, 1/data)}{p(hypothesis \\, 2/data)}= \\frac {\\frac {p(hypothesis \\, 1) \\cdot p(data/hypothesis \\, 1)}{p(data)}}{\\frac {p(hypothesis \\, 2) \\cdot p(data/hypothesis \\, 2)}{p(data)}}\n$$\n\nCancelling the annoying denominator yields the following: $$\n\\frac {p(hypothesis \\, 1) \\cdot p(data/hypothesis \\, 1)}{p(hypothesis \\, 2) \\cdot p(data/hypothesis \\, 2)}\n$$\n\nThe ratio of posterior probabilities equals to the product of the prior ratio and the likelihood ratio. When this value is bigger then 1, it can considered as evidence in favor of Hypothesis 1.\n\n# Posterior probability of hypotheses given p-values\n\nUsing Bayes theorem, it is now possible to construct a posterior probability for each of the original hypotheses, given an observed p-value.\n\n## Likelihood\n\nWhat is the probability of p-values given each hypothesis? we actually answered this question earlier when we visualized the density distributions of p-values.\n\nFor example, the probability of observing a p-value between 0.03 and 0.04 given the null hypothesis is:\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-14_8c5154280da19f087e9481c2c0efa948'}\n\n```{.r .cell-code}\ndata.frame(\"p\" = p_values_null) |>\nggplot(aes(x = p)) +\n  geom_density(alpha = .6, fill = \"#FF6666\") +\n  geom_area(aes(x = stage(p, after_stat = oob_censor(x, c(0.03, 0.04)))), stat = \"density\", fill = \"gray\") +\n  annotate(\"text\", x = 0.14, y = 0.5, label = \"italic(p) == 0.0099\",\n  parse = TRUE) +\n  geom_vline(xintercept = 0.03, color = \"black\", linewidth = 0.7) +\n  geom_vline(xintercept = 0.04, color = \"black\", linewidth = 0.7) +\n  scale_x_continuous(breaks = seq(0, 1, 0.05)) +\n  theme_classic() +\n  labs(y = \"\", x = \"P-Value\", title = \"Density distribution of p-values under the null hypothesis\",\n       subtitle = \"Gray area represents the probability of p between 0.03 and 0.04\") +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        plot.title = element_text(family = \"serif\", size = 16, hjust = 0.5),\n        plot.subtitle = element_text(family = \"serif\", size = 13, hjust = 0.5))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nThe probability of observing the same range of p-values when the effect size is $0.65$ is:\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-15_c530f56ad09a2f96cd6afafafbec104c'}\n\n```{.r .cell-code}\ndata.frame(\"p\" = p_values_80) |>\nggplot(aes(x = p)) +\n  geom_density(alpha = .6, fill = \"#FF6666\") +\n  geom_area(aes(x = stage(p, after_stat = oob_censor(x, c(0.03, 0.04)))), stat = \"density\", fill = \"gray\") +\n  annotate(\"text\", x = 0.14, y = 8, label = \"italic(p) == 0.0437\",\n  parse = TRUE) +\n  geom_vline(xintercept = 0.03, color = \"black\", linewidth = 0.7) +\n  geom_vline(xintercept = 0.04, color = \"black\", linewidth = 0.7) +\n  scale_x_continuous(breaks = seq(0, 1, 0.05)) +\n  theme_classic() +\n  labs(y = \"\", x = \"P-Value\", title = \"Density distribution of p-values when the effect size is 0.65\",\n       subtitle = \"Gray area represents the probability of p between 0.03 and 0.04\") +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        plot.title = element_text(family = \"serif\", size = 16, hjust = 0.5),\n        plot.subtitle = element_text(family = \"serif\", size = 13, hjust = 0.5))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nAnd for effect size of $1.03$?\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-16_8e0f9f5d4c2a6a1aa1ebc651eed9d81d'}\n\n```{.r .cell-code}\ndata.frame(\"p\" = p_values_99) |>\nggplot(aes(x = p)) +\n  geom_density(alpha = .6, fill = \"#FF6666\") +\n  geom_area(aes(x = stage(p, after_stat = oob_censor(x, c(0.03, 0.04)))), stat = \"density\", fill = \"gray\") +\n  annotate(\"text\", x = 0.14, y = 376, label = \"italic(p) == 0.0056\",\n  parse = TRUE) +\n  geom_vline(xintercept = 0.03, color = \"black\", linewidth = 0.7) +\n  geom_vline(xintercept = 0.04, color = \"black\", linewidth = 0.7) +\n  scale_x_continuous(breaks = seq(0, 1, 0.05), limits = c(0, 1)) +\n  theme_classic() +\n  labs(y = \"\", x = \"P-Value\", title = \"Density distribution of p-values when the effect size is 1.03\",\n       subtitle = \"Gray area represents the probability of p between 0.03 and 0.04\") +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        plot.title = element_text(family = \"serif\", size = 16, hjust = 0.5),\n        plot.subtitle = element_text(family = \"serif\", size = 13, hjust = 0.5))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nThe probability of getting a p-value in the range of $[0.03,0.04]$ is lower for greater effect sizes.\n\n# Prior\n\nExplicitly incorporating prior knowledge into statistical modelling is a great strength of Bayesian analysis. But, in order to keep things simple, I will stay agnostic and assign each hypothesis an equal prior probability. This is known as a *Flat prior*.\\\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-17_d582861de20d8490d4efc00d5f791050'}\n\n```{.r .cell-code}\nggplot(data.frame(x = c(0, 1.5)), aes(x)) +\n  stat_function(fun = dunif, geom = \"area\", args = list(min = 0, max = 1.5), fill = \"#FF6666\") +\n  scale_x_continuous(breaks = seq(0, 1.5, 0.1)) +\n  labs(x = \"Effect Size\", y = \"\", title = \"Each effect size has equal prior probability\") +\n  theme_classic() +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        plot.title = element_text(family = \"serif\", size = 16, hjust = 0.5),\n        axis.title.x = element_text(family = \"serif\", size = 11))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nIn this case the prior ratio cancels out and posterior ratio is equal to the likelihood ratio.\n\n# Rejecting the null?\n\nGiven p-value between 0.03 and 0.04, should we reject the null hypothesis or not?\n\nAs good Frequentists, we should see all p-values under the pre-defined alpha of 0.05 as evidence against the null hypothesis.\n\n**But**, as good Bayesian we should ask ourselves, Given $0.03 \\le p \\le 0.04$, what is more probable? effect size of $0$ or $1.03$?\n\nDefining the the flat prior:\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-18_69a091375d8d3c83be7fc28f2191b83c'}\n\n```{.r .cell-code}\nprior_null <- 0.5\nprior_1.03 <- 1 - prior_null\n```\n:::\n\n\nDefining the likelihood of p-values in the given range, under each hypothesis:\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-19_0db9ca113d09d0b4a08f6f2fe07dc6c8'}\n\n```{.r .cell-code}\nlikelihood_null <- length(p_values_null[p_values_null >= 0.03 & p_values_null <= 0.04])/length(p_values_null)\n\nlikelihood_1.03 <- length(p_values_99[p_values_99 >= 0.03 & p_values_99 <= 0.04])/length(p_values_99)\n```\n:::\n\n\nWhat is the posterior ratio?\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-20_91e5fd11310bc36817cf626f89d3fa5d'}\n\n```{.r .cell-code}\n(posterior_ratio <- (prior_null * likelihood_null)/(prior_1.03 * likelihood_1.03))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.771684\n```\n:::\n:::\n\n\nGiven a p-value between 0.03 and 0.04, an effect size of $0$ is 1.77 more probable then effect size of $1.03$!\n\n# So, what is the answer???\n\nWhat is the solution to this paradox? who is right? is statistical hypothesis testing is broken?\n\nThe source of the current disagreement is the interpretation of probability. Frequentists insist that hypotheses are True or False. There is no place for degrees of confidence in statements. Bayesians assign probability to hypotheses and statements themselves.\n\nWhether you find yourself in the first camp, or in the second camp, remember:\n\n> All models are wrong (George Box)\n\nThis is our superpower!\n\n![Meme-form](meme.png)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}