{
  "hash": "bacbab7cf23948d23c6ed3583572408d",
  "result": {
    "markdown": "---\ntitle: \"Netflix trends\"\ndescription: \"Text data visualization and analysis\"\nauthor: \"Tomer Zipori\"\ndate: 2023-06-13\ntitle-block-banner: \"#990011FF\"\ntitle-block-banner-color: \"#FCF6F5FF\"\ncategories: [code, text analysis, visualization, NLP]\nimage: \"medieval.png\"\nexecute: \n  warning: false\n  message: false\n  cache: true\nformat:\n  html:\n    theme: cosmo\n    toc: true\n    toc-depth: 2\n    toc-location: right\neditor: visual\n---\n\n\nThis post will be dedicated yet again to some experimentation with text analysis. I will be using mostly basic and classic NLP tools like word counting and DFM's (Document-Feature matrix).\n\nI hope that in the next NLP post I will be using some more advanced methods like transformers (go BERT!).\n\nI will be using the popular *Netflix* [dataset](https://www.kaggle.com/code/niharika41298/netflix-visualizations-recommendation-eda) from Kaggle. It contains data of movies and TV shows added to Netflix's streaming service since 2008.\n\nIt's going to be a relatively short post, the first part will include some nice visualization (word clouds), and in the second part I will try and build a mini show-recommendation function.\n\n# Setup\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-1_529bc7f50f1b802203b3bb60d4088b34'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(extrafont)\nlibrary(lubridate)\nlibrary(ggwordcloud)\nlibrary(quanteda)\nlibrary(quanteda.textplots)\nlibrary(quanteda.textstats)\n```\n:::\n\n::: {.cell hash='index_cache/html/unnamed-chunk-2_a50dde8e6e051f5c3ca30ffbb6c23f13'}\n\n:::\n\n\n# Data loading\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-3_a5363e4f65ea604dd213d151a4b08e3b'}\n\n```{.r .cell-code}\ndata <- read_csv(\"netflix_titles.csv\", show_col_types = F)\n\nhead(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 12\n  show_id type    title    director cast  country date_added release_year rating\n  <chr>   <chr>   <chr>    <chr>    <chr> <chr>   <chr>             <dbl> <chr> \n1 s1      Movie   Dick Jo… Kirsten… <NA>  United… September…         2020 PG-13 \n2 s2      TV Show Blood &… <NA>     Ama … South … September…         2021 TV-MA \n3 s3      TV Show Ganglan… Julien … Sami… <NA>    September…         2021 TV-MA \n4 s4      TV Show Jailbir… <NA>     <NA>  <NA>    September…         2021 TV-MA \n5 s5      TV Show Kota Fa… <NA>     Mayu… India   September…         2021 TV-MA \n6 s6      TV Show Midnigh… Mike Fl… Kate… <NA>    September…         2021 TV-MA \n# ℹ 3 more variables: duration <chr>, listed_in <chr>, description <chr>\n```\n:::\n:::\n\n\nLet's look at one of the more interesting fields in here:\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-4_16530a3e7260785018018613e5e30095'}\n\n```{.r .cell-code}\nhead(data$description, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"As her father nears the end of his life, filmmaker Kirsten Johnson stages his death in inventive and comical ways to help them both face the inevitable.\"\n[2] \"After crossing paths at a party, a Cape Town teen sets out to prove whether a private-school swimming star is her sister who was abducted at birth.\"     \n[3] \"To protect his family from a powerful drug lord, skilled thief Mehdi and his expert team of robbers are pulled into a violent and deadly turf war.\"      \n```\n:::\n:::\n\n\nWe get to have the description of every TV show or movie added. This has potential...\n\n# Preprocessing\n\nThis part is sort of boring, but I'm creating a *Date* column in `lubridate` format. Didn't know that `R` has built in object for this.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-5_424959b24104884091df0de45b81ec77'}\n\n```{.r .cell-code}\nmonths_names <- data.frame(name = month.name,\n                           month = seq(1,12,1))\n\ndata_clean <- data |>\n  mutate(month_added = word(date_added, 1),\n         day_added = str_remove_all(word(date_added, 2), \",\"),\n         year_added = word(date_added, 3)) |>\n  left_join(months_names, by = join_by(month_added == name)) |>\n  mutate(date_added = paste(day_added, month, year_added, sep = \".\")) |>\n  select(-month_added, -day_added, -month) |>\n  mutate(date_added = dmy(date_added)) |>\n  mutate(date_added_ym = zoo::as.yearmon(date_added))\n```\n:::\n\n\n## Text analysis of show description\n\nHere I'm creating the DFM. It is basically a huge matrix with each document (show description) being a row, and each unique word being a column. Values represent the frequency of each word in each document.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-6_9b7c23a9fb326e4d851fa384bb618760'}\n\n```{.r .cell-code}\ndata_clean_dfm <- data_clean |>\n  mutate(description = str_remove_all(description, pattern = \"[[:punct:]]\")) |>\n  corpus(text_field = \"description\") |>\n  tokens(remove_separators = T) |>\n  tokens_remove(stopwords()) |>\n  dfm()\n\ndata_clean_dfm\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDocument-feature matrix of: 8,807 documents, 20,824 features (99.93% sparse) and 13 docvars.\n       features\ndocs    father nears end life filmmaker kirsten johnson stages death inventive\n  text1      1     1   1    1         1       1       1      1     1         1\n  text2      0     0   0    0         0       0       0      0     0         0\n  text3      0     0   0    0         0       0       0      0     0         0\n  text4      0     0   0    0         0       0       0      0     0         0\n  text5      0     0   0    1         0       0       0      0     0         0\n  text6      0     0   0    0         0       0       0      0     0         0\n[ reached max_ndoc ... 8,801 more documents, reached max_nfeat ... 20,814 more features ]\n```\n:::\n:::\n\n\nFor example: The word 'father' appear once in the first document, and doesn't appear in documents 2 to 6.\n\n# Content trends in Netflix's new additions\n\nAlthough it is pretty basic, the DFM allows us to check many things. For example: Did Netflix's new content changed over the years? I thought it would be interesting to see the most frequent words in the descriptions of items added in each year.\n\nAs you can see, I have added a relative frequency column to the data. This is to control for the different number of items added in each year.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-7_5e0197b903c4fd5496c0d251327df0bc'}\n\n```{.r .cell-code}\nplot_data <- data_clean_dfm |>\n  textstat_frequency(groups = year_added) |>\n  filter(rank %in% c(1:100) & group %in% c(\"2016\", \"2017\", \"2017\", \"2019\", \"2020\", \"2021\")) |>\n  group_by(group) |>\n  mutate(r_frequency = 100*frequency/sum(frequency)) |>\n  ungroup()\n\ncloud1 <- plot_data |>\n  ggplot(aes(label = feature, size = r_frequency, color = r_frequency)) +\n  scale_size_area(max_size = 7) +\n  geom_text_wordcloud(seed = 14, ) +\n  scale_color_gradient(low = \"#000000\", high = \"#ff0000\") +\n  facet_wrap(~group, nrow = 1) +\n  labs(title = \"Content trends in Netflix's new additions every year\",\n       subtitle = \"Most common words in the description of every newly added Movie or TV series\") +\n  theme_minimal() +\n  theme(panel.background = element_blank(),\n        plot.background = element_blank(),\n        strip.text.x = element_text(size = 30, family = \"Avengeance Heroic Avenger\"),\n        plot.title = element_text(size = 32, family = \"Bebas Neue\", hjust = 0.5),\n        plot.subtitle = element_text(size = 22, family = \"Bebas Neue\", hjust = 0.5))\n```\n:::\n\n::: {.cell hash='index_cache/html/unnamed-chunk-8_5ad1f689cdb13c4bdd642584b4c324d2'}\n\n```{.r .cell-code}\ncloud1\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=1248}\n:::\n:::\n\n\n# Recommendation generator\n\nThe Algorithm. Everyone online for the past 10 years is the algorithm every app or website uses to predict our taste in videos/products/TV shows/twitts or Facebook posts.\n\nIn this part I try to create my own mini 'algorithm', But first, a short explanation on vector representation and similarity between texts.\n\n## TF-IDF (Term Frequency - Inverse Document Frequency)\n\n### TF - Term Frequency\n\nUp until now I have used the most basic DFM - the count DFM. It is basically counts the number of times each word appear in the document, and is sometimes also called 'Bag of Words'.\\\nOne problem that arises in our inner statistician is that longer documents contain more words! therefore, different term frequencies can be hard to compare. The simple solution is to calculate *Proportional frequency* in the following way:\\\n$$\nterm \\ frequency= \\frac{N \\ times \\ a \\ word \\ appear \\ in \\ the \\ document}{Total \\ number \\ of \\ words \\ in \\ the \\ document}\n$$\n\nThis gives us the **Term frequency** part of the tf-idf method.\n\n### Inverse Document Frequency\n\nSo we have the term frequency of every token (word). We can start and make show recommendations, right? One problem that still exist is that some words could be prevalent in **every** document. For example, if the word 'Life' appears in a large number of descriptions, it is not so useful in identifying someones taste in TV shows and movies. If, on the other hand, the word 'Father' appear only in a small number of descriptions, it will be a lot more helpful to us. Therefore, we want to represent term *Uniqueness*. How can we do that?\n\nFor each token (word) the Inverse-Document-Frequency is calculated:\\\n$$\nidf=log( \\frac{Total \\ number \\ of \\ documents}{N \\ documents \\ containing \\ the \\ term})\n$$\n\nThis number is getting close to zero the more documents containing the word, and is getting larger the less documents contain it (and the more documents we have).\n\nTo get the final tf-idf value for every term, we simply multiply the Term-Frequency with the Inverse-Document-Frequency:\\\n$$\ntfidf(word)=tf(word) \\cdot idf(word)\n$$\n\nIn creating the tf-idf DFM, I have also converted every word to lower case and to it's stem.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-9_bc834c97b37126348385b6f15dbf8695'}\n\n```{.r .cell-code}\nnetflix_tf_idf <- data_clean_dfm |>\n  dfm_tolower() |>\n  dfm_wordstem() |>\n  dfm_tfidf()\n```\n:::\n\n\n## Cosine Similarity\n\nNow that we have the DFM, we have a vector representation of every description of every show. Every word in our dataset is a dimension and it's tf-idf in each document is the vector component. For example, the vector representation of the description of \"Attack on Titan\" is:\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-10_708fa853e80c26a69a4c9345937ac3ce'}\n\n```{.r .cell-code}\ndfm_subset(netflix_tf_idf, subset = title == \"Attack on Titan\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDocument-feature matrix of: 1 document, 15,097 features (99.90% sparse) and 13 docvars.\n         features\ndocs      father near end life filmmak kirsten johnson stage death invent\n  text779      0    0   0    0       0       0       0     0     0      0\n[ reached max_nfeat ... 15,087 more features ]\n```\n:::\n:::\n\n\nNaturally these vector are mostly zeros...\n\nBut now that we have vectors we can do all the cool things that we can do with vectors! For example, we can recommend TV shows and movies based on how they are *similar* to a previously watched show.\n\nIf we return to Physics class, we can think of vectors as arrows with length and direction.\n\n![Two vectors](vectors.png)\n\nIt is natural to think of the similarity between two vectors as the measure to which they point in the same direction. In other words, the angle between two vectors. In two dimensions, it is possible to calculate the *cosine* between two vectors the following way:\\\n$$\ncos \\theta= \\frac{ \\vec{a} \\cdot \\vec{b}}{|\\vec{a}| \\cdot |\\vec{b}|}\n$$\n\nWithout turning this into a real Physics class, in the numerator is the dot-product of the vectors, and in the denominator is the product of lengths of the vectors. For example, the cosine between the vectors $\\vec{a}=(1,1)$ and $\\vec{b}=(-4,3)$ is:\\\n$$\ncos \\theta= \\frac{1 \\cdot (-4)+1 \\cdot (3)}{ \\sqrt{1^2+1^2} \\cdot \\sqrt{(-4)^2+3^2}}=\\frac{-1}{5 \\sqrt{2}}\n$$\n\nBut what is the angle between \\~15,000 dimensions vectors?! fortunately, this calculation remains the same in any dimension.\n\nWhat is the relation between the angle and it's cosine? without digging too deep, the larger the angle, the smaller the cosine. So the larger the cosine, the greater the similarity.\n\n::: callout-note\nNice insight: the cosine similarity is nothing but the un-standardized pearson correlation coefficient! Let $x$ and $y$ be vectors of Z scores. The correlation coefficient is:\\\n$$\nr_{xy}= \\frac{\\sum_{i=1}^{N}{x_i \\cdot y_i}}{N}\n$$ While the cosine of the angle between them is:\\\n$$\ncos \\theta= \\frac{\\vec{x} \\cdot \\vec{y}}{|\\vec{x}| \\cdot |\\vec{y}|}\n$$ The length of a vector of Z scores is $\\sqrt{N}$, therefore the denominator is always: $\\sqrt{N} \\cdot \\sqrt{N}=N$.\\\nThe nominator is the dot product of the vectors, which is exactly the sum of the by-component products. Finally we get:\\\n$$\nr_{xy}=cos \\ \\theta_{xy}\n$$\n:::\n\n### Functions\n\nThe main idea of the recommendation generator is simple. If you liked a TV show or a movie, you will probably like shows with similar description. Not any similarity, cosine similarity!\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-11_69fc9a48059352d68f0a3573d23831cf'}\n\n```{.r .cell-code}\nget_recommendation <- function(show, liked = T) {\n  library(dplyr)\n  library(quanteda)\n  \n  features <- netflix_tf_idf@docvars\n  show_id <- features$docname_[tolower(features$title) %in% tolower(show)]\n  show_id <- as.integer(str_remove_all(show_id, pattern = \"text\"))\n  \n  simil_mat <- textstat_simil(netflix_tf_idf[show_id,], netflix_tf_idf, method = \"cosine\")\n  \n  if (liked) {\n  simil_df <- data.frame(shows = simil_mat@Dimnames[[2]],\n                      simil = simil_mat@x) |>\n  arrange(-simil) |>\n  inner_join(select(netflix_tf_idf@docvars, docname_, title),\n             by = join_by(shows == docname_)) |>\n    select(-shows, match = simil) |>\n    mutate(match = (match-min(match))/(max(match)-min(match))) |>\n    head(11)\n  }\n  if (!liked){\n    simil_df <- data.frame(shows = simil_mat@Dimnames[[2]],\n                      simil = simil_mat@x) |>\n  arrange(simil) |>\n  inner_join(select(netflix_tf_idf@docvars, docname_, title),\n             by = join_by(shows == docname_)) |>\n    select(-shows, match = simil) |>\n    mutate(match = 1-(match-min(match))/(max(match)-min(match))) |>\n    head(11)\n  }\n  \n  return(simil_df[-1,])\n}\n\nget_recommendation_plot <- function(show) {\n  library(ggplot2)\n  \n  plot_df <- get_recommendation(show)\n  \n  ggplot(plot_df, aes(reorder(title, -match, identity), match, fill = match)) +\n  geom_col() +\n  labs(x = \"Show\", y = \"Match\") +\n  scale_fill_gradient(low = \"#000000\", high = \"#990011FF\") +\n  theme_classic() +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),\n        legend.position = \"none\")\n}\n```\n:::\n\n\n### Getting show recommendations\n\nLet's use an example. Maybe I watched \"Attack on Titan\", what are the top 10 recommended shows for me?\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-12_2867b73c553eb5bef8fc9dca2c8e4107'}\n\n```{.r .cell-code}\nwatched <- \"Attack on Titan\"\n\nget_recommendation(watched)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       match                               title\n2  0.1570728 Little Singham aur Kaal ka Mahajaal\n3  0.1444591                             The BFG\n4  0.1367644                    Sym-Bionic Titan\n5  0.1339566                           The Giant\n6  0.1335654                               Agent\n7  0.1227649                Pup Star: World Tour\n8  0.1207346              Power Battle Watch Car\n9  0.1159523                         Rising High\n10 0.1149643           League of Legends Origins\n11 0.1136349                A Sort of Homecoming\n```\n:::\n:::\n\n\nWhat if I did not like a show? no problem, you will be recommended shows and movies that are not similar.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-13_503fd1e92bdd52acb4eb03618831b2b3'}\n\n```{.r .cell-code}\nget_recommendation(\"I Am Sam\", liked = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   match                               title\n2      1                           Ganglands\n3      1               Jailbirds New Orleans\n4      1                        Kota Factory\n5      1    My Little Pony: A New Generation\n6      1                             Sankofa\n7      1       The Great British Baking Show\n8      1                        The Starling\n9      1 Vendetta: Truth, Lies and The Mafia\n10     1                    Bangkok Breaking\n11     1    Confessions of an Invisible Girl\n```\n:::\n:::\n\n\n#### In a plot\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-14_07b9fdb76aae8df7a1eb6f0ba26d3163'}\n\n```{.r .cell-code}\nget_recommendation_plot(watched)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n# Conclusion\n\nThis post is an extra step of my in the NLP domain. As always I try to explain the theory behind the interesting parts of the process. For example, I remember when I discovered the connection between the correlation coefficient and the angle between vectors, it was really eye opening!\\\nThe recommendation functions could be vastly improved with some more modern NLP methods (Transformers!). I hope that in the near future I will have the time to post about that to.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}