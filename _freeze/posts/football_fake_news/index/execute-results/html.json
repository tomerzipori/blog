{
  "hash": "1ac1f345a7246df5da422a3589f4d353",
  "result": {
    "markdown": "---\ntitle: \"Real Vs. Fake news in football\"\nauthor: \"Tomer Zipori\"\ndescription: \"Experiment in NLP and some text classification\"\ndate: 05-12-2023\ncategories: [code, analysis, Machine Learning, NLP]\nimage: \"football-fake-news.png\"\nexecute: \n  warning: false\n  message: false\n  cache: true\nformat:\n  html:\n    theme: cosmo\n    toc: true\n    toc-depth: 2\n    toc-location: right\neditor: visual\nbibliography: references.bib\n---\n\n\nAs part of my data-science courses this fall semester, I have started also to get into *Natural Language Processing* (NLP, but of the good kind). This is the first time I have ever tried it, so let's see how it went.\n\nBrief overview of the post:\\\n      1. Basic preprocessing and creation of *Data Feature Matrix* (DFM), using packages `stringr` and `quanteda`.\\\n      2. Classification with Naive-Bayes method.\\\n      3. Classification with Logistic regression, and feature importance plot.\n\n# Background\n\nThis dataset is the \"Fake News football\" dataset from uploaded to [Kaggle](https://www.kaggle.com/datasets/shawkyelgendy/fake-news-football) by Shawky El-Gendy. It contains \\~42,000 tweets about the Egyptian football league, some of them are fake new, and some are true.\n\nThe work in this notebook is highly influenced by [@welbers2017].\n\n# Setup\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-1_f83cbd2941e4f775713a49c35d3fdadb'}\n\n```{.r .cell-code}\nlibrary(tidyverse)           # data loading, data preparation and stringr\nlibrary(quanteda)            # text preprocessing and DFM creation\nlibrary(quanteda.textmodels) # machine learning models\nlibrary(glue)                # that's a secret\n```\n:::\n\n\n# Loading data\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-2_88302c9135a715e213b3eddfca3aafc1'}\n\n```{.r .cell-code}\nreal <- read_csv(\"real.csv\", show_col_types = F)\nfake <- read_csv(\"fake.csv\", show_col_types = F)\n```\n:::\n\n\n## Adding labels\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-3_9517af9cf6061d262bc187f6f63bf795'}\n\n```{.r .cell-code}\nreal$label <- \"real\"\nfake$label <- \"fake\"\n```\n:::\n\n\n## Combining data frames\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-4_999b31d14c8b3639c25eb42c31ccba48'}\n\n```{.r .cell-code}\ntweets <- rbind(real, fake) |>\n  drop_na() |> # dropping NA rows\n  mutate(tweet = str_trim(tweet) |> str_squish()) # removing white-spaces in the start and beginning of tweet, and between words\n```\n:::\n\n\n# Text preprocessing\n\nCreating a `corpus` object, which is basically a named character vector that also holds other variables such as the tweet's label. These are called `docvars`.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-5_de1980b4bc624f3696b98deef71e6155'}\n\n```{.r .cell-code}\ncorp <- corpus(tweets, text_field = \"tweet\")\n\nhead(corp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorpus consisting of 6 documents and 1 docvar.\ntext1 :\n\"sun downs technical director: al-ahly respected us and playe...\"\n\ntext2 :\n\"shawky gharib after the tie with enppi: our goal is to retur...\"\n\ntext3 :\n\"egyptian sports news today, wednesday 1/25/2023, which is ma...\"\n\ntext4 :\n\"the main referees committee of the egyptian football associa...\"\n\ntext5 :\n\"haji bari, the striker of the future team, is undergoing a f...\"\n\ntext6 :\n\"zamalek is preparing for a strong match against the arab con...\"\n```\n:::\n:::\n\n\n## Creating DFMs & cleaning text\n\nCreating a count-based DFM (frequency of each word in each document). Before creating the DFM I employ 3 steps of normalization:\\\n      1. Removing special characters and expressions (punctuation, urls...).\\\n      2. Converting words to their stem-form (the word *example* converts to *exampl*). This step should decrease       the number of unique tokens, so that our DFM would be less sparse.\\\n      3. Converting every character to lower-case letters.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-6_ed23f8adb4dfd4373302b6f06c48f0f9'}\n\n```{.r .cell-code}\ndata_feature_mat <- corp |>\n  tokens(remove_punct = T, remove_numbers = T, remove_url = T, remove_separators = T, remove_symbols = T) |>\n  tokens_wordstem() |>\n  dfm(tolower = T)\n\nhead(data_feature_mat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDocument-feature matrix of: 6 documents, 18,869 features (99.89% sparse) and 1 docvar.\n       features\ndocs    sun down technic director al-ah respect us and play to\n  text1   1    1       1        1     1       1  1   1    1  1\n  text2   0    0       0        0     0       0  0   0    0  2\n  text3   0    0       1        1     0       0  0   0    0  0\n  text4   0    0       0        0     0       0  0   1    0  0\n  text5   0    0       0        0     0       0  0   0    0  1\n  text6   0    0       0        0     0       0  0   0    0  0\n[ reached max_nfeat ... 18,859 more features ]\n```\n:::\n:::\n\n\n# Machine Learning\n\n### Base-rate\n\nIn classification tasks, it is important to check the initial base-rate of the different classes. Failing to notice a skewed sample will result in false interpretations of performance metrics.\n\n> *Imagine that a certain medical condition affects 1% of the population. A test that systematically gives negative results will have 99% accuracy!*\n\nWe will calculate the base-rate in the following way:\\\n$$\n\\frac{N_{real}}{N_{real}+N_{fake}}\\\n$$\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-7_4c921829e68c7518f898087fdac185d2'}\n\n```{.r .cell-code}\ntable(docvars(data_feature_mat, 'label'))['real']\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n real \n21869 \n```\n:::\n:::\n\n::: {.cell hash='index_cache/html/unnamed-chunk-8_6cef7703830f755dbcf373a62d7913eb'}\n\n```{.r .cell-code}\ntable(docvars(data_feature_mat, 'label'))['fake']\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n fake \n19991 \n```\n:::\n:::\n\n::: {.cell hash='index_cache/html/unnamed-chunk-9_7d8d98c235bcb280ce27a5bb25783e15'}\n\n:::\n\n\nThe base rate is $0.522$.\n\n### Train-test split\n\nUsually it is advised that the Train-test split would occur before the creation of the DFM. That is because some methods of calculation are proportional and depends on other documents. Because we are employing a count-based DFM, the values in each cell of the matrix are independent of other documents. Therefore it is not necessary to split before creating the DFM's.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-10_64bf58946f4146abc6a1d139d0e9aae8'}\n\n```{.r .cell-code}\nset.seed(14)\n\ntrain_dfm <- dfm_sample(data_feature_mat, size = 0.8 * nrow(data_feature_mat)) # large sample, we can use 80-20 train-test split\n\ntest_dfm <- data_feature_mat[setdiff(docnames(data_feature_mat), docnames(train_dfm)),]\n```\n:::\n\n\n## Naive-Bayes classifier\n\nA Naive-Bayes classifier predicts the class (in this case of a document) in the following way:\\\n\n1.  The likelihood of each token to appear in documents of each class is calculated from the training data. For example, if the word *manager* appeared in $11$% of the documents in the first class, and in $2$% of the documents in the second class, the likelihood of it in each class is:\\\n    $$\n    \\displaylines{p(manager\\ |\\ class\\ 1)=0.11\\\\p(manager\\ |\\ class\\ 2)=0.02}\n    $$\n\n2.  The prior probability of each document to be classified to each class is also learned from the training data, and it is the base-rate frequencies of the two classes.\\\n\n3.  For each document in the test set, the likelihood of it given it is from each of the classes is calculated by multiplying the likelihoods of the tokens appearing in it. So if a document is for example the sentence *I like turtles*, then the likelihood of it to belong class 1 is:\\\n    $$\n    \\displaylines{p(I\\ |\\ class\\ 1)\\cdotp(like\\ |\\ class\\ 1)\\cdotp(turtles\\ |\\ class\\ 1)}\n    $$\\\n    More formally, if a document belong to a certain class $k$, then it's likelihood of being comprised of a set of tokens $t$ is:\\\n    $$\n    \\prod_{i=1}^{n}p(t_i\\ |\\ class\\ k)\n    $$\n\n4.  According to Bayes theorem, the probability of the document to belong to class $k$ - the posterior probability - is proportional to the product of the likelihood of it's tokens given this class and the prior probability of any document to belong to this class:\\\n    $$\n    p(class\\ k\\ |\\ t) \\propto p(t\\ |\\ class\\ k)\\cdotp(class\\ k)\n    $$\n\n5.  Because the Naive-Bayes classifier is comparing between classes, the standardizing term is not needed. The class that has the largest product of prior and likelihood is the class the document will be classified to.\n\n## Fitting the model\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-11_f55b3923c3f158039467b422086164b0'}\n\n```{.r .cell-code}\nnb_model <- textmodel_nb(train_dfm, y = docvars(train_dfm, \"label\"))\n```\n:::\n\n\n## Test performance\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-12_beb12728719c4087bb6aaf11ee589396'}\n\n```{.r .cell-code}\npred_nb <- predict(nb_model, newdata = test_dfm)\n\n(conmat_nb <- table(pred_nb, docvars(test_dfm, \"label\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       \npred_nb fake real\n   fake 3893  380\n   real  143 3956\n```\n:::\n:::\n\n\nSeems nice, let's look at some metrics.\n\nConfusion matrix\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-13_735f7120ab84f226ed04475a45185bd4'}\n\n```{.r .cell-code}\ncaret::confusionMatrix(conmat_nb, mode = \"everything\", positive = \"real\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n       \npred_nb fake real\n   fake 3893  380\n   real  143 3956\n                                          \n               Accuracy : 0.9375          \n                 95% CI : (0.9321, 0.9426)\n    No Information Rate : 0.5179          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.8752          \n                                          \n Mcnemar's Test P-Value : < 2.2e-16       \n                                          \n            Sensitivity : 0.9124          \n            Specificity : 0.9646          \n         Pos Pred Value : 0.9651          \n         Neg Pred Value : 0.9111          \n              Precision : 0.9651          \n                 Recall : 0.9124          \n                     F1 : 0.9380          \n             Prevalence : 0.5179          \n         Detection Rate : 0.4725          \n   Detection Prevalence : 0.4896          \n      Balanced Accuracy : 0.9385          \n                                          \n       'Positive' Class : real            \n                                          \n```\n:::\n:::\n\n\nNice results!\n\n## Logistic regression\n\nThe nice thing about the `textmodel_lr` method from the `quanteda.textmodels` package is that it does the Cross-Validation for us!\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-14_d4fac98c7a2fc4dca3848cdd0b6bfdc5'}\n\n```{.r .cell-code}\nlr_model <- textmodel_lr(x = train_dfm, y = docvars(train_dfm, \"label\"))\n```\n:::\n\n\n## Test performance\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-15_e52b1120af23c9cdafe78b7883525a9f'}\n\n```{.r .cell-code}\npred_lr <- predict(lr_model, newdata = test_dfm)\n\n(conmat_lr <- table(pred_lr, docvars(test_dfm, \"label\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       \npred_lr fake real\n   fake 3795  188\n   real  241 4148\n```\n:::\n:::\n\n\nAlso seems nice.\n\nConfusion matrix\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-16_4427652dc6661c861efa1560aa3c48db'}\n\n```{.r .cell-code}\ncaret::confusionMatrix(conmat_lr, mode = \"everything\", positive = \"real\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n       \npred_lr fake real\n   fake 3795  188\n   real  241 4148\n                                          \n               Accuracy : 0.9488          \n                 95% CI : (0.9438, 0.9534)\n    No Information Rate : 0.5179          \n    P-Value [Acc > NIR] : < 2e-16         \n                                          \n                  Kappa : 0.8973          \n                                          \n Mcnemar's Test P-Value : 0.01205         \n                                          \n            Sensitivity : 0.9566          \n            Specificity : 0.9403          \n         Pos Pred Value : 0.9451          \n         Neg Pred Value : 0.9528          \n              Precision : 0.9451          \n                 Recall : 0.9566          \n                     F1 : 0.9508          \n             Prevalence : 0.5179          \n         Detection Rate : 0.4955          \n   Detection Prevalence : 0.5242          \n      Balanced Accuracy : 0.9485          \n                                          \n       'Positive' Class : real            \n                                          \n```\n:::\n:::\n\n\nSlight improvement...\n\n## Plot important words for classification\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-17_81d6f326a6f7b96659a8658fa7d609ec'}\n\n```{.r .cell-code  code-fold=\"true\"}\nlr_summary <- summary(lr_model) # summarizing the model\n\ncoefs <- data.frame(lr_summary$estimated.feature.scores) # extracting coefficients\n\n\ncol_vec <- c(\"#fc7753\", \"#e3e3e3\", \"#66d7d1\", \"black\")\n\ncoefs |>\n  \n  # preparing df for plot\n  rownames_to_column(var = \"Token\") |>\n  rename(Coefficient = real) |>\n  filter(Coefficient != 0 & Token != \"(Intercept)\") |>\n  mutate(bigger_then_0 = Coefficient > 0) |>\n  \n  # ggplotting\n  ggplot(aes(x = Token, y = Coefficient, color = bigger_then_0)) +\n  geom_point() +\n  scale_color_manual(values = c(col_vec[1], col_vec[3])) +\n  scale_y_continuous(n.breaks = 10) +\n  labs(title = \"Most important words for classifying if a tweet is fake news\",\n       x = \"\") +\n  theme_classic() +\n  geom_hline(yintercept = 0, color = \"black\", linetype = 2, linewidth = 1, show.legend = F) +\n  theme(plot.background = element_rect(color = col_vec[2], fill = col_vec[2]),\n        panel.background = element_rect(color = col_vec[2], fill = col_vec[2]),\n        axis.line = element_line(color = col_vec[4]),\n        axis.title = element_text(color = col_vec[4]),\n        axis.text = element_text(color = col_vec[4]),\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1, size = 12, face = \"bold\"),\n        legend.position = \"none\",\n        plot.title = element_text(size = 16, color = col_vec[4], hjust = .5, family = \"serif\", face = \"bold\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){width=200%}\n:::\n:::\n\n\n# Conclusion\n\nI think that for a first attempt at implementing NLP, the current notebook came out nice. I have learned a lot not only about NLP, but also on Naive-Bayes classifiers which is nice.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}